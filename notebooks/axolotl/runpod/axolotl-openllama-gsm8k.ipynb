{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJvyUmZdktu0",
    "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/axolotl\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC48y25Lkqa5",
    "outputId": "2757a3c8-3790-4fd3-be39-03be8f533b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!accelerate config --config_file configs/accelerate/default_config.yaml default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "--s-HPfqUwH9",
    "outputId": "c524f89e-bafc-486e-b760-ff4697c16da6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on https://gist.github.com/fearnworks/723709806cebc67bafe1eb8138e7efbd\n",
      "base_model: openlm-research/open_llama_3b_600bt_preview\n",
      "base_model_config: openlm-research/open_llama_3b_600bt_preview\n",
      "model_type: LlamaForCausalLM\n",
      "tokenizer_type: LlamaTokenizer\n",
      "load_in_8bit: false\n",
      "load_in_4bit: true\n",
      "strict: false\n",
      "push_dataset_to_hub:\n",
      "datasets:\n",
      "  - path: QingyiSi/Alpaca-CoT\n",
      "    data_files:\n",
      "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
      "    type: \"alpaca:chat\"\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.01\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "sequence_len: 2048\n",
      "max_packed_sequence_len:\n",
      "lora_r: 16\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules:\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "wandb_project: openllama-qlora-model\n",
      "wandb_watch: all\n",
      "wandb_run_id:\n",
      "wandb_log_model: checkpoint\n",
      "output_dir: ./qlora-out\n",
      "batch_size: 1\n",
      "micro_batch_size: 1\n",
      "num_epochs: 1\n",
      "optimizer: paged_adamw_32bit\n",
      "torchdistx_path:\n",
      "lr_scheduler: cosine\n",
      "learning_rate: 0.0002\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: true\n",
      "fp16: false\n",
      "tf32: true\n",
      "gradient_checkpointing: true\n",
      "early_stopping_patience:\n",
      "resume_from_checkpoint:\n",
      "auto_resume_from_checkpoints: true\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention: true\n",
      "flash_attention:\n",
      "gptq_groupsize:\n",
      "gptq_model_v1:\n",
      "warmup_steps: 10\n",
      "eval_steps: 20\n",
      "save_steps: 100\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.0\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  bos_token: \"<s>\"\n",
      "  eos_token: \"</s>\"\n",
      "  unk_token: \"<unk>\"\n"
     ]
    }
   ],
   "source": [
    "!cat examples/openllama/qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jICMPJuomFsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK1tFOFrWbmoa2ckCJYhzgBHKTSMeR/AeuScCCzugqlI utensilcandel@gmail.com')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading tokenizer...\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Unable to find prepared dataset in last_run_prepared/ad4057cb8b111b35ea37b1112981addc\n",
      "INFO:root:Loading raw datasets...\n",
      "Downloading readme: 100%|██████████████████| 8.26k/8.26k [00:00<00:00, 16.6MB/s]\n",
      "Downloading and preparing dataset json/QingyiSi--Alpaca-CoT to /root/.cache/huggingface/datasets/QingyiSi___json/QingyiSi--Alpaca-CoT-a2fee0ff0bfd6656/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4...\n",
      "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
      "Downloading data: 100%|████████████████████| 4.45M/4.45M [00:00<00:00, 50.3MB/s]\u001b[A\n",
      "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  2.53it/s]\n",
      "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 2027.21it/s]\n",
      "Dataset json downloaded and prepared to /root/.cache/huggingface/datasets/QingyiSi___json/QingyiSi--Alpaca-CoT-a2fee0ff0bfd6656/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4. Subsequent calls will reuse this data.\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 725.53it/s]\n",
      "INFO:root:tokenizing, merging, and shuffling master dataset\n",
      "INFO:root:Saving merged prepared dataset to disk... last_run_prepared/ad4057cb8b111b35ea37b1112981addc\n",
      "INFO:root:loading model and peft_config...                                      \n",
      "INFO:root:patching with xformers attention\n",
      "Replaced attention with xformers_attention\u001b[0m\n",
      "Downloading (…)lve/main/config.json: 100%|██████| 506/506 [00:00<00:00, 104kB/s]\u001b[0m\u001b[0m\n",
      "Downloading pytorch_model.bin: 100%|████████| 6.85G/6.85G [01:00<00:00, 114MB/s]\u001b[0m\u001b[0m\n",
      "Downloading (…)neration_config.json: 100%|██████| 137/137 [00:00<00:00, 189kB/s]\u001b[0m\u001b[0m\n",
      "\u001b[0mINFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "\u001b[0mINFO:root:found linear modules: ['down_proj', 'up_proj', 'q_proj', 'gate_proj', 'o_proj', 'v_proj', 'k_proj']\n",
      "trainable params: 25425920 || all params: 1841147520 || trainable%: 1.3809822256936803\u001b[0m\n",
      "INFO:root:Compiling torch model\n",
      "\u001b[0mINFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230530_003932-o5mkzy0o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msoft-cherry-1\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-model\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-model/runs/o5mkzy0o\u001b[0m\n",
      "\u001b[0m\u001b[0m{'loss': 1.257, 'learning_rate': 2e-05, 'epoch': 0.0}\u001b[0m               \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.3123, 'learning_rate': 4e-05, 'epoch': 0.0}\u001b[0m          \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.4045, 'learning_rate': 6e-05, 'epoch': 0.0}\u001b[0m          \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9349, 'learning_rate': 8e-05, 'epoch': 0.0}\u001b[0m          \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.22, 'learning_rate': 0.0001, 'epoch': 0.0}\u001b[0m           \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.3218, 'learning_rate': 0.00012, 'epoch': 0.0}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0265, 'learning_rate': 0.00014, 'epoch': 0.0}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.4701, 'learning_rate': 0.00016, 'epoch': 0.0}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9963, 'learning_rate': 0.00018, 'epoch': 0.0}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2696, 'learning_rate': 0.0002, 'epoch': 0.0}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.5303, 'learning_rate': 0.0001999999909590104, 'epoch': 0.0}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.211, 'learning_rate': 0.00019999996383604325, 'epoch': 0.0}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.3497, 'learning_rate': 0.00019999991863110342, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8236, 'learning_rate': 0.0001999998553441991, 'epoch': 0.0}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8478, 'learning_rate': 0.00019999977397534176, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0028, 'learning_rate': 0.00019999967452454608, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2194, 'learning_rate': 0.00019999955699183004, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8946, 'learning_rate': 0.00019999942137721493, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6164, 'learning_rate': 0.00019999926768072524, 'epoch': 0.0}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9531, 'learning_rate': 0.00019999909590238875, 'epoch': 0.0}\u001b[0mm\n",
      "  0%|                                         | 20/7398 [00:07<42:47,  2.87it/s]\u001b[0m\u001b[0m\n",
      "  0%|                                                    | 0/75 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "  4%|█▊                                          | 3/75 [00:00<00:04, 16.29it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "  7%|██▉                                         | 5/75 [00:00<00:05, 13.19it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "  9%|████                                        | 7/75 [00:00<00:05, 11.82it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 12%|█████▎                                      | 9/75 [00:00<00:05, 11.50it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 15%|██████▎                                    | 11/75 [00:00<00:05, 11.34it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 17%|███████▍                                   | 13/75 [00:01<00:05, 11.20it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 20%|████████▌                                  | 15/75 [00:01<00:05, 11.00it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 23%|█████████▋                                 | 17/75 [00:01<00:05, 11.02it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 25%|██████████▉                                | 19/75 [00:01<00:05, 10.99it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 28%|████████████                               | 21/75 [00:01<00:04, 10.85it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 31%|█████████████▏                             | 23/75 [00:02<00:04, 10.71it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 33%|██████████████▎                            | 25/75 [00:02<00:04, 10.81it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 36%|███████████████▍                           | 27/75 [00:02<00:04, 10.78it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 39%|████████████████▋                          | 29/75 [00:02<00:04, 10.64it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 41%|█████████████████▊                         | 31/75 [00:02<00:04, 10.54it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 44%|██████████████████▉                        | 33/75 [00:02<00:03, 10.62it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 47%|████████████████████                       | 35/75 [00:03<00:03, 10.64it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 49%|█████████████████████▏                     | 37/75 [00:03<00:03, 10.69it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 52%|██████████████████████▎                    | 39/75 [00:03<00:03, 10.71it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 55%|███████████████████████▌                   | 41/75 [00:03<00:03, 10.63it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 57%|████████████████████████▋                  | 43/75 [00:03<00:03, 10.58it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 60%|█████████████████████████▊                 | 45/75 [00:04<00:02, 10.48it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 63%|██████████████████████████▉                | 47/75 [00:04<00:02, 10.35it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 65%|████████████████████████████               | 49/75 [00:04<00:02, 10.34it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 68%|█████████████████████████████▏             | 51/75 [00:04<00:02, 10.37it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 71%|██████████████████████████████▍            | 53/75 [00:04<00:02, 10.12it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 73%|███████████████████████████████▌           | 55/75 [00:05<00:01, 10.23it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 76%|████████████████████████████████▋          | 57/75 [00:05<00:01, 10.22it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 79%|█████████████████████████████████▊         | 59/75 [00:05<00:01, 10.15it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 81%|██████████████████████████████████▉        | 61/75 [00:05<00:01, 10.12it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 84%|████████████████████████████████████       | 63/75 [00:05<00:01, 10.24it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 87%|█████████████████████████████████████▎     | 65/75 [00:06<00:00, 10.28it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 89%|██████████████████████████████████████▍    | 67/75 [00:06<00:00, 10.28it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 92%|███████████████████████████████████████▌   | 69/75 [00:06<00:00, 10.31it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 95%|████████████████████████████████████████▋  | 71/75 [00:06<00:00, 10.35it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 97%|█████████████████████████████████████████▊ | 73/75 [00:06<00:00, 10.29it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.9859086275100708, 'eval_runtime': 7.1462, 'eval_samples_per_second': 10.495, 'eval_steps_per_second': 10.495, 'epoch': 0.0}\u001b[0m\n",
      "  0%|                                         | 20/7398 [00:14<42:47,  2.87it/s]\u001b[0m\n",
      "100%|███████████████████████████████████████████| 75/75 [00:07<00:00, 10.32it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1123, 'learning_rate': 0.00019999890604223658, 'epoch': 0.0}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6588, 'learning_rate': 0.000199998698100303, 'epoch': 0.0}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7699, 'learning_rate': 0.0001999984720766256, 'epoch': 0.0}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6795, 'learning_rate': 0.0001999982279712453, 'epoch': 0.0}\u001b[0m0m\n",
      "  0%|▏                                      | 24/7398 [00:16<2:15:18,  1.10s/it]\u001b[0m\u001b[0mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 273, in <module>\n",
      "    fire.Fire(train)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 261, in train\n",
      "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1696, in train\n",
      "    return inner_training_loop(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1973, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2787, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2819, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1547, in _call_impl\n",
      "    hook_result = hook(self, args, result)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/wandb_torch.py\", line 110, in <lambda>\n",
      "    lambda mod, inp, outp: parameter_log_hook(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/wandb_torch.py\", line 105, in parameter_log_hook\n",
      "    self.log_tensor_stats(data.cpu(), \"parameters/\" + prefix + name)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/wandb/wandb_torch.py\", line 231, in log_tensor_stats\n",
      "    tensor = flat.histc(bins=self._num_bins, min=tmin, max=tmax)\n",
      "\u001b[0m\u001b[0mRuntimeError: \"histogram_cpu\" not implemented for 'Byte'\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: / 0.014 MB of 0.014 MB uploaded (0.000 MB deduped)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second ▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ▁▂▃▃▄▅▆▆▇███████████████\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss ▆▆▇▃▆▆▄█▄▆█▆▇▃▃▄▆▃▁▄▅▁▂▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss 0.98591\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime 7.1462\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second 10.495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second 10.495\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step 24\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate 0.0002\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss 0.6795\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33msoft-cherry-1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-model/runs/o5mkzy0o\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230530_003932-o5mkzy0o/logs\u001b[0m\n",
      "\u001b[0m\u001b[0mTraceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 928, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 588, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.9/bin/python3', 'scripts/finetune.py', 'examples/openllama/qlora.yml']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py examples/openllama/qlora.yml"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP0jHt4WNuLaC5ecmX0YtWl",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
