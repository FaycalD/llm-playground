{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJvyUmZdktu0",
    "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/axolotl\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC48y25Lkqa5",
    "outputId": "2757a3c8-3790-4fd3-be39-03be8f533b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!accelerate config --config_file configs/accelerate/default_config.yaml default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on https://gist.github.com/fearnworks/723709806cebc67bafe1eb8138e7efbd\n",
      "base_model: openlm-research/open_llama_3b_600bt_preview\n",
      "base_model_config: openlm-research/open_llama_3b_600bt_preview\n",
      "model_type: LlamaForCausalLM\n",
      "tokenizer_type: LlamaTokenizer\n",
      "load_in_8bit: false\n",
      "load_in_4bit: true\n",
      "strict: false\n",
      "push_dataset_to_hub:\n",
      "datasets:\n",
      "  - path: QingyiSi/Alpaca-CoT\n",
      "    data_files:\n",
      "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
      "    type: \"alpaca:chat\"\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.01\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "sequence_len: 2048\n",
      "max_packed_sequence_len:\n",
      "lora_r: 16\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules:\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "wandb_project: openllama-qlora-gsm8k-16\n",
      "wandb_watch:\n",
      "wandb_run_id:\n",
      "wandb_log_model: checkpoint\n",
      "output_dir: ./qlora-out\n",
      "batch_size: 128\n",
      "micro_batch_size: 32\n",
      "num_epochs: 1\n",
      "optimizer: paged_adamw_32bit\n",
      "torchdistx_path:\n",
      "lr_scheduler: cosine\n",
      "learning_rate: 0.0002\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: true\n",
      "fp16: false\n",
      "tf32: true\n",
      "gradient_checkpointing: true\n",
      "early_stopping_patience:\n",
      "resume_from_checkpoint:\n",
      "auto_resume_from_checkpoints: true\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention: true\n",
      "flash_attention:\n",
      "gptq_groupsize:\n",
      "gptq_model_v1:\n",
      "warmup_steps: 10\n",
      "eval_steps: 20\n",
      "save_steps: 100\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.0\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  bos_token: \"<s>\"\n",
      "  eos_token: \"</s>\"\n",
      "  unk_token: \"<unk>\"\n"
     ]
    }
   ],
   "source": [
    "!cat examples/openllama/qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "jICMPJuomFsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK1tFOFrWbmoa2ckCJYhzgBHKTSMeR/AeuScCCzugqlI utensilcandel@gmail.com')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading tokenizer...\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Loading prepared dataset from disk at last_run_prepared/ad4057cb8b111b35ea37b1112981addc...\n",
      "INFO:root:Prepared dataset loaded from disk...\n",
      "INFO:root:loading model and peft_config...\n",
      "INFO:root:patching with xformers attention\n",
      "Replaced attention with xformers_attention\u001b[0m\n",
      "INFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "\u001b[0mINFO:root:found linear modules: ['o_proj', 'down_proj', 'q_proj', 'k_proj', 'v_proj', 'up_proj', 'gate_proj']\n",
      "\u001b[0mtrainable params: 25425920 || all params: 1841147520 || trainable%: 1.3809822256936803\u001b[0m\n",
      "INFO:root:Compiling torch model\n",
      "\u001b[0mINFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230530_064735-elabmzse\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbright-glitter-3\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16/runs/elabmzse\u001b[0m\n",
      "\u001b[0m\u001b[0m{'loss': 1.2442, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m             \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2531, 'learning_rate': 4e-05, 'epoch': 0.03}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2315, 'learning_rate': 6e-05, 'epoch': 0.05}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1909, 'learning_rate': 8e-05, 'epoch': 0.07}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2224, 'learning_rate': 0.0001, 'epoch': 0.09}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1506, 'learning_rate': 0.00012, 'epoch': 0.1}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.126, 'learning_rate': 0.00014, 'epoch': 0.12}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0871, 'learning_rate': 0.00016, 'epoch': 0.14}\u001b[0m       \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0702, 'learning_rate': 0.00018, 'epoch': 0.16}\u001b[0m       \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0137, 'learning_rate': 0.0002, 'epoch': 0.17}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9296, 'learning_rate': 0.00019978589232386035, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9998, 'learning_rate': 0.00019914448613738106, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9573, 'learning_rate': 0.00019807852804032305, 'epoch': 0.22}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9433, 'learning_rate': 0.00019659258262890683, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8739, 'learning_rate': 0.0001946930129495106, 'epoch': 0.26}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9177, 'learning_rate': 0.0001923879532511287, 'epoch': 0.28}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8696, 'learning_rate': 0.00018968727415326884, 'epoch': 0.29}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8012, 'learning_rate': 0.00018660254037844388, 'epoch': 0.31}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8335, 'learning_rate': 0.00018314696123025454, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8431, 'learning_rate': 0.00017933533402912354, 'epoch': 0.34}\u001b[0m\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 20/58 [08:10<15:58, 25.21s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8390889167785645, 'eval_runtime': 3.5174, 'eval_samples_per_second': 21.322, 'eval_steps_per_second': 0.853, 'epoch': 0.34}\u001b[0m\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                            | 20/58 [08:13<15:58, 25.21s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8162, 'learning_rate': 0.00017518398074789775, 'epoch': 0.36}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7809, 'learning_rate': 0.00017071067811865476, 'epoch': 0.38}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7745, 'learning_rate': 0.00016593458151000688, 'epoch': 0.4}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8042, 'learning_rate': 0.00016087614290087208, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7849, 'learning_rate': 0.00015555702330196023, 'epoch': 0.43}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.802, 'learning_rate': 0.00015000000000000001, 'epoch': 0.45}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7542, 'learning_rate': 0.00014422886902190014, 'epoch': 0.47}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8154, 'learning_rate': 0.000138268343236509, 'epoch': 0.48}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7722, 'learning_rate': 0.00013214394653031616, 'epoch': 0.5}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7561, 'learning_rate': 0.00012588190451025207, 'epoch': 0.52}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7811, 'learning_rate': 0.00011950903220161285, 'epoch': 0.53}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.722, 'learning_rate': 0.00011305261922200519, 'epoch': 0.55}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7374, 'learning_rate': 0.00010654031292301432, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7295, 'learning_rate': 0.0001, 'epoch': 0.59}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7509, 'learning_rate': 9.345968707698569e-05, 'epoch': 0.6}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7046, 'learning_rate': 8.694738077799488e-05, 'epoch': 0.62}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7783, 'learning_rate': 8.049096779838719e-05, 'epoch': 0.64}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7586, 'learning_rate': 7.411809548974792e-05, 'epoch': 0.66}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7726, 'learning_rate': 6.785605346968386e-05, 'epoch': 0.67}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7483, 'learning_rate': 6.173165676349103e-05, 'epoch': 0.69}\u001b[0mm\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 40/58 [16:05<06:55, 23.10s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7597585916519165, 'eval_runtime': 3.5177, 'eval_samples_per_second': 21.32, 'eval_steps_per_second': 0.853, 'epoch': 0.69}\u001b[0m\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã             | 40/58 [16:08<06:55, 23.10s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7898, 'learning_rate': 5.577113097809989e-05, 'epoch': 0.71}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7524, 'learning_rate': 5.000000000000002e-05, 'epoch': 0.72}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7478, 'learning_rate': 4.444297669803981e-05, 'epoch': 0.74}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7241, 'learning_rate': 3.9123857099127936e-05, 'epoch': 0.76}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7325, 'learning_rate': 3.406541848999312e-05, 'epoch': 0.78}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7192, 'learning_rate': 2.9289321881345254e-05, 'epoch': 0.79}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7819, 'learning_rate': 2.4816019252102273e-05, 'epoch': 0.81}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7312, 'learning_rate': 2.0664665970876496e-05, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7189, 'learning_rate': 1.6853038769745467e-05, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7318, 'learning_rate': 1.339745962155613e-05, 'epoch': 0.86}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7236, 'learning_rate': 1.0312725846731175e-05, 'epoch': 0.88}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7162, 'learning_rate': 7.612046748871327e-06, 'epoch': 0.9}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7256, 'learning_rate': 5.306987050489442e-06, 'epoch': 0.91}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7075, 'learning_rate': 3.40741737109318e-06, 'epoch': 0.93}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7581, 'learning_rate': 1.921471959676957e-06, 'epoch': 0.95}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7113, 'learning_rate': 8.555138626189618e-07, 'epoch': 0.97}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7006, 'learning_rate': 2.141076761396521e-07, 'epoch': 0.98}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7996, 'learning_rate': 0.0, 'epoch': 1.0}\u001b[0m            \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'train_runtime': 1399.3844, 'train_samples_per_second': 5.287, 'train_steps_per_second': 0.041, 'train_loss': 0.8478451391746258, 'epoch': 1.0}\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 58/58 [23:25<00:00, 24.24s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0mINFO:root:Training Completed!!! Saving pre-trained model to ./qlora-out\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[32m(success).\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss ‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime ‚ñÅ‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second ‚ñà‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate ‚ñÇ‚ñÇ‚ñÉ‚ñÖ‚ñÖ‚ñá‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss ‚ñà‚ñà‚ñà‚ñà‚ñá‚ñÜ‚ñÜ‚ñÑ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                      eval/loss 0.75976\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                   eval/runtime 3.5177\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:        eval/samples_per_second 21.32\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:          eval/steps_per_second 0.853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                    train/epoch 1.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/global_step 58\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                     train/loss 0.7996\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/total_flos 2.746531146584064e+16\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               train/train_loss 0.84785\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            train/train_runtime 1399.3844\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/train_samples_per_second 5.287\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   train/train_steps_per_second 0.041\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mbright-glitter-3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16/runs/elabmzse\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230530_064735-elabmzse/logs\u001b[0m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py examples/openllama/qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on https://gist.github.com/fearnworks/723709806cebc67bafe1eb8138e7efbd\n",
      "base_model: openlm-research/open_llama_3b_600bt_preview\n",
      "base_model_config: openlm-research/open_llama_3b_600bt_preview\n",
      "model_type: LlamaForCausalLM\n",
      "tokenizer_type: LlamaTokenizer\n",
      "load_in_8bit: false\n",
      "load_in_4bit: true\n",
      "strict: false\n",
      "push_dataset_to_hub:\n",
      "datasets:\n",
      "  - path: QingyiSi/Alpaca-CoT\n",
      "    data_files:\n",
      "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
      "    type: \"alpaca:chat\"\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.01\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "sequence_len: 2048\n",
      "max_packed_sequence_len:\n",
      "lora_r: 16\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.05\n",
      "lora_target_modules:\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "wandb_project: openllama-qlora-gsm8k-16\n",
      "wandb_watch:\n",
      "wandb_run_id:\n",
      "wandb_log_model: checkpoint\n",
      "output_dir: ./qlora-out\n",
      "batch_size: 128\n",
      "micro_batch_size: 32\n",
      "num_epochs: 2\n",
      "optimizer: paged_adamw_32bit\n",
      "torchdistx_path:\n",
      "lr_scheduler: cosine\n",
      "learning_rate: 0.0002\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: true\n",
      "fp16: false\n",
      "tf32: true\n",
      "gradient_checkpointing: true\n",
      "early_stopping_patience:\n",
      "resume_from_checkpoint:\n",
      "auto_resume_from_checkpoints: true\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention: true\n",
      "flash_attention:\n",
      "gptq_groupsize:\n",
      "gptq_model_v1:\n",
      "warmup_steps: 10\n",
      "eval_steps: 5\n",
      "save_steps: 10\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.0\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  bos_token: \"<s>\"\n",
      "  eos_token: \"</s>\"\n",
      "  unk_token: \"<unk>\"\n"
     ]
    }
   ],
   "source": [
    "!cat examples/openllama/qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK1tFOFrWbmoa2ckCJYhzgBHKTSMeR/AeuScCCzugqlI utensilcandel@gmail.com')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading tokenizer...\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Loading prepared dataset from disk at last_run_prepared/ad4057cb8b111b35ea37b1112981addc...\n",
      "INFO:root:Prepared dataset loaded from disk...\n",
      "INFO:root:loading model and peft_config...\n",
      "INFO:root:patching with xformers attention\n",
      "Replaced attention with xformers_attention\u001b[0m\n",
      "INFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "\u001b[0mINFO:root:found linear modules: ['o_proj', 'k_proj', 'down_proj', 'gate_proj', 'up_proj', 'v_proj', 'q_proj']\n",
      "\u001b[0mtrainable params: 25425920 || all params: 1841147520 || trainable%: 1.3809822256936803\u001b[0m\n",
      "INFO:root:Compiling torch model\n",
      "\u001b[0mINFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230530_073119-y84grpru\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33methereal-blaze-8\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16/runs/y84grpru\u001b[0m\n",
      "\u001b[0m\u001b[0m{'loss': 1.2442, 'learning_rate': 2e-05, 'epoch': 0.02}\u001b[0m             \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2531, 'learning_rate': 4e-05, 'epoch': 0.03}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2316, 'learning_rate': 6e-05, 'epoch': 0.05}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1909, 'learning_rate': 8e-05, 'epoch': 0.07}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.2228, 'learning_rate': 0.0001, 'epoch': 0.09}\u001b[0m        \u001b[0m\u001b[0m\n",
      "  4%|‚ñà‚ñä                                         | 5/116 [02:00<44:00, 23.79s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 1.2206956148147583, 'eval_runtime': 3.5083, 'eval_samples_per_second': 21.378, 'eval_steps_per_second': 0.855, 'epoch': 0.09}\u001b[0m\n",
      "  4%|‚ñà‚ñä                                         | 5/116 [02:04<44:00, 23.79s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.52it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1506, 'learning_rate': 0.00012, 'epoch': 0.1}\u001b[0m        \u001b[0m\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.1258, 'learning_rate': 0.00014, 'epoch': 0.12}\u001b[0m       \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0868, 'learning_rate': 0.00016, 'epoch': 0.14}\u001b[0m       \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0696, 'learning_rate': 0.00018, 'epoch': 0.16}\u001b[0m       \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 1.0123, 'learning_rate': 0.0002, 'epoch': 0.17}\u001b[0m        \u001b[0m\u001b[0m\n",
      "  9%|‚ñà‚ñà‚ñà‚ñå                                      | 10/116 [04:05<42:35, 24.11s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 1.0031449794769287, 'eval_runtime': 3.5074, 'eval_samples_per_second': 21.383, 'eval_steps_per_second': 0.855, 'epoch': 0.17}\u001b[0m\n",
      "  9%|‚ñà‚ñà‚ñà‚ñå                                      | 10/116 [04:08<42:35, 24.11s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.52it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-10)... \u001b[0mDone. 4.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9288, 'learning_rate': 0.00019995608365087946, 'epoch': 0.19}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9988, 'learning_rate': 0.00019982437317643217, 'epoch': 0.21}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9562, 'learning_rate': 0.0001996049842615217, 'epoch': 0.22}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9431, 'learning_rate': 0.00019929810960135172, 'epoch': 0.24}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8732, 'learning_rate': 0.0001989040187322164, 'epoch': 0.26}\u001b[0mm\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 15/116 [06:20<42:30, 25.26s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8914053440093994, 'eval_runtime': 3.5109, 'eval_samples_per_second': 21.362, 'eval_steps_per_second': 0.854, 'epoch': 0.26}\u001b[0m\n",
      " 13%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                                    | 15/116 [06:23<42:30, 25.26s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9166, 'learning_rate': 0.00019842305779475968, 'epoch': 0.28}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8686, 'learning_rate': 0.0001978556492299504, 'epoch': 0.29}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8002, 'learning_rate': 0.0001972022914080411, 'epoch': 0.31}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8322, 'learning_rate': 0.00019646355819083589, 'epoch': 0.33}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8417, 'learning_rate': 0.00019564009842765225, 'epoch': 0.34}\u001b[0m\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 20/116 [08:26<40:53, 25.55s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8365222811698914, 'eval_runtime': 3.5078, 'eval_samples_per_second': 21.381, 'eval_steps_per_second': 0.855, 'epoch': 0.34}\u001b[0m\n",
      " 17%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè                                  | 20/116 [08:29<40:53, 25.55s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-20)... \u001b[0mDone. 4.3s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8142, 'learning_rate': 0.00019473263538541914, 'epoch': 0.36}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.779, 'learning_rate': 0.0001937419661134121, 'epoch': 0.38}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.773, 'learning_rate': 0.00019266896074318334, 'epoch': 0.4}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8019, 'learning_rate': 0.00019151456172430183, 'epoch': 0.41}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7822, 'learning_rate': 0.00019027978299657436, 'epoch': 0.43}\u001b[0m\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 25/116 [10:33<37:59, 25.05s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8042923212051392, 'eval_runtime': 3.5105, 'eval_samples_per_second': 21.365, 'eval_steps_per_second': 0.855, 'epoch': 0.43}\u001b[0m\n",
      " 22%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                                 | 25/116 [10:36<37:59, 25.05s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.52it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7983, 'learning_rate': 0.00018896570909947475, 'epoch': 0.45}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7499, 'learning_rate': 0.0001875734942195637, 'epoch': 0.47}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8117, 'learning_rate': 0.00018610436117673555, 'epoch': 0.48}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7672, 'learning_rate': 0.0001845596003501826, 'epoch': 0.5}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.75, 'learning_rate': 0.0001829405685450202, 'epoch': 0.52}\u001b[0m[0m\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 30/116 [12:29<32:57, 22.99s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7807092666625977, 'eval_runtime': 3.5125, 'eval_samples_per_second': 21.352, 'eval_steps_per_second': 0.854, 'epoch': 0.52}\u001b[0m\n",
      " 26%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä                               | 30/116 [12:32<32:57, 22.99s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-30)... \u001b[0mDone. 5.0s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.773, 'learning_rate': 0.00018124868780056814, 'epoch': 0.53}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7144, 'learning_rate': 0.00017948544414133534, 'epoch': 0.55}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7301, 'learning_rate': 0.00017765238627180424, 'epoch': 0.57}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7205, 'learning_rate': 0.00017575112421616202, 'epoch': 0.59}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7425, 'learning_rate': 0.00017378332790417273, 'epoch': 0.6}\u001b[0mm\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 35/116 [14:43<34:02, 25.21s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7604002356529236, 'eval_runtime': 3.5129, 'eval_samples_per_second': 21.35, 'eval_steps_per_second': 0.854, 'epoch': 0.6}\u001b[0m\n",
      " 30%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                             | 35/116 [14:46<34:02, 25.21s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6946, 'learning_rate': 0.00017175072570443312, 'epoch': 0.62}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7673, 'learning_rate': 0.00016965510290629972, 'epoch': 0.64}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7443, 'learning_rate': 0.00016749830015182107, 'epoch': 0.66}\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7573, 'learning_rate': 0.00016528221181905217, 'epoch': 0.67}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7314, 'learning_rate': 0.00016300878435817113, 'epoch': 0.69}\u001b[0m\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 40/116 [16:44<29:44, 23.48s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7439857721328735, 'eval_runtime': 3.5165, 'eval_samples_per_second': 21.328, 'eval_steps_per_second': 0.853, 'epoch': 0.69}\u001b[0m\n",
      " 34%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç                           | 40/116 [16:48<29:44, 23.48s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-40)... \u001b[0mDone. 4.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.776, 'learning_rate': 0.00016068001458185936, 'epoch': 0.71}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7374, 'learning_rate': 0.0001582979479114472, 'epoch': 0.72}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7296, 'learning_rate': 0.00015586467658036524, 'epoch': 0.74}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.704, 'learning_rate': 0.0001533823377964791, 'epoch': 0.76}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7101, 'learning_rate': 0.00015085311186492206, 'epoch': 0.78}\u001b[0m\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 45/116 [18:55<29:24, 24.85s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7302144169807434, 'eval_runtime': 3.5161, 'eval_samples_per_second': 21.33, 'eval_steps_per_second': 0.853, 'epoch': 0.78}\u001b[0m\n",
      " 39%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                         | 45/116 [18:58<29:24, 24.85s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.695, 'learning_rate': 0.00014827922027307451, 'epoch': 0.79}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7594, 'learning_rate': 0.0001456629237393713, 'epoch': 0.81}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7071, 'learning_rate': 0.00014300652022765207, 'epoch': 0.83}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6926, 'learning_rate': 0.00014031234292879725, 'epoch': 0.84}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7024, 'learning_rate': 0.00013758275821142382, 'epoch': 0.86}\u001b[0m\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 50/116 [20:53<26:09, 23.78s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7137561440467834, 'eval_runtime': 3.5151, 'eval_samples_per_second': 21.336, 'eval_steps_per_second': 0.853, 'epoch': 0.86}\u001b[0m\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà                        | 50/116 [20:57<26:09, 23.78s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-50)... \u001b[0mDone. 4.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.695, 'learning_rate': 0.0001348201635434399, 'epoch': 0.88}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6864, 'learning_rate': 0.00013202698538628376, 'epoch': 0.9}\u001b[0mm\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6878, 'learning_rate': 0.00012920567706369758, 'epoch': 0.91}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6696, 'learning_rate': 0.00012635871660690676, 'epoch': 0.93}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7173, 'learning_rate': 0.00012348860457809838, 'epoch': 0.95}\u001b[0m\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 55/116 [23:11<26:35, 26.15s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7016600370407104, 'eval_runtime': 3.5126, 'eval_samples_per_second': 21.352, 'eval_steps_per_second': 0.854, 'epoch': 0.95}\u001b[0m\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ                      | 55/116 [23:14<26:35, 26.15s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.52it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6703, 'learning_rate': 0.00012059786187410984, 'epoch': 0.97}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6587, 'learning_rate': 0.0001176890275122573, 'epoch': 0.98}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.755, 'learning_rate': 0.00011476465640024814, 'epoch': 1.0}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6574, 'learning_rate': 0.00011182731709213659, 'epoch': 1.02}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.685, 'learning_rate': 0.00010887958953229349, 'epoch': 1.03}\u001b[0mm\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 60/116 [25:10<22:37, 24.24s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6936182379722595, 'eval_runtime': 3.5128, 'eval_samples_per_second': 21.351, 'eval_steps_per_second': 0.854, 'epoch': 1.03}\u001b[0m\n",
      " 52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã                    | 60/116 [25:13<22:37, 24.24s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-60)... \u001b[0mDone. 4.5s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6595, 'learning_rate': 0.00010592406278937144, 'epoch': 1.05}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6599, 'learning_rate': 0.00010296333278225599, 'epoch': 1.07}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6358, 'learning_rate': 0.0001, 'epoch': 1.09}\u001b[0m        \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6629, 'learning_rate': 9.703666721774402e-05, 'epoch': 1.1}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6514, 'learning_rate': 9.407593721062859e-05, 'epoch': 1.12}\u001b[0mm\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 65/116 [27:16<20:24, 24.01s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6857554912567139, 'eval_runtime': 3.5136, 'eval_samples_per_second': 21.346, 'eval_steps_per_second': 0.854, 'epoch': 1.12}\u001b[0m\n",
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå                  | 65/116 [27:20<20:24, 24.01s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.65, 'learning_rate': 9.112041046770653e-05, 'epoch': 1.14}\u001b[0m[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6758, 'learning_rate': 8.817268290786343e-05, 'epoch': 1.16}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6679, 'learning_rate': 8.523534359975189e-05, 'epoch': 1.17}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6512, 'learning_rate': 8.231097248774274e-05, 'epoch': 1.19}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6886, 'learning_rate': 7.940213812589018e-05, 'epoch': 1.21}\u001b[0mm\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 70/116 [29:16<18:12, 23.75s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6772447824478149, 'eval_runtime': 3.5136, 'eval_samples_per_second': 21.345, 'eval_steps_per_second': 0.854, 'epoch': 1.21}\u001b[0m\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé                | 70/116 [29:19<18:12, 23.75s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-70)... \u001b[0mDone. 4.9s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6713, 'learning_rate': 7.651139542190164e-05, 'epoch': 1.22}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6354, 'learning_rate': 7.364128339309326e-05, 'epoch': 1.24}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6542, 'learning_rate': 7.079432293630244e-05, 'epoch': 1.26}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6404, 'learning_rate': 6.797301461371625e-05, 'epoch': 1.28}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6428, 'learning_rate': 6.517983645656014e-05, 'epoch': 1.29}\u001b[0mm\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 75/116 [31:24<16:34, 24.27s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6739645600318909, 'eval_runtime': 3.5156, 'eval_samples_per_second': 21.334, 'eval_steps_per_second': 0.853, 'epoch': 1.29}\u001b[0m\n",
      " 65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè              | 75/116 [31:28<16:34, 24.27s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6615, 'learning_rate': 6.24172417885762e-05, 'epoch': 1.31}\u001b[0m0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6519, 'learning_rate': 5.96876570712028e-05, 'epoch': 1.33}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6212, 'learning_rate': 5.699347977234799e-05, 'epoch': 1.34}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6658, 'learning_rate': 5.43370762606287e-05, 'epoch': 1.36}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6778, 'learning_rate': 5.172077972692553e-05, 'epoch': 1.38}\u001b[0mm\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 80/116 [33:29<15:06, 25.18s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6691932082176208, 'eval_runtime': 3.5127, 'eval_samples_per_second': 21.351, 'eval_steps_per_second': 0.854, 'epoch': 1.38}\u001b[0m\n",
      " 69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ             | 80/116 [33:33<15:06, 25.18s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-80)... \u001b[0mDone. 4.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6079, 'learning_rate': 4.914688813507797e-05, 'epoch': 1.4}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6365, 'learning_rate': 4.661766220352097e-05, 'epoch': 1.41}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6444, 'learning_rate': 4.4135323419634766e-05, 'epoch': 1.43}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6416, 'learning_rate': 4.170205208855281e-05, 'epoch': 1.45}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6477, 'learning_rate': 3.931998541814069e-05, 'epoch': 1.47}\u001b[0mm\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 85/116 [35:43<13:29, 26.10s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6674001812934875, 'eval_runtime': 3.5145, 'eval_samples_per_second': 21.34, 'eval_steps_per_second': 0.854, 'epoch': 1.47}\u001b[0m\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä           | 85/116 [35:46<13:29, 26.10s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6205, 'learning_rate': 3.69912156418289e-05, 'epoch': 1.48}\u001b[0m0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6319, 'learning_rate': 3.471778818094785e-05, 'epoch': 1.5}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.621, 'learning_rate': 3.250169984817897e-05, 'epoch': 1.52}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6297, 'learning_rate': 3.034489709370033e-05, 'epoch': 1.53}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6187, 'learning_rate': 2.8249274295566864e-05, 'epoch': 1.55}\u001b[0m\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 90/116 [37:46<10:48, 24.95s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6632371544837952, 'eval_runtime': 3.5146, 'eval_samples_per_second': 21.339, 'eval_steps_per_second': 0.854, 'epoch': 1.55}\u001b[0m\n",
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå         | 90/116 [37:49<10:48, 24.95s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-90)... \u001b[0mDone. 4.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6143, 'learning_rate': 2.6216672095827266e-05, 'epoch': 1.57}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.5994, 'learning_rate': 2.4248875783837987e-05, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6513, 'learning_rate': 2.234761372819577e-05, 'epoch': 1.6}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6388, 'learning_rate': 2.0514555858664663e-05, 'epoch': 1.62}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6426, 'learning_rate': 1.875131219943187e-05, 'epoch': 1.64}\u001b[0mm\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 95/116 [40:00<08:51, 25.31s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6618602871894836, 'eval_runtime': 3.513, 'eval_samples_per_second': 21.349, 'eval_steps_per_second': 0.854, 'epoch': 1.64}\u001b[0m\n",
      " 82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç       | 95/116 [40:04<08:51, 25.31s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6199, 'learning_rate': 1.7059431454979824e-05, 'epoch': 1.66}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.616, 'learning_rate': 1.5440399649817385e-05, 'epoch': 1.67}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6057, 'learning_rate': 1.3895638823264446e-05, 'epoch': 1.69}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6056, 'learning_rate': 1.2426505780436326e-05, 'epoch': 1.71}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6328, 'learning_rate': 1.103429090052528e-05, 'epoch': 1.72}\u001b[0mm\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 100/116 [42:01<06:24, 24.04s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6609156131744385, 'eval_runtime': 3.5167, 'eval_samples_per_second': 21.327, 'eval_steps_per_second': 0.853, 'epoch': 1.72}\u001b[0m\n",
      " 86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé     | 100/116 [42:04<06:24, 24.04s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-100)... \u001b[0mDone. 4.3s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.632, 'learning_rate': 9.720217003425647e-06, 'epoch': 1.74}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.64, 'learning_rate': 8.485438275698154e-06, 'epoch': 1.76}\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6066, 'learning_rate': 7.331039256816663e-06, 'epoch': 1.78}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6938, 'learning_rate': 6.258033886587911e-06, 'epoch': 1.79}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.5785, 'learning_rate': 5.267364614580861e-06, 'epoch': 1.81}\u001b[0mm\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 105/116 [44:12<04:31, 24.70s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6608909368515015, 'eval_runtime': 3.514, 'eval_samples_per_second': 21.343, 'eval_steps_per_second': 0.854, 'epoch': 1.81}\u001b[0m\n",
      " 91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 105/116 [44:16<04:31, 24.70s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6235, 'learning_rate': 4.359901572347758e-06, 'epoch': 1.83}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6388, 'learning_rate': 3.5364418091641373e-06, 'epoch': 1.84}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6427, 'learning_rate': 2.7977085919589254e-06, 'epoch': 1.86}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.634, 'learning_rate': 2.144350770049597e-06, 'epoch': 1.88}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6041, 'learning_rate': 1.576942205240317e-06, 'epoch': 1.9}\u001b[0m0m\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 110/116 [46:15<02:22, 23.83s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.660330593585968, 'eval_runtime': 3.5192, 'eval_samples_per_second': 21.312, 'eval_steps_per_second': 0.852, 'epoch': 1.9}\u001b[0m\n",
      " 95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 110/116 [46:19<02:22, 23.83s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-110)... \u001b[0mDone. 4.5s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.647, 'learning_rate': 1.0959812677835968e-06, 'epoch': 1.91}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6561, 'learning_rate': 7.018903986483083e-07, 'epoch': 1.93}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6183, 'learning_rate': 3.950157384783104e-07, 'epoch': 1.95}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.5926, 'learning_rate': 1.7562682356786487e-07, 'epoch': 1.97}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6411, 'learning_rate': 4.391634912056519e-08, 'epoch': 1.98}\u001b[0mm\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 115/116 [48:22<00:24, 24.19s/it]\u001b[0m\u001b[0m\n",
      "  0%|                                                     | 0/3 [00:00<?, ?it/s]\u001b[0m\u001b[A\u001b[0m\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà               | 2/3 [00:01<00:00,  1.33it/s]\u001b[0m\u001b[A\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.6603356003761292, 'eval_runtime': 3.517, 'eval_samples_per_second': 21.325, 'eval_steps_per_second': 0.853, 'epoch': 1.98}\u001b[0m\n",
      " 99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 115/116 [48:25<00:24, 24.19s/it]\u001b[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:02<00:00,  1.51it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6556, 'learning_rate': 0.0, 'epoch': 2.0}\u001b[0m            \u001b[0m\u001b[0m[0m\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 116/116 [48:46<00:00, 24.30s/it]\u001b[0mThe intermediate checkpoints of PEFT may not be saved correctly, using `TrainerCallback` to save adapter_model.bin in corresponding folders, here are some examples https://github.com/huggingface/peft/issues/96\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 273, in <module>\n",
      "    fire.Fire(train)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 261, in train\n",
      "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1696, in train\n",
      "    return inner_training_loop(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2095, in _inner_training_loop\n",
      "    self._load_best_model()\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2292, in _load_best_model\n",
      "    self._issue_warnings_after_load(load_result)\n",
      "\u001b[0m\u001b[0mUnboundLocalError: local variable 'load_result' referenced before assignment\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime ‚ñÇ‚ñÅ‚ñÉ‚ñÅ‚ñÉ‚ñÑ‚ñÑ‚ñÜ‚ñÜ‚ñÜ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÜ‚ñÑ‚ñÖ‚ñÖ‚ñÑ‚ñá‚ñÖ‚ñà‚ñá\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second ‚ñà‚ñà‚ñÜ‚ñà‚ñÜ‚ñÖ‚ñÖ‚ñÉ‚ñÉ‚ñÉ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÖ‚ñÑ‚ñÑ‚ñÖ‚ñÇ‚ñÑ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second ‚ñà‚ñà‚ñÜ‚ñà‚ñà‚ñÜ‚ñÜ‚ñÉ‚ñÉ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÉ‚ñÜ‚ñÅ‚ñÉ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñÜ‚ñá‚ñá‚ñá‚ñá‚ñá‚ñá‚ñà‚ñà‚ñà\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ‚ñÇ‚ñÉ‚ñÖ‚ñá‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñá‚ñá‚ñá‚ñá‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss ‚ñà‚ñà‚ñá‚ñÜ‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÇ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss 0.66034\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime 3.517\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second 21.325\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second 0.853\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch 2.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step 116\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss 0.6556\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33methereal-blaze-8\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-qlora-gsm8k-16/runs/y84grpru\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 68 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230530_073119-y84grpru/logs\u001b[0m\n",
      "\u001b[0m\u001b[0mTraceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 928, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 588, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.9/bin/python3', 'scripts/finetune.py', 'examples/openllama/qlora.yml']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py examples/openllama/qlora.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are ad hoc cells handling issues during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  lsof\n",
      "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 253 kB of archives.\n",
      "After this operation, 458 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 lsof amd64 4.93.2+dfsg-1.1build2 [253 kB]\n",
      "Fetched 253 kB in 1s (364 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package lsof.\n",
      "(Reading database ... 21634 files and directories currently installed.)\n",
      "Preparing to unpack .../lsof_4.93.2+dfsg-1.1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking lsof (4.93.2+dfsg-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up lsof (4.93.2+dfsg-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install lsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND    PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n",
      "docker-in    1 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "bash         7 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    1u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    2u   CHR     1,3      0t0    6 /dev/null\n",
      "jupyter-l 2308 root    0r   CHR     1,3      0t0    6 /dev/null\n",
      "jupyter-l 2308 root   12r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2541 root    4r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2947 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   2947 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   2947 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   2947 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2947 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   2947 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   2947 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   2947 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  149u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  154u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   3545 root    4r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   4493 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   4493 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   4493 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   4493 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   4493 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   4493 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   4493 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   4493 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  146u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  150u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "sh        4950 root   10u   CHR     5,0      0t0   13 /dev/tty\n",
      "python3   5051 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   5051 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   5051 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   5051 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   5051 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   5051 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   5051 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   5051 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  146u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  150u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "tmux:\\x20 5801 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "tmux:\\x20 5801 root    1u   CHR     1,3      0t0    6 /dev/null\n",
      "tmux:\\x20 5801 root    2u   CHR     1,3      0t0    6 /dev/null\n",
      "nvitop    5817 root    3u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "nvitop    5817 root    4u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "nvitop    5817 root    5u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "nvitop    5817 root    6u   CHR   195,0      0t0  473 /dev/nvidia0\n"
     ]
    }
   ],
   "source": [
    "!lsof /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        2308  0.3  0.0 657788 121716 ?       Sl   06:23   0:12 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/jupyter-lab --allow-root --no-browser --port=8888 --ip=* --ServerApp.token=sc --ServerApp.allow_origin=* --ServerApp.preferred_dir=/workspace/\n",
      "root        2541  1.2  0.0 770888 63136 ?        Ssl  06:24   0:49 /root/miniconda3/envs/py3.9/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-b418b3cc-7364-4528-9692-c16146c0265d.json\n",
      "root        2947 17.2  1.5 28435920 8331824 ?    Sl   06:25  11:03 /root/miniconda3/envs/py3.9/bin/python3 scripts/finetune.py examples/openllama/qlora.yml\n",
      "root        3545  0.0  0.0 783972 75900 ?        Ssl  06:26   0:01 /root/miniconda3/envs/py3.9/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0f272296-97b6-48fa-8d30-86ffcc2af46e.json\n",
      "root        4493 15.0  1.5 34518728 8286092 ?    Sl   06:38   7:35 /root/miniconda3/envs/py3.9/bin/python3 scripts/finetune.py examples/openllama/qlora.yml\n",
      "root        5051 33.4  1.5 44973312 8286964 ?    Sl   07:12   5:30 /root/miniconda3/envs/py3.9/bin/python3 scripts/finetune.py examples/openllama/qlora.yml\n",
      "root        5817  2.8  0.0 314940 88792 pts/5    Sl+  07:24   0:08 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/nvitop -m full\n",
      "root        5933  0.0  0.0   2880   952 pts/4    Ss+  07:29   0:00 /usr/bin/sh -c ps aux|grep python\n",
      "root        5935  0.0  0.0   3836  2044 pts/4    S+   07:29   0:00 grep python\n"
     ]
    }
   ],
   "source": [
    "!ps aux|grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2947 4493 5051"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        2308  0.3  0.0 657788 121776 ?       Sl   06:23   0:12 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/jupyter-lab --allow-root --no-browser --port=8888 --ip=* --ServerApp.token=sc --ServerApp.allow_origin=* --ServerApp.preferred_dir=/workspace/\n",
      "root        2541  1.2  0.0 770888 63136 ?        Ssl  06:24   0:49 /root/miniconda3/envs/py3.9/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-b418b3cc-7364-4528-9692-c16146c0265d.json\n",
      "root        3545  0.0  0.0 783972 75900 ?        Ssl  06:26   0:01 /root/miniconda3/envs/py3.9/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-0f272296-97b6-48fa-8d30-86ffcc2af46e.json\n",
      "root        5817  2.6  0.0 314940 88792 pts/5    Sl+  07:24   0:08 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/nvitop -m full\n",
      "root        5946  0.0  0.0   2880  1044 pts/4    Ss+  07:30   0:00 /usr/bin/sh -c ps aux|grep python\n",
      "root        5948  0.0  0.0   3836  2048 pts/4    S+   07:30   0:00 grep python\n"
     ]
    }
   ],
   "source": [
    "!ps aux|grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install zip\n",
    "!zip -r last_run_prepared.zip -xi last_run_prepared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvitop -m full"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP0jHt4WNuLaC5ecmX0YtWl",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
