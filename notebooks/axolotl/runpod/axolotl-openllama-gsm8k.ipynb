{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zJvyUmZdktu0",
    "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/axolotl\n"
     ]
    }
   ],
   "source": [
    "%cd /workspace/axolotl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mC48y25Lkqa5",
    "outputId": "2757a3c8-3790-4fd3-be39-03be8f533b35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!accelerate config --config_file configs/accelerate/default_config.yaml default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Based on https://gist.github.com/fearnworks/723709806cebc67bafe1eb8138e7efbd\n",
      "base_model: openlm-research/open_llama_7b_700bt_preview\n",
      "base_model_config: openlm-research/open_llama_7b_700bt_preview\n",
      "model_type: LlamaForCausalLM\n",
      "tokenizer_type: LlamaTokenizer\n",
      "load_in_8bit: false\n",
      "load_in_4bit: true\n",
      "strict: false\n",
      "push_dataset_to_hub:\n",
      "datasets:\n",
      "  # - path: AtlasUnified/Code-Instruct-Sets\n",
      "  #   data_files:\n",
      "  #     - unmasked-set-1.jsonl\n",
      "  #     - unmasked-set-2.jsonl\n",
      "  #     - unmasked-set-3.jsonl\n",
      "  #     - unmasked-set-4.jsonl\n",
      "  #   type: alpaca_code_instruct\n",
      "  # - path: winglian/pygmalion-cleaned\n",
      "  #   data_files:\n",
      "  #     - v13_no_ai.cleaned.jsonl\n",
      "  #   type: pygmalion\n",
      "  #   shards: 4\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - hf/ARC-Challenge.jsonl\n",
      "  #     - hf/ARC-Easy.jsonl\n",
      "  #     - hf/riddle_sense.jsonl\n",
      "  #   type: explainchoice:chat\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - hf/gsm8k.jsonl\n",
      "  #     - custom/logic_inference_oa.jsonl\n",
      "  #   type: alpaca_chat.load_qa\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - custom/in_context_qa.jsonl\n",
      "  #   type: context_qa\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - custom/in_context_qa.jsonl\n",
      "  #   type: context_qa.load_404\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - custom/jokes_explained_500up.jsonl\n",
      "  #   type: sharegpt_jokes\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - custom/classify-self-chat.sharegpt.jsonl\n",
      "  #     - custom/coding-self-chat.sharegpt.jsonl\n",
      "  #     - custom/prose-gpt4.sharegpt.jsonl\n",
      "  #     - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
      "  #   type: sharegpt_simple\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - custom/guanaco-cleaned.en.jsonl\n",
      "  #   type: sharegpt_simple.load_guanaco\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - openai/tldr.jsonl\n",
      "  #   type: summarizetldr:chat\n",
      "  # - path: winglian/evals\n",
      "  #   data_files:\n",
      "  #     - hellaswag/hellaswag.jsonl\n",
      "  #   type: explainchoice:chat\n",
      "  #   shards: 60\n",
      "  # - path: metaeval/ScienceQA_text_only\n",
      "  #   type: concisechoice:chat\n",
      "  #   shards: 13\n",
      "  # - path: teknium/GPTeacher-General-Instruct\n",
      "  #   data_files: \n",
      "  #     - gpt4-instruct-similarity-0.6-dataset.json\n",
      "  #   type: gpteacher:chat\n",
      "  - path: QingyiSi/Alpaca-CoT\n",
      "    data_files:\n",
      "      # - chain-of-thought/formatted_cot_data/aqua_train.jsonl\n",
      "      # - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
      "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
      "      # - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
      "      # - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
      "    type: \"alpaca:chat\"\n",
      "dataset_prepared_path: last_run_prepared\n",
      "val_set_size: 0.01\n",
      "adapter: qlora\n",
      "lora_model_dir:\n",
      "sequence_len: 2048\n",
      "max_packed_sequence_len: 2048\n",
      "lora_r: 64\n",
      "lora_alpha: 32\n",
      "lora_dropout: 0.000001\n",
      "lora_target_modules:\n",
      "lora_target_linear: true\n",
      "lora_fan_in_fan_out:\n",
      "wandb_project: openllama-7b-qlora-gsm8k\n",
      "wandb_watch:\n",
      "wandb_run_id:\n",
      "wandb_log_model: checkpoint\n",
      "output_dir: ./qlora-out\n",
      "batch_size: 36\n",
      "micro_batch_size: 12\n",
      "num_epochs: 2\n",
      "optimizer: paged_adamw_32bit\n",
      "torchdistx_path:\n",
      "lr_scheduler: cosine\n",
      "learning_rate: 0.0001\n",
      "train_on_inputs: false\n",
      "group_by_length: false\n",
      "bf16: true\n",
      "fp16: false\n",
      "tf32: true\n",
      "gradient_checkpointing: true\n",
      "early_stopping_patience:\n",
      "resume_from_checkpoint:\n",
      "auto_resume_from_checkpoints: true\n",
      "local_rank:\n",
      "logging_steps: 1\n",
      "xformers_attention: true\n",
      "flash_attention:\n",
      "gptq_groupsize:\n",
      "gptq_model_v1:\n",
      "warmup_steps: 10\n",
      "eval_steps: 5\n",
      "save_steps: 10\n",
      "debug:\n",
      "deepspeed:\n",
      "weight_decay: 0.000001\n",
      "fsdp:\n",
      "fsdp_config:\n",
      "special_tokens:\n",
      "  bos_token: \"<s>\"\n",
      "  eos_token: \"</s>\"\n",
      "  unk_token: \"<unk>\"\n"
     ]
    }
   ],
   "source": [
    "!cat examples/openllama/qlora.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "jICMPJuomFsx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting ds_accelerator to cuda (auto detect)\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
      "  warn(msg)\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIK1tFOFrWbmoa2ckCJYhzgBHKTSMeR/AeuScCCzugqlI utensilcandel@gmail.com')}\n",
      "  warn(msg)\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
      "Either way, this might cause trouble in the future:\n",
      "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
      "  warn(msg)\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
      "Setting ds_accelerator to cuda (auto detect)\n",
      "INFO:root:loading tokenizer...\n",
      "Using pad_token, but it is not set yet.\n",
      "INFO:root:Loading prepared packed dataset from disk at last_run_prepared/21a0611c6c2b67b31f00097fa2a91c26...\n",
      "INFO:root:Prepared packed dataset loaded from disk...\n",
      "INFO:root:loading model and peft_config...\n",
      "INFO:root:patching with xformers attention\n",
      "Replaced attention with xformers_attention\u001b[0m\n",
      "Loading checkpoint shards: 100%|██████████████████| 2/2 [00:22<00:00, 11.10s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0mINFO:root:converting PEFT model w/ prepare_model_for_int8_training\n",
      "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/utils/other.py:76: FutureWarning: prepare_model_for_int8_training is deprecated and will be removed in a future version. Use prepare_model_for_kbit_training instead.\n",
      "  warnings.warn(\n",
      "\u001b[0mINFO:root:found linear modules: ['up_proj', 'k_proj', 'o_proj', 'q_proj', 'down_proj', 'v_proj', 'gate_proj']\n",
      "trainable params: 159907840 || all params: 3660320768 || trainable%: 4.368683788535114\u001b[0m\n",
      "INFO:root:Compiling torch model\n",
      "\u001b[0mINFO:root:Pre-saving adapter config to ./qlora-out\n",
      "INFO:root:Starting trainer...\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230530_132151-h09sclfq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mwarm-jazz-4\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-7b-qlora-gsm8k\u001b[0m\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-7b-qlora-gsm8k/runs/h09sclfq\u001b[0m\n",
      "\u001b[0m\u001b[0m{'loss': 0.9761, 'learning_rate': 1e-05, 'epoch': 0.05}\u001b[0m             \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9798, 'learning_rate': 2e-05, 'epoch': 0.09}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9884, 'learning_rate': 3e-05, 'epoch': 0.14}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9763, 'learning_rate': 4e-05, 'epoch': 0.19}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9544, 'learning_rate': 5e-05, 'epoch': 0.23}\u001b[0m         \u001b[0m\u001b[0m\n",
      " 12%|█████▏                                      | 5/42 [06:15<46:19, 75.13s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.9713916182518005, 'eval_runtime': 4.5052, 'eval_samples_per_second': 1.776, 'eval_steps_per_second': 0.222, 'epoch': 0.23}\u001b[0m\n",
      " 12%|█████▏                                      | 5/42 [06:20<46:19, 75.13s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.77it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.945, 'learning_rate': 6e-05, 'epoch': 0.28}\u001b[0m          \u001b[0m\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.9351, 'learning_rate': 7e-05, 'epoch': 0.33}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.939, 'learning_rate': 8e-05, 'epoch': 0.38}\u001b[0m          \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8687, 'learning_rate': 9e-05, 'epoch': 0.42}\u001b[0m         \u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8673, 'learning_rate': 0.0001, 'epoch': 0.47}\u001b[0m        \u001b[0m\u001b[0m\n",
      " 24%|██████████▏                                | 10/42 [12:36<40:17, 75.55s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8864110708236694, 'eval_runtime': 4.5046, 'eval_samples_per_second': 1.776, 'eval_steps_per_second': 0.222, 'epoch': 0.47}\u001b[0m\n",
      " 24%|██████████▏                                | 10/42 [12:41<40:17, 75.55s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.76it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-10)... \u001b[0mDone. 8.5s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8534, 'learning_rate': 9.975923633360985e-05, 'epoch': 0.52}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8569, 'learning_rate': 9.903926402016153e-05, 'epoch': 0.56}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8359, 'learning_rate': 9.784701678661045e-05, 'epoch': 0.61}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.832, 'learning_rate': 9.619397662556435e-05, 'epoch': 0.66}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.8159, 'learning_rate': 9.409606321741775e-05, 'epoch': 0.7}\u001b[0m0m\n",
      " 36%|███████████████▎                           | 15/42 [19:11<34:30, 76.68s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.8275843858718872, 'eval_runtime': 4.53, 'eval_samples_per_second': 1.766, 'eval_steps_per_second': 0.221, 'epoch': 0.7}\u001b[0m\n",
      " 36%|███████████████▎                           | 15/42 [19:16<34:30, 76.68s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.71it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7892, 'learning_rate': 9.157348061512727e-05, 'epoch': 0.75}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7924, 'learning_rate': 8.865052266813685e-05, 'epoch': 0.8}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.799, 'learning_rate': 8.535533905932738e-05, 'epoch': 0.84}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7749, 'learning_rate': 8.171966420818228e-05, 'epoch': 0.89}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7811, 'learning_rate': 7.777851165098012e-05, 'epoch': 0.94}\u001b[0mm\n",
      " 48%|████████████████████▍                      | 20/42 [25:33<27:52, 76.00s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7880770564079285, 'eval_runtime': 4.5175, 'eval_samples_per_second': 1.771, 'eval_steps_per_second': 0.221, 'epoch': 0.94}\u001b[0m\n",
      " 48%|████████████████████▍                      | 20/42 [25:37<27:52, 76.00s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.72it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-20)... \u001b[0mDone. 9.2s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7833, 'learning_rate': 7.35698368412999e-05, 'epoch': 0.98}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7587, 'learning_rate': 6.91341716182545e-05, 'epoch': 1.03}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7265, 'learning_rate': 6.451423386272312e-05, 'epoch': 1.08}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7492, 'learning_rate': 5.9754516100806423e-05, 'epoch': 1.12}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.748, 'learning_rate': 5.490085701647805e-05, 'epoch': 1.17}\u001b[0m0m\n",
      " 60%|█████████████████████████▌                 | 25/42 [31:58<21:28, 75.77s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7627310752868652, 'eval_runtime': 4.5212, 'eval_samples_per_second': 1.769, 'eval_steps_per_second': 0.221, 'epoch': 1.17}\u001b[0m\n",
      " 60%|█████████████████████████▌                 | 25/42 [32:02<21:28, 75.77s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.71it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7563, 'learning_rate': 5e-05, 'epoch': 1.22}\u001b[0m         \u001b[0m\u001b[0m[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7462, 'learning_rate': 4.509914298352197e-05, 'epoch': 1.27}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7474, 'learning_rate': 4.0245483899193595e-05, 'epoch': 1.31}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7416, 'learning_rate': 3.5485766137276894e-05, 'epoch': 1.36}\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7199, 'learning_rate': 3.086582838174551e-05, 'epoch': 1.41}\u001b[0mm\n",
      " 71%|██████████████████████████████▋            | 30/42 [38:20<15:10, 75.91s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7485314011573792, 'eval_runtime': 4.5169, 'eval_samples_per_second': 1.771, 'eval_steps_per_second': 0.221, 'epoch': 1.41}\u001b[0m\n",
      " 71%|██████████████████████████████▋            | 30/42 [38:25<15:10, 75.91s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.73it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-30)... \u001b[0mDone. 9.2s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7203, 'learning_rate': 2.6430163158700115e-05, 'epoch': 1.45}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7279, 'learning_rate': 2.2221488349019903e-05, 'epoch': 1.5}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.73, 'learning_rate': 1.8280335791817733e-05, 'epoch': 1.55}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7017, 'learning_rate': 1.4644660940672627e-05, 'epoch': 1.59}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7203, 'learning_rate': 1.134947733186315e-05, 'epoch': 1.64}\u001b[0mm\n",
      " 83%|███████████████████████████████████▊       | 35/42 [44:55<08:56, 76.64s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7415170073509216, 'eval_runtime': 4.5141, 'eval_samples_per_second': 1.772, 'eval_steps_per_second': 0.222, 'epoch': 1.64}\u001b[0m\n",
      " 83%|███████████████████████████████████▊       | 35/42 [44:59<08:56, 76.64s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.70it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7033, 'learning_rate': 8.426519384872733e-06, 'epoch': 1.69}\u001b[0mm[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7364, 'learning_rate': 5.903936782582253e-06, 'epoch': 1.73}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7107, 'learning_rate': 3.8060233744356633e-06, 'epoch': 1.78}\u001b[0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7261, 'learning_rate': 2.152983213389559e-06, 'epoch': 1.83}\u001b[0mm\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.6897, 'learning_rate': 9.607359798384785e-07, 'epoch': 1.88}\u001b[0mm\n",
      " 95%|████████████████████████████████████████▉  | 40/42 [51:17<02:32, 76.04s/it]\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0m                                                                        \u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[A\u001b[0m{'eval_loss': 0.7393863201141357, 'eval_runtime': 4.5227, 'eval_samples_per_second': 1.769, 'eval_steps_per_second': 0.221, 'epoch': 1.88}\u001b[0m\n",
      " 95%|████████████████████████████████████████▉  | 40/42 [51:21<02:32, 76.04s/it]\u001b[0m\n",
      "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00,  6.74it/s]\u001b[0m\u001b[A\u001b[0m\u001b[0m\n",
      "                                                                                \u001b[0m\u001b[A\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Adding directory to artifact (./qlora-out/checkpoint-40)... \u001b[0mDone. 9.4s\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.717, 'learning_rate': 2.407636663901591e-07, 'epoch': 1.92}\u001b[0m0m\n",
      "\u001b[0m\u001b[0m\u001b[0m{'loss': 0.7106, 'learning_rate': 0.0, 'epoch': 1.97}\u001b[0m           \u001b[0m\u001b[0m\n",
      "100%|███████████████████████████████████████████| 42/42 [54:07<00:00, 79.75s/it]\u001b[0mThe intermediate checkpoints of PEFT may not be saved correctly, using `TrainerCallback` to save adapter_model.bin in corresponding folders, here are some examples https://github.com/huggingface/peft/issues/96\n",
      "\u001b[0mTraceback (most recent call last):\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 273, in <module>\n",
      "    fire.Fire(train)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
      "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
      "    component, remaining_args = _CallAndUpdateTrace(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
      "    component = fn(*varargs, **kwargs)\n",
      "\u001b[0m\u001b[0m  File \"/workspace/axolotl/scripts/finetune.py\", line 261, in train\n",
      "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1696, in train\n",
      "    return inner_training_loop(\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2095, in _inner_training_loop\n",
      "    self._load_best_model()\n",
      "\u001b[0m\u001b[0m  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2292, in _load_best_model\n",
      "    self._issue_warnings_after_load(load_result)\n",
      "\u001b[0m\u001b[0mUnboundLocalError: local variable 'load_result' referenced before assignment\n",
      "\u001b[0m\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss █▅▄▂▂▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime ▁▁█▅▆▄▄▆\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second ██▁▅▃▅▅▃\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second ██▁▁▁▁█▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step ▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate ▂▂▃▄▅▅▆▇▇██████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss ████▇▇▇▇▅▅▅▅▄▄▄▃▃▄▃▃▃▂▂▂▃▂▂▂▂▂▂▂▁▂▁▂▁▂▁▁\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:               eval/loss 0.73939\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:            eval/runtime 4.5227\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: eval/samples_per_second 1.769\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   eval/steps_per_second 0.221\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:             train/epoch 1.97\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:       train/global_step 42\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:     train/learning_rate 0.0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:              train/loss 0.7106\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mwarm-jazz-4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/openllama-7b-qlora-gsm8k/runs/h09sclfq\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 24 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230530_132151-h09sclfq/logs\u001b[0m\n",
      "\u001b[0m\u001b[0mTraceback (most recent call last):\n",
      "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
      "    sys.exit(main())\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
      "    args.func(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 928, in launch_command\n",
      "    simple_launcher(args)\n",
      "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 588, in simple_launcher\n",
      "    raise subprocess.CalledProcessError(returncode=process.returncode, cmd=cmd)\n",
      "subprocess.CalledProcessError: Command '['/root/miniconda3/envs/py3.9/bin/python3', 'scripts/finetune.py', 'examples/openllama/qlora.yml']' returned non-zero exit status 1.\n"
     ]
    }
   ],
   "source": [
    "!accelerate launch scripts/finetune.py examples/openllama/qlora.yml"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are ad hoc cells handling issues during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  lsof\n",
      "0 upgraded, 1 newly installed, 0 to remove and 49 not upgraded.\n",
      "Need to get 253 kB of archives.\n",
      "After this operation, 458 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 lsof amd64 4.93.2+dfsg-1.1build2 [253 kB]\n",
      "Fetched 253 kB in 1s (364 kB/s)0m\u001b[33m\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "\n",
      "\u001b7\u001b[0;23r\u001b8\u001b[1ASelecting previously unselected package lsof.\n",
      "(Reading database ... 21634 files and directories currently installed.)\n",
      "Preparing to unpack .../lsof_4.93.2+dfsg-1.1build2_amd64.deb ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [  0%]\u001b[49m\u001b[39m [..........................................................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 20%]\u001b[49m\u001b[39m [###########...............................................] \u001b8Unpacking lsof (4.93.2+dfsg-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 40%]\u001b[49m\u001b[39m [#######################...................................] \u001b8Setting up lsof (4.93.2+dfsg-1.1build2) ...\n",
      "\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 60%]\u001b[49m\u001b[39m [##################################........................] \u001b8\u001b7\u001b[24;0f\u001b[42m\u001b[30mProgress: [ 80%]\u001b[49m\u001b[39m [##############################################............] \u001b8\n",
      "\u001b7\u001b[0;24r\u001b8\u001b[1A\u001b[J"
     ]
    }
   ],
   "source": [
    "!apt install lsof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COMMAND    PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n",
      "docker-in    1 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "bash         7 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    1u   CHR     1,3      0t0    6 /dev/null\n",
      "sshd        19 root    2u   CHR     1,3      0t0    6 /dev/null\n",
      "jupyter-l 2308 root    0r   CHR     1,3      0t0    6 /dev/null\n",
      "jupyter-l 2308 root   12r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2541 root    4r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2947 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   2947 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   2947 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   2947 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   2947 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   2947 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   2947 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   2947 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  149u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   2947 root  154u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   3545 root    4r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   4493 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   4493 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   4493 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   4493 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   4493 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   4493 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   4493 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   4493 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  146u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  150u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   4493 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "sh        4950 root   10u   CHR     5,0      0t0   13 /dev/tty\n",
      "python3   5051 root  mem    CHR 195,255           472 /dev/nvidiactl\n",
      "python3   5051 root  mem    CHR   195,0           473 /dev/nvidia0\n",
      "python3   5051 root  mem    CHR   234,0           481 /dev/nvidia-uvm\n",
      "python3   5051 root    3r   CHR     1,9      0t0   11 /dev/urandom\n",
      "python3   5051 root  132u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   5051 root  133u   CHR   234,0      0t0  481 /dev/nvidia-uvm\n",
      "python3   5051 root  134u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  135u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  136u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  139u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "python3   5051 root  140u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  141u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  142u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  145u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  146u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  147u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  148u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  150u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  151u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  152u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "python3   5051 root  153u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "tmux:\\x20 5801 root    0u   CHR     1,3      0t0    6 /dev/null\n",
      "tmux:\\x20 5801 root    1u   CHR     1,3      0t0    6 /dev/null\n",
      "tmux:\\x20 5801 root    2u   CHR     1,3      0t0    6 /dev/null\n",
      "nvitop    5817 root    3u   CHR 195,255      0t0  472 /dev/nvidiactl\n",
      "nvitop    5817 root    4u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "nvitop    5817 root    5u   CHR   195,0      0t0  473 /dev/nvidia0\n",
      "nvitop    5817 root    6u   CHR   195,0      0t0  473 /dev/nvidia0\n"
     ]
    }
   ],
   "source": [
    "!lsof /dev/nvidia*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kill -9 2960 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root        2353  0.7  0.0 576260 110108 ?       Sl   12:51   0:05 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/jupyter-lab --allow-root --no-browser --port=8888 --ip=* --ServerApp.token=sc --ServerApp.allow_origin=* --ServerApp.preferred_dir=/workspace/\n",
      "root        2636  1.6  0.0 770824 63020 ?        Ssl  12:52   0:12 /root/miniconda3/envs/py3.9/bin/python3 -m ipykernel_launcher -f /root/.local/share/jupyter/runtime/kernel-b2638c7c-467b-4866-a969-c97f1b037796.json\n",
      "root        3776  3.5  0.0 316080 90152 pts/2    Sl+  12:55   0:19 /root/miniconda3/envs/py3.9/bin/python3 /root/miniconda3/envs/py3.9/bin/nvitop -m full\n",
      "root        5019  0.0  0.0   2880   952 pts/3    Ss+  13:04   0:00 /usr/bin/sh -c ps aux|grep python\n",
      "root        5022  0.0  0.0   3836  1968 pts/3    S+   13:04   0:00 grep python\n"
     ]
    }
   ],
   "source": [
    "!ps aux|grep python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install zip\n",
    "!zip -r last_run_prepared.zip -xi last_run_prepared"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!pip install nvitop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvitop -m full"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyP0jHt4WNuLaC5ecmX0YtWl",
   "gpuType": "T4",
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
