{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/notebooks/axolotl/runpod/axolotl-falcon-40b-qlora-deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpbQuaxDY7H"
      },
      "source": [
        "<!-- https://jupyterlab.readthedocs.io/en/stable/user/commands.html#commands-list -->\n",
        "<button data-commandLinker-command=\"apputils:change-theme\" data-commandlinker-args='{\"theme\": \"JupyterLab Dark\"}' href=\"#\">Dark theme</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/\"}' href=\"#\">llm-playground</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/\"}' href=\"#\">axolotl</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/notebooks/axolotl/runpod\"}' href=\"#\">Runpod notebooks</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples\"}' href=\"#\">axolotl configs</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/storage\"}' href=\"#\">Storage</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/axolotl-trained\"}' href=\"#\">axolotl-trained</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples/falcon/config-40b-qlora.yml\"}' href=\"#\">Edit qlora config</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/ds_config.json\"}' href=\"#\">Edit ds config</button>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yoSiRvDY7G"
      },
      "source": [
        "# Finetuning falcon-40b\n",
        "\n",
        "- Axolotl+QLoRA\n",
        "- minotaur datasets\n",
        "- deepspeed ZeRO 3 8xGPU\n",
        "\n",
        "Entry script:\n",
        "\n",
        "https://github.com/utensil/llm-playground/blob/main/scripts/entry/ax_lite.sh\n",
        "\n",
        "Mount volume disk to `/content`\n",
        "\n",
        "Save to:\n",
        "\n",
        "- repo: `utensil/llm-playground`\n",
        "- path: `notebooks/axolotl/runpod/axolotl-falcon-40b-qlora-deepspeed.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZd-cf70HM2n"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrtaRkauKLL"
      },
      "source": [
        "### Set HF Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIyW4yWuHvd"
      },
      "outputs": [],
      "source": [
        "# %env HF_DATASETS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOfHUYQduRX"
      },
      "outputs": [],
      "source": [
        "#%env TRANSFORMERS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR9oblHTduRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaKgWKnduRY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJMsnPnOduRY"
      },
      "outputs": [],
      "source": [
        "!ln -s /content/cache /root/.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZ6RzFyiAue"
      },
      "source": [
        "### Speed up model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huZ5eZUUgJ76",
        "outputId": "82339f22-55ef-441f-9af4-c20e2edaf575"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease                         \n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [108 kB]\n",
            "Get:6 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [458 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1181 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [459 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [866 kB]\n",
            "Fetched 3301 kB in 2s (2199 kB/s)                        \n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libaria2-0 libc-ares2 libicu70 libssh2-1 libxml2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libaria2-0 libc-ares2 libicu70 libssh2-1 libxml2\n",
            "0 upgraded, 6 newly installed, 0 to remove and 56 not upgraded.\n",
            "Need to get 13.0 MB of archives.\n",
            "After this operation, 43.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libicu70 amd64 70.1-2 [10.6 MB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libxml2 amd64 2.9.13+dfsg-1ubuntu0.3 [763 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libc-ares2 amd64 1.18.1-1ubuntu0.22.04.2 [45.0 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libssh2-1 amd64 1.10.0-3 [109 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libaria2-0 amd64 1.36.0-1 [1086 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/universe amd64 aria2 amd64 1.36.0-1 [381 kB]\n",
            "Fetched 13.0 MB in 1s (25.2 MB/s)\n",
            "debconf: delaying package configuration, since apt-utils is not installed\n",
            "Selecting previously unselected package libicu70:amd64.\n",
            "(Reading database ... 17816 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libicu70_70.1-2_amd64.deb ...\n",
            "Unpacking libicu70:amd64 (70.1-2) ...\n",
            "Selecting previously unselected package libxml2:amd64.\n",
            "Preparing to unpack .../1-libxml2_2.9.13+dfsg-1ubuntu0.3_amd64.deb ...\n",
            "Unpacking libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "Preparing to unpack .../2-libc-ares2_1.18.1-1ubuntu0.22.04.2_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Selecting previously unselected package libssh2-1:amd64.\n",
            "Preparing to unpack .../3-libssh2-1_1.10.0-3_amd64.deb ...\n",
            "Unpacking libssh2-1:amd64 (1.10.0-3) ...\n",
            "Selecting previously unselected package libaria2-0:amd64.\n",
            "Preparing to unpack .../4-libaria2-0_1.36.0-1_amd64.deb ...\n",
            "Unpacking libaria2-0:amd64 (1.36.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../5-aria2_1.36.0-1_amd64.deb ...\n",
            "Unpacking aria2 (1.36.0-1) ...\n",
            "Setting up libc-ares2:amd64 (1.18.1-1ubuntu0.22.04.2) ...\n",
            "Setting up libssh2-1:amd64 (1.10.0-3) ...\n",
            "Setting up libicu70:amd64 (70.1-2) ...\n",
            "Setting up libxml2:amd64 (2.9.13+dfsg-1ubuntu0.3) ...\n",
            "Setting up libaria2-0:amd64 (1.36.0-1) ...\n",
            "Setting up aria2 (1.36.0-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.1) ...\n",
            "Updated git hooks.\n",
            "Git LFS initialized.\n",
            "Requirement already satisfied: requests in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (2.31.0)\n",
            "Requirement already satisfied: huggingface_hub in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (0.15.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests) (3.1.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests) (2.0.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests) (2023.5.7)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (3.12.2)\n",
            "Requirement already satisfied: fsspec in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (2023.6.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (4.6.3)\n",
            "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from huggingface_hub) (23.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y aria2\n",
        "!git lfs install\n",
        "!pip install requests huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqzd-Ul4iMPf"
      },
      "outputs": [],
      "source": [
        "#%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmJ7tmaIifTR"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/utensil/llm-playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29dpWyRujiSZ",
        "outputId": "2b966936-d384-41e8-d24a-0cdc9a0fd78b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/llm-playground\n"
          ]
        }
      ],
      "source": [
        "%cd /content/llm-playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8nAsJ4ggJ77",
        "scrolled": true,
        "outputId": "3a194d34-cb4e-4330-c122-50b151dbbe45"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Working directory changed to: /content/llm-playground/helper/..\n",
            "Skipping: .gitattributes\n",
            "Downloading the model to models/tiiuae_falcon-40b\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/README.md -d models/tiiuae_falcon-40b -o README.md\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/config.json -d models/tiiuae_falcon-40b -o config.json\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/configuration_RW.py -d models/tiiuae_falcon-40b -o configuration_RW.py\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/generation_config.json -d models/tiiuae_falcon-40b -o generation_config.json\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/modelling_RW.py -d models/tiiuae_falcon-40b -o modelling_RW.py\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00001-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00001-of-00009.bin\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00002-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00002-of-00009.bin\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00003-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00003-of-00009.bin\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "  0%|                                                    | 0/18 [00:00<?, ?it/s]\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/config.json\n",
            "\n",
            "\n",
            "Download Results:\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/generation_config.json\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "6637a7|\u001b[1;32mOK\u001b[0m  |   459KiB/s|models/tiiuae_falcon-40b/config.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/README.md\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c5b35f|\u001b[1;32mOK\u001b[0m  |   108KiB/s|models/tiiuae_falcon-40b/generation_config.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/modelling_RW.py\n",
            "81363e|\u001b[1;32mOK\u001b[0m  |    10MiB/s|models/tiiuae_falcon-40b/README.md\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "fe862a|\u001b[1;32mOK\u001b[0m  |    44MiB/s|models/tiiuae_falcon-40b/modelling_RW.py\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/configuration_RW.py\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "d1376f|\u001b[1;32mOK\u001b[0m  |   2.3MiB/s|models/tiiuae_falcon-40b/configuration_RW.py\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00004-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00004-of-00009.bin\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00005-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00005-of-00009.bin\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00006-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00006-of-00009.bin\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083790&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM3OTB9fX1dfQ__&Signature=HnJft2O8EtF4YRHLD3buN1bnSM18f1HTV%7E80iIWRWGqJGLc24hIXQOUBLbVqiuZ86vUhuXjEtYss-9OccCFQQ3QkpyRtU32DA8395ninT9oybkC0xE5i5GciujZq5OGTDM1vxZ5SQLc350ucvloF5gnJFyJt6f3sa%7E9bLB45X9wl1Hb%7ER%7EilCJiSXgteG6ZMY4XqTc18MxHUx1cNUWNU8gqr3e-9c950rytGlCRqaYbvAfDRFRScjEq07uo0Z5mfBmJbUBkValCv61gP6Al1pbkFpieLrMur9Adotg6L7jgfc56X0oNUtzZB2EL2vuojAjQekyg7F7p%7ETCNCqMxWpQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00007-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00007-of-00009.bin\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00008-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00008-of-00009.bin\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:08 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083310&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMzMTB9fX1dfQ__&Signature=IPEZZhWqIRCZEKEOLSB0nV1QnVQlE4gDgJ24yCzUnziD0Y77WNApeNAWmyFl%7EEQWPiOxDkiechknhDN9RQ37ozyxKEaK42vZzMlXRIC%7EQWYN%7EFZCt616QiMfm%7EoGdh4wLmVYgYu431n2njeIDZIhGAXF01SPC8ZkLZo3PzHxNzS%7EU9dtZMnSKrAtuBLEEwwXPkrWkYHeI4XlA8avyM-HXdYygTw75avc-oHusr-2CO5B0OVISR4xoMUCXlfCRNMvI4YngfCKHLd01YyRaeOCxN8l7ycXE0kFOsC8bggr4F9YVN2cbsWc-smmA1Ab0aoEAn2Ib31BJHurNRY8omJMsg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082451&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0NTF9fX1dfQ__&Signature=czCfzfRsSuWRTspYjr1v3gCBzyOlFz82El8-E%7E3a1cg0lejscVOZowglhmO0GxEu8xi5Y5i%7Ed061VHinMJsXYv6icAoKSBwmboS52ZaKh8krsKYKYa7k9Ff2z13uRZQJ6WL7JVa2rrqKG-gFVtPd6mBimFjzUpHm%7E%7E%7EDcGxaYLSJoJ-ux-0bHVHltInKrV2l9t5N0uAhMxvbv5Kd023neA%7EyBsMZgaSAupO8iTGA%7EilBSYF8-5Dwp-XZbTmMInKicBJAYOpZ7tdsmVE11GDllhXo3eHb63j-gqNQxa%7EUHcvgaAk%7EAvwW-TWjvxvUqPFpyL4M8uCzMl8ReZxqZrJKOw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082406&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0MDZ9fX1dfQ__&Signature=umKgbzUD1PmGROwg51SickGT4zvbh1Dz3QE%7EyPzxaxtcXs-7BV24GThpN1U2epinEZvLz%7ESEieDkCKgL46C6cXc-FnsEQtqfjxJMUp%7Ezp4IsaYej1OKf6lCnOAnelrz1cPaGFt7fxJMzwEn3ADQVvFG7YibxtNYVps6p1xF8lgtpiDhGu-e4HkQ75HhYtyAd6Gg5ROW55x10HO%7EFdpRy1XW8Iy-MK3DUf9JZGlUM3ZbcdkivzVIWYlCFy7VfgCXBG6KB5Pdfm9pFXWBMZ%7EAlvGkixQkHdWmU9J5iy69VgmtZxc5DaIFuVxdmD%7EdXc7PNy4KIx4Uv1qVctZs-CEoZrg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083475&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NzV9fX1dfQ__&Signature=tdcrIRLZDKb2pSVsTpK9NnUOpd%7Euc0LMkmXqvfFh1VAV7hHjLCjtewYVl-FI90pYXpnvfQEa2lXYCj-xI2nqbn51%7Ecbn6V9PncbivS%7Eq9tszmxWaCDdYNFS1MuotxXIGykcW-clYJQ2uKjwgaxrWvLk6UR3idmc0fiFZ8cKvci8NEuK8CHF9Vw9FPkF2jCNBXFFsf4JCDkuE9IBuWsWHhI1T0YaIxwT%7EN0PYU7Bqng7aHnb0hlV1M7Eo6GKu5OD36fDnXETtncFj0iW27pCTW5pACNmi5hUf9YiZurqcn10JKZhBQGkZUW3ZnClQJI1Gi3bY46zNGPgB0OJuOo83cA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082406&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0MDZ9fX1dfQ__&Signature=umKgbzUD1PmGROwg51SickGT4zvbh1Dz3QE%7EyPzxaxtcXs-7BV24GThpN1U2epinEZvLz%7ESEieDkCKgL46C6cXc-FnsEQtqfjxJMUp%7Ezp4IsaYej1OKf6lCnOAnelrz1cPaGFt7fxJMzwEn3ADQVvFG7YibxtNYVps6p1xF8lgtpiDhGu-e4HkQ75HhYtyAd6Gg5ROW55x10HO%7EFdpRy1XW8Iy-MK3DUf9JZGlUM3ZbcdkivzVIWYlCFy7VfgCXBG6KB5Pdfm9pFXWBMZ%7EAlvGkixQkHdWmU9J5iy69VgmtZxc5DaIFuVxdmD%7EdXc7PNy4KIx4Uv1qVctZs-CEoZrg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083135&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMxMzV9fX1dfQ__&Signature=VNJQaqbIzjdkbuVdgvj7NEP-lrHNi8jBTLNo1d9AOOsnZ4rp3nLuHIeqKVxP-L968wNpQlKlSn9a3-dnkxQtBR5ZTqeld5hOIYF4JswDbIXXNAq%7EY%7E6uXfOi3QzcviC9Wa-0UmaB9MgKc2ApOMygu4jWXjCuFKRVuKXBkd5cZym8TrTVmUTtubPZNN7R7ifiIfq3wOuU2ACKFdVVuhVW0pTV55fXKlmJax-k4stHRdLgQ2Lt1O4YUtflqQEz4kUF95l02e-whqy85xwobUyHDpVlqhx2MJ7CBybQfe%7EL25p6R8QWTNnLYiWpnI%7E5d4KitJT63di4V4E3MdeUB4HeYw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083475&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NzV9fX1dfQ__&Signature=tdcrIRLZDKb2pSVsTpK9NnUOpd%7Euc0LMkmXqvfFh1VAV7hHjLCjtewYVl-FI90pYXpnvfQEa2lXYCj-xI2nqbn51%7Ecbn6V9PncbivS%7Eq9tszmxWaCDdYNFS1MuotxXIGykcW-clYJQ2uKjwgaxrWvLk6UR3idmc0fiFZ8cKvci8NEuK8CHF9Vw9FPkF2jCNBXFFsf4JCDkuE9IBuWsWHhI1T0YaIxwT%7EN0PYU7Bqng7aHnb0hlV1M7Eo6GKu5OD36fDnXETtncFj0iW27pCTW5pACNmi5hUf9YiZurqcn10JKZhBQGkZUW3ZnClQJI1Gi3bY46zNGPgB0OJuOo83cA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD4\n",
            "5OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082189&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIxODl9fX1dfQ__&Signature=0kiOQMRS-sf7321ZCcnZuDTYHlZURl8CsD0Q135NEDfmPDQWgSo-XrBSp%7EYr-RGA2ehP4osZplAoO%7Ez4gMnrYhy5tqVflUDcmPlbKo5QF-EyLOkV6XFQprb7qQG7WykU3TaMUIKYMpJ8%7EIKLxFm71VlXiMtDxlLzvGgqaahTXPr8IGc8YMESR9AlB-%7EOr2uM79e1qjdgjxwMvmocNH5hFiiCz9k3R2%7E6ih0b90Gkx90vIpf-4cQ94TVWBlNcFOXPQQ-4AujOoq2TKeBckiynGDTEe8N6S3ApYrJxQRgGSpyhlovRmR1OacCK7dYjV2jqqRR8I-ygQFtvvC9P8ZqbWw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082273&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIyNzN9fX1dfQ__&Signature=fyDDNb-xIKzApf3ezGQ0edomiPdNxPJd1VDt2MjjDEnkZQbSJiwPbMBnII0GpZ5PTDD2nC1m2UW%7EKqalGCD0fwAnBeZFzor4-4JtoAtc3HCIBF5%7ECRUpVVnOhY%7EDaEy9A2A%7EIYh1zTfGx7gguvq0EvVS0XSbJ%7Euz-pg3OIXBsGl94rBykkuFoynLGW5M5Jz4gZFtFB-G9NgqFVUDEJFXxMf5dMKFF3ySNlWHUHdeCvO0DcuBQymXImxkPirYFML5laiHnyV62xIRO%7EDwSUeZ0Dwd0JiPzm-JeqZwoCXi8V9APb%7EmcLL9ChV7LQvB33kkXTGYEhvdbq%7Es4WBqyk68mQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081503&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE1MDN9fX1dfQ__&Signature=RoIKUSc%7EiPsyhVkTLxRGcCmNO2btCgL6juzBi0sjpgJe2khAs0pCTuZjbq83%7EKoZ5ZwgYw8drOYwiee1GjyHNUK%7EDpuXzEOruqgmXKxxrvy58fqvzUFC1qevRYsBHJAF-PHMfk9rdvJN7BCJzsXjlEeuoLQXC%7EvTT9xlmU-IHSpgWPuUyumuV5ciXrtPEcetlyshkJRb8A-AL-Bw5JezVoMs08T4fpD2bf9zysh0fKTHsi33NNwNRRLAB3OB8Gi%7EB7ojy3CG7QrR4GN2s8EgDPSn4tKkJLq-s3eOglnOl2IoFHTUNtULcRmgPKeIV1DUhFOhBeGl0uWYAx-ZuXT%7E3A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083177&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMxNzd9fX1dfQ__&Signature=D9tVlEqNuK4U5nM092rpIn3Pi%7EF9KzOMz8jg9R4P732uVEoSmALr0gM2R7c1ZIndYlI%7EyPHooChbu5mWaB7KFY-YWs18D91fm9N4c2VGeN5fuE11oT4HrYs0F3eNOyFr9NTdVGBD87fVl5YXa3KTYhdqs9dy1utBN0yJwPbCyUbT7CuduC0IJ%7Eotl5xWRKJMA2HnWpffQebiHrG%7EI5Te%7EOCw02emFpDkkZKXf7i4EB7sa79JXzx6QCLZBLK79a8djJvvXo6wKp8duZy5n7yM3YllBxssVIdVpYvJCA5EaSFvJaAHAPpVm%7Eir2BvOu4M8nxs-smG1Qs%7E4PURanccSew__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083390&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMzOTB9fX1dfQ__&Signature=0N3WGqa9wLWmip253yP12IP4PlIkQrS5Eqn1qqliWz-jPwOZntM%7EcWL19X2W-6gS54sMWIld8s1eb9GgkN1vOrbq7lLGiaW6SrVNNwlCkRXn%7EO0Lykie4WFSkTFzIQrH7id7UWEr7QdAKxBHjCvz83hew9BHfeuqZ8JwX6zLhGOUk70SHjToORgxnIZ3E8X8zIdhTeJa7YGZ47Bohe1czJmp1a93dPkr3xsoGFipNV%7ESoTyQb79%7EuvrjorSADyu4VjDWKhxAKPwfl%7EpihaIhcOym1aoKvgx0WZI5it1sDcd-CYbw9wmi4myFAJmJWxloIo%7EtRcDpbNO5ftR9uRu8Uw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081551&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE1NTF9fX1dfQ__&Signature=tBN2%7E20U7VA5vBvNSO8%7EJGIZ6q60LMkfE8J7eP6YwAeCZevsA98CwaJm%7EbbxtcjKHxtO1g5jHMcM25qiYWclrh87X5JkPWGnhhdo23x2aDamTkJuDDwVXBDOX%7EUTIInv2ZQ2VXlht18kvPGnhKZ5RIvPc5m6lA2z4RFJyJAakL6M-oekbc1f7rUnF-pW6cw9B1An6pqUKfz82733souTEKrMBSIjsm6kwucL5JQcdTRKGsFC0ilDmHj90IhyuBqMYw8FdcOpQjMPh9YO4iApUjHtijntsL1noaECi%7EnYEtx7R5LNu8N9SWMcOBUe4g-iPciR9j-vLoaa%7EtAWXLf-6Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083701&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM3MDF9fX1dfQ__&Signature=Z5YRjm7CdXugTIOlsTH8e9YctHqwi8Ty7K8x%7EdVKS1W7Wlo1vcd%7EFDlfHqcQ72JnBx%7ERkh76mxOSAua3oHT6zokdWTAVIGwzlrol6w5a0-1nF6XogZk0U9Ty0EqRNgX4e%7EXh%7EZ96pP%7ED-3E32Ms%7EeT1dgHNOKCHVMMIMwaO2gATHMRbjLc7S2GN4Mw-uTLqLJTR-5SSC8EMxkyKP2PAA9A9MjoschlAHGVCzr5iYlnpgj8QkxG65LEhPZHwig01ym6KOsPMONBYkOLB5jM87RbdNeLiFd8q2CN5IMnDmC9gdUzcQp9R4ht49KRwquKy-rwh4Y1xa6oBiAGqPIlZDfg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081893&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE4OTN9fX1dfQ__&Signature=UteM5S%7EHJxbZb85-ctt3qHAk-bRaGpdVbyg52lydMm-p0yY-rLd-v8ejajwtiX8WI2uZo-paIUvJL%7EPGHGJTc-7syv3SMtyrGEcM1jfuRadhTW17jp0ebaJ3Ag06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O4znW9f1mgOE-PNxg7umPdYWpMrzIAEQkcuhnTXymdcCgjkFdtKpppzx7DoCcwZ2eZcEoSZbd2qLlp8QM2PvTL7WWKYibbmIzfkUTvfzcIl3C8%7Ew5mfNeYGEHs2i-QhFJbu%7E7ANxlBO9fVBlj2anKIra8Z-7DFRiLX9iZEzEWu4%7EvdkFbQ4%7EwD7X5Z3m5hO6IyF2SQoT3CAQKZ-qlW5tTFhU%7ECI-Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083135&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMxMzV9fX1dfQ__&Signature=VNJQaqbIzjdkbuVdgvj7NEP-lrHNi8jBTLNo1d9AOOsnZ4rp3nLuHIeqKVxP-L968wNpQlKlSn9a3-dnkxQtBR5ZTqeld5hOIYF4JswDbIXXNAq%7EY%7E6uXfOi3QzcviC9Wa-0UmaB9MgKc2ApOMygu4jWXjCuFKRVuKXBkd5cZym8TrTVmUTtubPZNN7R7ifiIfq3wOuU2ACKFdVVuhVW0pTV55fXKlmJax-k4stHRdLgQ2Lt1O4YUtflqQEz4kUF95l02e-whqy85xwobUyHDpVlqhx2MJ7CBybQfe%7EL25p6R8QWTNnLYiWpnI%7E5d4KitJT63di4V4E3MdeUB4HeYw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083675&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM2NzV9fX1dfQ__&Signature=NtdycTKm55fBcP5TnTtHYMUZpyBUCrridh8n9pZcdlV7WwRwzyPpyKkrBbHmGICYsKGMQNRm3z6iEX%7Egtxds1sW250vpsQYOn2KesGYBp1hnidI07EGdj8enX4b%7Ezj%7E6CbJ5SHyI%7Exru2QaDgTEUvchh1-HBlOjofyI98I0ETn3NRs%7EvjzO865wYf8viDFkPUbIDs1btdP9ifO0i0T5cK3vUtT66zEkQBrf9-9Jy1eRpShUnfuQ9-XDKW%7EGkVhroc4wf7Vkl8Q5ypno-7IqVD92-2TzKkdkbr1hLjJTv2kgc3zntZfMdur5jMXdcO2zke1-%7EtVR1FhehPCuCypL3LA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083475&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NzV9fX1dfQ__&Signature=tdcrIRLZDKb2pSVsTpK9NnUOpd%7Euc0LMkmXqvfFh1VAV7hHjLCjtewYVl-FI90pYXpnvfQEa2lXYCj-xI2nqbn51%7Ecbn6V9PncbivS%7Eq9tszmxWaCDdYNFS1MuotxXIGykcW-clYJQ2uKjwgaxrWvLk6UR3idmc0fiFZ8cKvci8NEuK8CHF9Vw9FPkF2jCNBXFFsf4JCDkuE9IBuWsWHhI1T0YaIxwT%7EN0PYU7Bqng7aHnb0hlV1M7Eo6GKu5OD36fDnXETtncFj0iW27pCTW5pACNmi5hUf9YiZurqcn10JKZhBQGkZUW3ZnClQJI1Gi3bY46zNGPgB0OJuOo83cA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083236&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMyMzZ9fX1dfQ__&Signature=G5iWX3BYtlcIi8rseS4tq4p2BQUpQzGVRx4PC1KFHS2E5H7oB9O9wonaDHEBlB4tUKE%7ETbPNkmFPw8q2HgXzthzOZ6PIW3Nq4yz-3B5lwLhPJMvRuFDJQvsOTcLH4lwvuf1tC4q4MAE7QXnN8FBTNznl8PMI9GQxN0rpsO35lkYWroVhNX3p74lVEdVET7s5qJvG%7EAJXDU4HuAGh7BKvTY185Rf%7EGJJJYSQ%7EV3o4pXI3jLQ%7EKhZ0ZUVVZMCBeJJ%7Esy5AN7T3BtDy5XZVFClUKfPyVxNTKES4y%7EcuJ9%7E%7EvZP4DKgCC5WnG8LRTVkSzlJ4vWHp2jBKMb9vABnfDhqKlA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083675&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM2NzV9fX1dfQ__&Signature=NtdycTKm55fBcP5TnTtHYMUZpyBUCrridh8n9pZcdlV7WwRwzyPpyKkrBbHmGICYsKGMQNRm3z6iEX%7Egtxds1sW250vpsQYOn2KesGYBp1hnidI07EGdj8enX4b%7Ezj%7E6CbJ5SHyI%7Exru2QaDgTEUvchh1-HBlOjofyI98I0ETn3NRs%7EvjzO865wYf8viDFkPUbIDs1btdP9ifO0i0T5cK3vUtT66zEkQBrf9-9Jy1eRpShUnfuQ9-XDKW%7EGkVhroc4wf7Vkl8Q5ypno-7IqVD92-2TzKkdkbr1hLjJTv2kgc3zntZfMdur5jMXdcO2zke1-%7EtVR1FhehPCuCypL3LA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082781&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI3ODF9fX1dfQ__&Signature=qTTcJtUDM%7E3ZTeDXfY%7EEnXR781lckTnajgx0U26deln%7Eryajg7eIF-HEvWBInZ0bdhWVApjuJAFV4Ak1ITiiEbcczZTtbpcCb6teQT9nrKUlPsaRMOiIYlKHHHFIbx5C7PBePkx1T%7EFrR9iHVVXrdPo9Ts%7E558FGumCeMPxZrVg5bfA8%7E70crPtnEdN5SFvxPSa1PoW3I23oaOg37lGV3USY0yRBt72RQcv-oi-OeqdeIF01C6Tg5sbgaEJtstgnH17IFGca0vMt4w6b%7EIEDM7qliPi8j9EuvDoJVdQreRvdr8hml4DCOKZcudiKdLPQWs%7EvMf2S3LNfjR627MhDQg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081503&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE1MDN9fX1dfQ__&Signature=RoIKUSc%7EiPsyhVkTLxRGcCmNO2btCgL6juzBi0sjpgJe2khAs0pCTuZjbq83%7EKoZ5ZwgYw8drOYwiee1GjyHNUK%7EDpuXzEOruqgmXKxxrvy58fqvzUFC1qevRYsBHJAF-PHMfk9rdvJN7BCJzsXjlEeuoLQXC%7EvTT9xlmU-IHSpgWPuUyumuV5ciXrtPEcetlyshkJRb8A-AL-Bw5JezVoMs08T4fpD2bf9zysh0fKTHsi33NNwNRRLAB3OB8Gi%7EB7ojy3CG7QrR4GN2s8EgDPSn4tKkJLq-s3eOglnOl2IoFHTUNtULcRmgPKeIV1DUhFOhBeGl0uWYAx-ZuXT%7E3A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083590&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM1OTB9fX1dfQ__&Signature=MQdBz9no%7Emo6QFQ1C5-ztOwPy4496CCYehsGTy9cRB%7E4%7ErmQrFB5Yeyd8xH3Jv%7EkLqkcdXajZdzwZMDyjoWYSOA1vQgZUbYwmUE7QJw41ZIm6A3ggzeURlspRV3MkRMYOmcDLV1oHkDsDHcMsq0dRaBQpuZMGStsiUJe9StByB458rc1akOSV0BJXz5JBcazGigYNjjsAernYjO4Brm4E6761vQ88%7E47ZrP2dRrJjlRR-sy9uLIwtLwKnT%7E981H1x2G%7E7KqqLJdhYyFAavZYraZfecoM6ZentQdM%7EC1YGJqvbZreuZZfLzu0TnqdrHQkshY1XIFMbQPMCZPIrQ7U-A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081684&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE2ODR9fX1dfQ__&Signature=b9cy15ZG7BtRy%7ESUD-AKkJbrfcIubOomx-ij%7EbVKlX26xF%7ExOColPEQhPmMSH2ElpenFayfNoKGoH8hSq7boud7A4JoMqN5OzMhDxbQrSaWzZLOtKMzz6QEWhfbDCr4%7EznZBG2tbsLlC6qYaaxp7gosvlcou81znzVhgNQae5QLsQY7m1dVv64X-s2eBPfbjWSDbzrjv3EFn9mBdBRTSv8O3VmEAXS9ZYeO02pgl6vvbVCcs-8crTkltJKybvy1xX8F25RGcw7LjeFIr6kaGpbeYa%7EaNP0tTPgQpDBaFSPLzRdI46AsshBYTvAiNwu2fkTdiTx1vi3VHaW91TIL9Dg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzj06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD4P6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "5OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083475&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NzV9fX1dfQ__&Signature=tdcrIRLZDKb2pSVsTpK9NnUOpd%7Euc0LMkmXqvfFh1VAV7hHjLCjtewYVl-FI90pYXpnvfQEa2lXYCj-xI2nqbn51%7Ecbn6V9PncbivS%7Eq9tszmxWaCDdYNFS1MuotxXIGykcW-clYJQ2uKjwgaxrWvLk6UR3idmc0fiFZ8cKvci8NEuK8CHF9Vw9FPkF2jCNBXFFsf4JCDkuE9IBuWsWHhI1T0YaIxwT%7EN0PYU7Bqng7aHnb0hlV1M7Eo6GKu5OD36fDnXETtncFj0iW27pCTW5pACNmi5hUf9YiZurqcn10JKZhBQGkZUW3ZnClQJI1Gi3bY46zNGPgB0OJuOo83cA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083550&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM1NTB9fX1dfQ__&Signature=zYz7H5aS7cZjvO5JjUh0KPIjlGpfrR81fsy2zq-TrSVKPzIJSwvyEPJSuxdlkkKlJvxKbU%7EJWbibBlvr-wTkdE2u-GaO%7E7VOnOvC3DQwI9jXNNQ98MXnfDkQJ9-3t2CAF4nk5IY-ytA6lSdPPb6LCZw6S9WsX7g2JeUEHjPxqTHo2prlusg-9aTteYBipo5lFyJvPpoDQUZv6DNiNemUkubyjsFQtuH3kNIBSltOaZZPfTLiOJiJVbL1smbt8Vju9TmmH9K5qcZ70jVg2-eQL4eIvVcNqyXPOiuaPdtXCDJSzk930FpCstGx91o0BqIs8pxOYvoYYi5jUy-iBwg8Pw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083236&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMyMzZ9fX1dfQ__&Signature=G5iWX3BYtlcIi8rseS4tq4p2BQUpQzGVRx4PC1KFHS2E5H7oB9O9wonaDHEBlB4tUKE%7ETbPNkmFPw8q2HgXzthzOZ6PIW3Nq4yz-3B5lwLhPJMvRuFDJQvsOTcLH4lwvuf1tC4q4MAE7QXnN8FBTNznl8PMI9GQxN0rpsO35lkYWroVhNX3p74lVEdVET7s5qJvG%7EAJXDU4HuAGh7BKvTY185Rf%7EGJJJYSQ%7EV3o4pXI3jLQ%7EKhZ0ZUVVZMCBeJJ%7Esy5AN7T3BtDy5XZVFClUKfPyVxNTKES4y%7EcuJ9%7E%7EvZP4DKgCC5WnG8LRTVkSzlJ4vWHp2jBKMb9vABnfDhqKlA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082781&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI3ODF9fX1dfQ__&Signature=qTTcJtUDM%7E3ZTeDXfY%7EEnXR781lckTnajgx0U26deln%7Eryajg7eIF-HEvWBInZ0bdhWVApjuJAFV4Ak1ITiiEbcczZTtbpcCb6teQT9nrKUlPsaRMOiIYlKHHHFIbx5C7PBePkx1T%7EFrR9iHVVXrdPo9Ts%7E558FGumCeMPxZrVg5bfA8%7E70crPtnEdN5SFvxPSa1PoW3I23oaOg37lGV3USY0yRBt72RQcv-oi-OeqdeIF01C6Tg5sbgaEJtstgnH17IFGca0vMt4w6b%7EIEDM7qliPi8j9EuvDoJVdQreRvdr8hml4DCOKZcudiKdLPQWs%7EvMf2S3LNfjR627MhDQg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083177&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMxNzd9fX1dfQ__&Signature=D9tVlEqNuK4U5nM092rpIn3Pi%7EF9KzOMz8jg9R4P732uVEoSmALr0gM2R7c1ZIndYlI%7EyPHooChbu5mWaB7KFY-YWs18D91fm9N4c2VGeN5fuE11oT4HrYs0F3\n",
            "eNOyFr9NTdVGBD87fVl5YXa3KTYhdqs9dy1utBN0yJwPbCyUbT7CuduC0IJ%7Eotl5xWRKJMA2HnWpffQebiHrG%7EI5Te%7EOCw02emFpDkkZKXf7i4EB7sa79JXzx6QCLZBLK79a8djJvvXo6wKp8duZy5n7yM3YllBxssVIdVpYvJCA5EaSFvJaAHAPpVm%7Eir2BvOu4M8nxs-smG1Qs%7E4PURanccSew__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083675&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM2NzV9fX1dfQ__&Signature=NtdycTKm55fBcP5TnTtHYMUZpyBUCrridh8n9pZcdlV7WwRwzyPpyKkrBbHmGICYsKGMQNRm3z6iEX%7Egtxds1sW250vpsQYOn2KesGYBp1hnidI07EGdj8enX4b%7Ezj%7E6CbJ5SHyI%7Exru2QaDgTEUvchh1-HBlOjofyI98I0ETn3NRs%7EvjzO865wYf8viDFkPUbIDs1btdP9ifO0i0T5cK3vUtT66zEkQBrf9-9Jy1eRpShUnfuQ9-XDKW%7EGkVhroc4wf7Vkl8Q5ypno-7IqVD92-2TzKkdkbr1hLjJTv2kgc3zntZfMdur5jMXdcO2zke1-%7EtVR1FhehPCuCypL3LA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45\n",
            "OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083324&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMzMjR9fX1dfQ__&Signature=HhTptGDWkHrSzp%7ECyS1pxJcw%7EBUA0-TtsJ9FJvEUBHdyBGxv89UtOltv3IgODWKK0lHgzuIEmUnAiWJA51CaB3y0HnNvD8eAzlG%7ERs7-mUuWzISkSZyivqIApU%7EFbJ3Z9Na6ryx0ZtwLODSW29vFcXrAc3w4swZW5rkWrMmxZvDwJyw8%7E8hvzXgIdbyDUhXoD2JWm5RgkJrMobkNr9KjTnN-FMER8yGDMkftYPCKhkQRNBTY66tLLWZnJxVm4-FgXQ1Pu6QEZwajhBWAI4bpCZ9KgqkG7GBLtGGnXwd4uqlYxcaJtef2J44FEeSP4YfO9j4e3mKQ7sg8lR0H%7EpJegA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082812&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI4MTJ9fX1dfQ__&Signature=RAd2FtZRjVtSLODZNvq5ZthRBKgbutEvxKyt1JtdqsJHmoPlkfByIhSzANqntlcM9DcQoopyq4frkGNZObGNH3PZccPLnjtdnMH78GGQgA8XPJ91Fw6QLssSKbCAbMjAMnWt4CP6SAOP%7Esm8TYqLZLAoWqBaAnXwfm2uW-cVff9VRB2kbw7G9IqMvokp5ryijQ7oz1dLuRWFF5nRu35VuqbDsym6LnLF6xIXcMQsosS4jxchiI%7EF-Vty1GHUFQu3FlUtm34E3T7TXZPysJnUjwSiTfUKd8NaAe-s14dH1WlXVtWJfqz0kGZXWDCa-09%7E1bOyYGOI9pljr2hjEu0kxQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082369&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIzNjl9fX1dfQ__&Signature=zyPc3vVzPOK6wEhsy-uxEXkngPpR0Qc9xeUy4CI8VbMukMju4vt%7EK5T7Tfmgfxsh-2E6X1qa8kl9DhrM9636G1RzVXOQtwziMNwgpOUV0%7EiMB4X%7E4n0LEXEPBDgyviEZcvszaF0DaCTs7Avi9QG5qPMy4XsvKuS14dkoKbknuO%7EFsR2Mszu9xSPFB42nqd0w1whyYigQLhT1eRGg61J0j8PQG4Gb4sKkcVyGY4oaVFXrI5-q0Ah%7EfFtTV2M5GKVU9lJtyS4LygFnZWXxdQAZ1XN3sUij%7Efg1edKESkinhI9KFHXhnVKnHKrCJNKz2By5jz4pZBF8iuidzPTGEabGVA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083811&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4MTF9fX1dfQ__&Signature=c6KAV2aVjp0PTJODtIVuw-zWp-1Je%7E1n3v1qA9gDeCCAFvxk25UEohScbXszocg9s5YTeuDOdgqvPHfNv5QUL78f6CI%7EfgGZXU3IA2HpkIobstTT4fxwkZNMS-lGjUXqCSasN3bjFbMvvU0GbV5XxdGY9IyM9KUCKN4yvJtj%7Ed2OHrqoqUWY4TxrfHa-iPOCraaM7a5MD8kBFRBITZ735A7YoBkXlMq5U7uUJqQl7nfD9BTg%7ErGXs42g2vDbKu448TeXZ1pIPRXsoLxsnHjNOc8zcqQ2cThx0Q2pQgNO9p5itwz-j%7EAGtgx2cL-CYUvWcIZ-dOyHT4bzoototKBjHg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081893&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE4OTN9fX1dfQ__&Signature=UteM5S%7EHJxbZb85-ctt3qHAk-bRaGpdVbyg52lydMm-p0yY-rLd-v8ejajwtiX8WI2uZo-paIUvJL%7EPGHGJTc-7syv3SMtyrGEcM1jfuRadhTW17jp0ebaJ3Ag4znW9f1mgOE-PNxg7umPdYWpMrzIAEQkcuhnTXymdcCgjkFdtKpppzx7DoCcwZ2eZcEoSZbd2qLlp8QM2PvTL7WWKYibbmIzfkUTvfzcIl3C8%7Ew5mfNeYGEHs2i-QhFJbu%7E7ANxlBO9fVBlj2anKIra8Z-7DFRiLX9iZEzEWu4%7EvdkFbQ4%7EwD7X5Z3m5hO6IyF2SQoT3CAQKZ-qlW5tTFhU%7ECI-Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=QXfm%7EVqX94W5I17cKDzZdKnKnOWmVwuIyxAVH4fO5IN4VLNFqfr1fKaOGnBUH-vlRxrxgtkdla4Sf26yGE8zNG8evW2MhLyTAeIFa5l4mHiv-3FrPcPI7PJFMlD45OHCvd0D-1T2VVQ9ZLpo4zQjgsr6QFbzhJLTh1D4J%7EuLHhLnoIcv8C9LMvydnGf30fV3OR%7EmOese6IJ3%7E0XeV3IYcQVnNVJaMtb36wMcgJRLaCpZ-LdXGU1QsxBcXrXkdopoGE7tHe2ABcCJYRSVx2s469BAPUBAygH3aZEIh1Ev8lGk2uZcx0dvr%7EVo9rGIpAWDQRfNPuXP9cWTU7I6GK%7EGXw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081961&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE5NjF9fX1dfQ__&Signature=w2miOOiDi0F5Apl3mXFGLPYtejxeB8%7EapJ8uefTQFS-lIyBqRTv2eQwqTVGOSMRq5Kzrj0VZ09MWNmimy09eLOt7TooLib1fqBMtSbinXhwlMxJAHVUeDhnNWIQlOJR4mBGfSsUMM9J9MxeUwAqjYW55sI31TmVGB5-kk8B2UvtlCASfLkZT3-9DiOH-FZbt99cOm5rDXushzo9UNCSD4bW52tuG8VTUuafzkU8OLS6hJ9x5YpLNDVZZqu7qoAxCEi5atBqNXYMc1TAavN61H%7EYm4pyQ5mG0r2FABSKQicHN49agBsDz2xVoXuDC0V%7EFwztjLj6dLW9ixcj1eKIksw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082369&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIzNjl9fX1dfQ__&Signature=zyPc3vVzPOK6wEhsy-uxEXkngPpR0Qc9xeUy4CI8VbMukMju4vt%7EK5T7Tfmgfxsh-2E6X1qa8kl9DhrM9636G1RzVXOQtwziMNwgpOUV0%7EiMB4X%7E4n0LEXEP\n",
            "BDgyviEZcvszaF0DaCTs7Avi9QG5qPMy4XsvKuS14dkoKbknuO%7EFsR2Mszu9xSPFB42nqd0w1whyYigQLhT1eRGg61J0j8PQG4Gb4sKkcVyGY4oaVFXrI5-q0Ah%7EfFtTV2M5GKVU9lJtyS4LygFnZWXxdQAZ1XN3sUij%7Efg1edKESkinhI9KFHXhnVKnHKrCJNKz2By5jz4pZBF8iuidzPTGEabGVA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083626&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM2MjZ9fX1dfQ__&Signature=TRlPsoPuNRzyEBnGCsZe8N4FbUXhcLrWmFxmfFdOSaqJSolock4B0y5ggpqfNdZw2vB7mel751Bfk8J9duJS2dcMYEXvpsK%7EtOPrK-6xd-8wnhR6lhSoeZsecQtW8AaCyCU--E0LoTJFutlpC5Al0tB8zQEvNveJjapvlYFGBRa2RIYhDcI58-q-DaTBEBEka89CVYfrmUHkVIjRdKokVz6LeydwDLEqIBFuFsLAfMdP2PeLq%7EAEKfxSUc9aqlIVgbmoM1iBtDyQSMf-WOotjC63XQKwKa9Q7nOnUqVCCsxxyaZNLHr63ecQeqtyAYZ%7Ez9%7E81Md-MWz53DfkqNOQQA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082812&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI4MTJ9fX1dfQ__&Signature=RAd2FtZRjVtSLODZNvq5ZthRBKgbutEvxKyt1JtdqsJHmoPlkfByIhSzANqntlcM9DcQoopyq4frkGNZObGNH3PZccPLnjtdnMH78GGQgA8XPJ91Fw6QLssSKbCAbMjAMnWt4CP6SAOP%7Esm8TYqLZLAoWqBaAnXwfm2uW-cVff9VRB2kbw7G9IqMvokp5ryijQ7oz1dLuRWFF5nRu35VuqbDsym6LnLF6xIXcMQsosS4jxchiI%7EF-Vty1GHUFQu3FlUtm34E3T7TXZPysJnUjwSiTfUKd8NaAe-s14dH1WlXVtWJfqz0kGZXWDCa-09%7E1bOyYGOI9pljr2hjEu0kxQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/d6376a884bc80a6ea95da83f963508e57d8508333c424bed7beda565e4aaa0f3?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00006-of-00009.bin%3B+filename%3D%22pytorch_model-00006-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvZDYzNzZhODg0YmM4MGE2ZWE5NWRhODNmOTYzNTA4ZTU3ZDg1MDgzMzNjNDI0YmVkN2JlZGE1NjVlNGFhYTBmMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=LcBqd-4dpfM835syRLjx-LyYrH74mp3nUOlNA7tW169adR7gLpHGaNCq1rAnMgbRHlOJ%7ESOpZPPJwvXYIPhho-RLdLTJIFg0CEXX09Yl3THnI9%7Eus22tUXvFzjP6vAIOLh3HkjSMl5j4pBDFBHftoX-qYKMCBZr77qAp0U8k3bssvMHdBw5sZ9a18Yey6Yr12JVNtO0BeyF3I%7EG-hTJz8jWoIUoVCV3rQ6AseX%7ErSi-8N%7EG0GpZ65JKFd5TeEI8sWQ4tcP5xvQnOuvP69in5wJsH7ufi5AB-R164V0MEBO4dq38B1SJVsP8egqOkWxPQXPG4DAHUtvvRbs0RmuBdgw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083875&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4NzV9fX1dfQ__&Signature=QeWcHr00oIINt1r0zJeibgFP1FAxt-xW16vOKmdVjPEB2Ovxwo0lU9jQ20Ta%7EtvQv06OS1K-W4gC1pLOIN3uLQEliHiF%7Eulh5wYXKT6gfb4vNEkftLgeqi99US06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/638cb048ee6ad66f85a176e980332b0d61cbddb2d5dc9b5ad1863d87e020350c?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00002-of-00009.bin%3B+filename%3D%22pytorch_model-00002-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvNjM4Y2IwNDhlZTZhZDY2Zjg1YTE3NmU5ODAzMzJiMGQ2MWNiZGRiMmQ1ZGM5YjVhZDE4NjNkODdlMDIwMzUwYz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=E9wc6gm703FY9vVPk71vPe9gkK-IQrV9uxwfR4KCEKHmJ3gNWjqKN9A%7ETntBQkncnFH8w4D-KOLGMghQ8K9NVo0-6RaFjrdBHSn3EHyVIoJSsIymC0Vt2ZdEljSH\n",
            "cfCQ1Shg3Q3pZGhZ57CXM8-MPpGtuRkr4rcNDwUwq1Sd502-H2OMJ60rvmvcyWZ1ZMulwDBa-7yzy85oMdbA66-fHeikdsisKsGsUFip%7EdUtdkW%7E5gB%7ETmIkDoERaghQNf7AC7xkvr%7EA75IBceI9dZZN2IF%7Eom3YRXh8eOSeNNnuxWDHwNOh6Q9%7EbrwYnxylhA2MFKBcfcooHw4MI%7EE2tXpKgOTQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "%7ExAj%7EZCaHa9sHJ1PmZgbNBmE5HI-HKVAKf2ZESP2mBETnCF12W89In-JpLXhY3TCL7l-UZdoMW5HAg8puSB1VXuZIM9pyonfK16KlYTp7rIw65sA5ehYZCZEmjQBZLsWEaXj8DCuQ4jBkYlDFf68bJjaavQ97QsTN4bZD9UD%7EvNS157uWZuZECAj3Tae3m-kRF7lrn3PuFzmiUbNvKAtdeKs%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/b2125ceee3992a093f9808efc9a720643a44e59b17baa77eddb0fbec6965ce5a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00003-of-00009.bin%3B+filename%3D%22pytorch_model-00003-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYjIxMjVjZWVlMzk5MmEwOTNmOTgwOGVmYzlhNzIwNjQzYTQ0ZTU5YjE3YmFhNzdlZGRiMGZiZWM2OTY1Y2U1YT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=u1t9ZrAUAV1%7ExbZR-urBv80R2PxxOb%7ExlclFHuo5vmd6ter-tpf3aSbnqCz5vUvWmh-CzzyiIl%7E3%7EIkNFv4ZteqASlciPkHcHNHJFj0bTdcSm4npC6u-6O7bviGrohXWbFft4pNBLaQuGoPJa8oLgNINCjw6nlivey30M8fYNS5d8Qa16xzJfG7CS67mTFutg24nYz2PDrzer5ZcegurDio5YfNCRvGX9MgEnEQnCjDbw6nX9fx0aWzswsyHgZCIVGBC18E%7EURDRzRiwhxsex7mo6ElWwl8DrDErZH7iu0hK9StGHiTbe%7EkvF59PXcCrqrJZ0vwbWNnJ9xknGS22sA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/384bd92e6d1d0b6a133b1d81bad915a10d954d1bfcdf4fdc8b6d0d81b4f5e9e8?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00001-of-00009.bin%3B+filename%3D%22pytorch_model-00001-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083875&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMzg0YmQ5MmU2ZDFkMGI2YTEzM2IxZDgxYmFkOTE1YTEwZDk1NGQxYmZjZGY0ZmRjOGI2ZDBkODFiNGY1ZTllOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4NzV9fX1dfQ__&Signature=QeWcHr00oIINt1r0zJeibgFP1FAxt-xW16vOKmdVjPEB2Ovxwo0lU9jQ20Ta%7EtvQv06OS1K-W4gC1pLOIN3uLQEliHiF%7Eulh5wYXKT6gfb4vNEkftLgeqi99UScfCQ1Shg3Q3pZGhZ57CXM8-MPpGtuRkr4rcNDwUwq1Sd502-H2OMJ60rvmvcyWZ1ZMulwDBa-7yzy85oMdbA66-fHeikdsisKsGsUFip%7EdUtdkW%7E5gB%7ETmIkDoERaghQNf7AC7xkvr%7EA75IBceI9dZZN2IF%7Eom3YRXh8eOSeNNnuxWDHwNOh6Q9%7EbrwYnxylhA2MFKBcfcooHw4MI%7EE2tXpKgOTQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082870&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI4NzB9fX1dfQ__&Signature=Mw3hvtDLz2JV%7E-ZQwE7mqFR6dvCtdpQrUPe-HFkI78aRlUsj%7EiVgy8ds3jwXGsfSOOOBIb%7E7Nm6XceUN971lOeluYrs35m-w-mnWAZIGGrbovccq%7EUqqFY9lnAb-xq%7EQ2YmX7Ii7qQmAqZoG9zyARm95rfALfYK1Q8CdjG0aEfhDo7xJnVli4z18K8mOI%7EOosG9BgCLWkB8JCrHaUNRBNx67oCavsC2jsM13YJYz4xHl%7EK00ZY0OjgINkF8LvUNbUlmU7vQMdWIKAtD1P6Ng3hAI5DM8OXWRf8R6m9Om7z-mYs7qDDcYFwoTLPwQRDbEjhXN3vR2zUd9A7Gk8lGw9g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083374&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMzNzR9fX1dfQ__&Signature=q8B9MEx0by7Lo0EXb%7EW8C1fsaRAULUi7M0eKO7-bwNyi8iS0Afpu7Q2FcdHobqaOq1IkuNXlzG6EBRfOzsfwzqmE0S9ZG1p5vi2G0cP6wvXuEW74MDFYvZrq05C%7E5xVTFwGwUouoK-Rm95SmkhWzbPfV8KsiKjOTcNrXTB1dfp%7E8P0bBV-DE7RApK2bbebV%7EAGcUlWlAsz2UcG0sfBXyB0LRRVj6uRr74mVichgR3yEF1xCOK6sE4lCnHTXOzBAeDuP46Aeigt40lkcgRPdOWiYlPVBOXE8Bi-0bmIbl-kuM7pKR4wtBvOtGD22YfP%7Ea-XqZLdeypf15B7eLKXVosg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083828&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4Mjh9fX1dfQ__&Signature=cALycggqm0aNzr6u4Ds8bGF7PvaaDC0zEhRN0sTTWeEPEGbHhwk37JSmsZqGIcy0W4RHhvwSCOmujj3BdVBLXRr0hi2cdnDVGG7DIHWEF7iMS2NCYVsi0JIUfAYz-8i7NttM8qMbjm19jR2XOFLtUJEAte7vkEsqm-XdEtiEULKSi76vB-jFydM-1YmcDk5Tw3k7cw%7EZHsMmRKls%7EvsSei-IS3aAwO1GpFhnPDklc6CgMzkPCvmn6fYKOPOet9rzdwJwVIFhzAOp7xM9GvwvKVTsjf5Ma1A45nwMzqTfvf66VngUjdlLY8GQW3ZAWc-dOp0eapNfDGEZu4AsbBjhyQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083828&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4Mjh9fX1dfQ__&Signature=cALycggqm0aNzr6u4Ds8bGF7PvaaDC0zEhRN0sTTWeEPEGbHhwk37JSmsZqGIcy0W4RHhvwSCOmujj3BdVBLXRr0hi2cdnDVGG7DIHWEF7iMS2NCYVsi0JIUfAYz-8i7NttM8qMbjm19jR2XOFLtUJEAte7vkEsqm-XdEtiEULKSi76vB-jFydM-1YmcDk5Tw3k7cw%7EZHsMmRKls%7EvsSei-IS3aAwO1GpFhnPDklc6CgMzkPCvmn6fYKOPOet9rzdwJwVIFhzAOp7xM9GvwvKVTsjf5Ma1A45nwMzqTfvf66VngUjdlLY8GQW3ZAWc-dOp0eapNfDGEZu4AsbBjhyQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082876&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI4NzZ9fX1dfQ__&Signature=q9MQ49W7FeWEYWio-i7dyEs5DoWzOQz0jsphIqZ2wLOmTfQ4wHzNgfHlMx6%7EWGviizPEkD6IQts3wdFJNjfqxFEhJ%7E5aktm5GXOk7rSyDmxY-crfaJvIWUFQTaFUjT7rn7UdYS8yS8spqb8MeMJHR8cZczNmxQtX4DcU0V9Lavbsh3qyoG14Tc29r5EMG5ed25Iv2X0dNYKMtRhW82Z6t826s4tuEPOK50RQi5y20rjM2KOZTWpsyIsk02825McYxf66jcpkInHiV1U%7ET5Q4ITzb0C7BexKWQ-gWi4N9lTY37yphV2li11COvVvAxKjqy309GREjrxeodi9cMz2RiA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082582&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI1ODJ9fX1dfQ__&Signature=au4TZNN6D08PVv3ZENthGngDFfl8Cwx0u4OC%7ExLZd7cBxwhyYKFMnUBiIBguhoBhLUA22DQbuAJwhexfTZAzM6-wn44OfIafr39TWaDlLRc4a1An2O3MSGB0F7sU5WQBvHfjPxse9-TcG7K6XLMl0DO0H1T8Vk8nyvgodPsAIZk-wavGk7P4QFq2a0qCf9MRrf4KpMczcT68vnsjwx9JLK3SkmYm6xdxg4M3pA0SShp6Od%7EbK14Anz8uP5j2UOAexTdWNfW7cWTvy38e0Nawjf0jL4xD3Q5LGEW%7E9R4W-WPojmYyPw7OWAzD9CyvrB4mW8c3BRxC3iZsZYOjEytv%7EA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083761&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM3NjF9fX1dfQ__&Signature=yF9T79rXukBhawas9yJNiV-AYGN-Vj%7E-nDTxbjWlz7w0F2aU-ye9JOv5BkLoY454PhEgiKGeNi8JYLevj1NAFXoWoB5tHGQ5WeZs2yjfd7dT-IlOu3Y-3PUpQ3Eqdxj5-Lnhd9rRPUr7vk3jqRROTZy90CHUfnKap7dy5xhpjWxhaafwpJfQVOwaEuKefjuKn6RRtTca0k9GaCfvJ7yXhRrFu9Q0ptMBdQ1o5Tjefo6AFjlMSqHBGXKn8tzAK2pD7hmZoYPbTnsUhBU3YLLC8A7S9H4Uj5srIor9ehBb%7EUxfPjRq8O161nGqqIe8KG%7EV7Ir1EP3XytFdLnu-2B2aZA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082464&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0NjR9fX1dfQ__&Signature=az57nOSOw%7EXgckQPCFQQOpc7miMSqb2-2bepaTVBt7g5vSh01ZO8MvoVoBASrfpjlb10W1wqMGvRjiy8bPiUoeULD2%7ET8jVN686jvkfAknoa4HNrYrpC78ZXBhIcu5n0HKkweeMAfTPISWKszjYUJbGH3Aquf%7ElgdWImOGd8YBMQxUO5OXxdoKccjjBQxelvptlmphW2Foj23EEi3hWRQ0eHs9BqXzAQCf7j8mziMz2MjUabYLG27Cpw6e8c8exgDmqB%7EC6fgfY7hZ6Qt6nQ1q3AUMPo0I5jn2tNRfsZBjFW5cSYQxZIbDeldo%7EIGaj1WazInWWvTVOQF0t29BtzYA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=pVr8ywcnZnxdqWJi2QQyX9wnZ3NpJMG5yku7Q49i2MB4wdA1OLpHRwdaTXRj2rprGnspUC5GXVaHq0brXczbXgeLRw7fAamU9plFB6KBMGmai3lVTjgSio5%7EI7I95buP1MH2LdAYTA9x7wdnVYQJV1kc7zXCqqEWVbalcX154RMsU4Ctlix44-E9KjJv1-tqV-PIv65dAgk4ItY5dFvwquARF-zLll-IAExKmEHJ2BkQW6M78vEmSIyIr1%7EBhP%7EqVnD6FzNs%7EMeHdUJa4UlvSoFK7LMldcXJX1Sd2-23DbG1-tD28q9qIEpHM9QRNWgfzIgiXLk634%7EAfPh08Rc6Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082198&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIxOTh9fX1dfQ__&Signature=CFdd5DJjZ9yH2Iy5Qovw5iDrqP3d38atTb2cMjXsv8bsARp5WF0%7EPKUg2I4LUetmaliozSZ5Z2Q10wC055sr9nuC6CfxsnVjRGiSC78L3AnV5sJSL3-aSeFdDfS-MaAZwyDr79r3ySfPgueAw0R-oKJ9hxwVrPcgsQaYQjIpld7aiYtLVs0X2ktpgIHykMIgq3zB4AtbEOCIwizj4Deu7h0yFjgZfeyED3nUCtGOA3RYaHwJqi%7EGMNYqpge7OMtp58XelTuSqsz5IJ%7EASzSMiKC7pcOcxcuRQj-QFD8H2c4rIc5GTg2suiLiMbZtcjuyA426-ISfwbpQcr9zaY1wzg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082636&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI2MzZ9fX1dfQ__&Signature=oBFiWHwPqtB%7EzxVcwMflDrIPoodKOqSmOAttoaRE4f88O9gjvgXxr5w8Dxqz24D5do6sWDggLJWC9beQaOgegQU5jWAJXz317gC6YlaSydlRu5meacC1PM91EMRAi6c49DzdYNl9PUX1EOCfpkQNi1GZjtnINY6%7EntQyDT8tELHVALj3vxFgISFbugOFsqRbVLiX2n3KNxQpDE0sg92sI-04zO0d%7EO5PNY01BZzp661LjbGfnMssdNQULybAr9GR85z8Lb%7EBUxkjp1sdvvVwIZi%7ELOdW1RKRpaujgR85yBOL3c2yc9vtfNi45zEycBdH1E18AANqLc2FrIq-7GKA8Q__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=pVr8ywcnZnxdqWJi2QQyX9wnZ3NpJMG5yku7Q49i2MB4wdA1OLpHRwdaTXRj2rprGnspUC5GXVaHq0brXczbXgeLRw7fAamU9plFB6KBMGmai3lVTjgSio5%7EI7I95buP1MH2LdAYTA9x7wdnVYQJV1kc7zXCqqEWVbalcX154RMsU4Ctlix44-E9KjJv1-tqV-PIv65dAgk4ItY5dFvwquARF-zLll-IAExKmEHJ2BkQW6M78vEmSIyIr1%7EBhP%7EqVnD6FzNs%7EMeHdUJa4UlvSoFK7LMldcXJX1Sd2-23DbG1-tD28q9qIEpHM9QRNWgfzIgiXLk634%7EAfPh08Rc6Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=pVr8ywcnZnxdqWJi2QQyX9wnZ3NpJMG5yku7Q49i2MB4wdA1OLpHRwdaTXRj2rprGnspUC5GXVaHq0brXczbXgeLRw7fAamU9plFB6KBMGmai3lVTjgSio5%7EI7I95buP1MH2LdAYTA9x7wdnVYQJV1kc7zXCqqEWVbalcX154RMsU4Ctlix44-E9KjJv1-tqV-PIv65dAgk4ItY5dFvwquARF-zLll-IAExKmEHJ2BkQW6M78vEmSIyIr1%7EBhP%7EqVnD6FzNs%7EMeHdUJa4UlvSoFK7LMldcXJX1Sd2-23DbG1-tD28q9qIEpHM9QRNWgfzIgiXLk634%7EAfPh08Rc6Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=pVr8ywcnZnxdqWJi2QQyX9wnZ3NpJMG5yku7Q49i2MB4wdA1OLpHRwdaTXRj2rprGnspUC5GXVaHq0brXczbXgeLRw7fAamU9plFB6KBMGmai3lVTjgSio5%7EI7I95buP1MH2LdAYTA9x7wdnVYQJV1kc7zXCqqEWVbalcX154RMsU4Ctlix44-E9KjJv1-tqV-PIv65dAgk4ItY5dFvwquARF-zLll-IAExKmEHJ2BkQW6M78vEmSIyIr1%7EBhP%7EqVnD6FzNs%7EMeHdUJa4UlvSoFK7LMldcXJX1Sd2-23DbG1-tD28q9qIEpHM9QRNWgfzIgiXLk634%7EAfPh08Rc6Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083447&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NDd9fX1dfQ__&Signature=krxaNv2a7hE4DTbSExPvAAb3mowQ-QDXx0K7Yo15XDJYg5%7Ev-rII-oFtocoym9afGlFWaWmZHQDoCdNwVnb0mIu7ImLrMN29OvO73XB4Z8LzuCqgXxzrDFC9PaGG2Y2nEr-OcTZ8MMAIDQbNwbvABgktA60ZkcNzMWVg19Jm29SQclp6B-AJXpWVM8NbX09We4vw9YXtqo7%7EGIotojNj1iERBogeYVr3rWCKmAR3clHJ5RosEHIXwUETVJ1twa5GHUir4ln4COxh0pMIkkBPFTAG-9eqMZTcuyYq44AOhGo8OR7BKZp6eCZbqLSTDONuUo6HvxEK5PCUMFln-LaPTw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083447&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NDd9fX1dfQ__&Signature=krxaNv2a7hE4DTbSExPvAAb3mowQ-QDXx0K7Yo15XDJYg5%7Ev-rII-oFtocoym9afGlFWaWmZHQDoCdNwVnb0mIu7ImLrMN29OvO73XB4Z8LzuCqgXxzrDFC9PaGG2Y2nEr-OcTZ8MMAIDQbNwbvABgktA60ZkcNzMWVg19Jm29SQclp6B-AJXpWVM8NbX09We4vw9YXtqo7%7EGIotojNj1iERBogeYVr3rWCKmAR3clHJ5RosEHIXwUETVJ1twa5GHUir4ln4COxh0pMIkkBPFTAG-9eqMZTcuyYq44AOhGo8OR7BKZp6eCZbqLSTDONuUo6HvxEK5PCUMFln-LaPTw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083310&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMzMTB9fX1dfQ__&Signature=IPEZZhWqIRCZEKEOLSB0nV1QnVQlE4gDgJ24yCzUnziD0Y77WNApeNAWmyFl%7EEQWPiOxDkiechknhDN9RQ37ozyxKEaK42vZzMlXRIC%7EQWYN%7EFZCt616QiMfm%7EoGdh4wLmVYgYu431n2njeIDZIhGAXF01SPC8ZkLZo3PzHxNzS%7EU9dtZMnSKrAtuBLEEwwXPkrWkYHeI4XlA8avyM-HXdYygTw75avc-oHusr-2CO5B0OVISR4xoMUCXlfCRNMvI4YngfCKHLd01YyRaeOCxN8l7ycXE0kFOsC8bggr4F9YVN2cbsWc-smmA1Ab0aoEAn2Ib31BJHurNRY8omJMsg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083447&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NDd9fX1dfQ__&Signature=krxaNv2a7hE4DTbSExPvAAb3mowQ-QDXx0K7Yo15XDJYg5%7Ev-rII-oFtocoym9afGlFWaWmZHQDoCdNwVnb0mIu7ImLrMN29OvO73XB4Z8LzuCqgXxzrDFC9PaGG2Y2nEr-OcTZ8MMAIDQbNwbvABgktA60ZkcNzMWVg19Jm29SQclp6B-AJXpWVM8NbX09We4vw9YXtqo7%7EGIotojNj1iERBogeYVr3rWCKmAR3clHJ5RosEHIXwUETVJ1twa5GHUir4ln4COxh0pMIkkBPFTAG-9eqMZTcuyYq44AOhGo8OR7BKZp6eCZbqLSTDONuUo6HvxEK5PCUMFln-LaPTw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE5MTB9fX1dfQ__&Signature=yRnYQKlo3Wj1h8el6WrZzh9XR-BiKQKrPLm5beqxzcib6zAotVuKeDmg%7EOfzKHzfKtQR6uB41Op-XZsy7n7dgDbVSchyBLCi01T3IUB3uj-ntFsMKARajqrxinbQcCdjkY7Cr0XKURJubCvsabp1O5UnNNYelFmbsbNeCNseBziUF3kd8tN4YjOALdaw9ilMSXAtzYaP1d1CTO-U%7EoPAtazBxsaHQXaoUl4LFhG%7EFmVQSVxand1uAeH%7Ec896iSUStxL5P1tKdTfr9z3JydQ8frY6moDVqTh%7EPUMXLVAJC77NVuT0tPdBwNa8LTflEdoO%7EJ0j5v-lwXkkQ25%7EA9M1Hg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=MkdWCYBLHifFNXJWSL9otlhUb96RvXFFgM4N-%7ElJBfpK4Cwb2W3Z3bnqoqnC4BypLMHILTbt2BDeTbyz54Bp%7EpdlZqpT2ddm3wiDll51Isq7MFYdxyQBr2ze8jz9KphqCnfP%7E9E9tK-vaRLjIQHsxNe1MB6RFpzPWi1BvWOlULMbKpjt51vKygxo42Utnh0wwt186XLIyziyM7uroCnO92xwcK9qQTYAuRekNFG-V5l7CCqd4SudOj4rveAEukYu7VewZU5qmvsbIgLe6d79TbMh1RcKRivlCJs2ZwX8eTo0mg3QglZsu%7EH8MAoq8j6seBqjy6oGe2wlIvDY0MpyTA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081878&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE4Nzh9fX1dfQ__&Signature=VW58NhsTRXgZ8qoHpt%7E99vSYM1DYicsobVK5Li2-YsZfpMPtLB96SaubS5HEz44jYwtPvRLpbl23lMiPrq34OYuPeGpqI2eiJuQOU6k0BIoQXwzqoiFP%7EpRMg4K3SgKAJlGT3mVwrI3sGXYNwUvdZD1vc2ANrgANF3fcc%7EUetGKVoxmjw4Zv516R-LAQBPUNe8DGXBCV4JVXnrQ0pFNTXWJeZgqhcaQkgCCp4fHQmrFleY8oqq2v1SejncVJj56isysUP3CZ3xnt3aJjhTz2G4xGsWjK8mfljlkfPTFoF2wfdf8tvsQBthxN8Qwg5dDkdUhM33k2A9wg5IlfZVZKnA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=jLU5ITQv3H%7EYi3ELIEcQtkCMizIPhean8BRe9BD4iXOCHRRtrwb34vb-tvzFnrlhB3kyjzINwvKuQZx7Un%7EWMcJn3g9F4rFg3SSF0rw8Pp3ddND%7ExFXXoAlFbrCEnxpjPHBIxIXjAgMd3IfWA-GWLUph2a7pRduprdEST9yy-lKddSf7hCHhCcPZ7080-eu%7EKkDJ32CUIjV3B3NXmifaPw52QLBtPq1TSuBVFw%7EZZyp46loeTtPT0UdyObmKrfk5wxE8tuaRo-ynomZza91klPAFkRDXAgKNZf7Dx1Q8mCRnjdS9QQuYx6T0Fay5E4nH4M1608ZFOpbEZGUD1ZXl1g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=pVr8ywcnZnxdqWJi2QQyX9wnZ3NpJMG5yku7Q49i2MB4wdA1OLpHRwdaTXRj2rprGnspUC5GXVaHq0brXczbXgeLRw7fAamU9plFB6KBMGmai3lVTjgSio5%7EI7I95buP1MH2LdAYTA9x7wdnVYQJV1kc7zXCqqEWVbalcX154RMsU4Ctlix44-E9KjJv1-tqV-PIv65dAgk4ItY5dFvwquARF-zLll-IAExKmEHJ2BkQW6M78vEmSIyIr1%7EBhP%7EqVnD6FzNs%7EMeHdUJa4UlvSoFK7LMldcXJX1Sd2-23DbG1-tD28q9qIEpHM9QRNWgfzIgiXLk634%7EAfPh08Rc6Ag__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=jLU5ITQv3H%7EYi3ELIEcQtkCMizIPhean8BRe9BD4iXOCHRRtrwb34vb-tvzFnrlhB3kyjzINwvKuQZx7Un%7EWMcJn3g9F4rFg3SSF0rw8Pp3ddND%7ExFXXoAlFbrCEnxpjPHBIxIXjAgMd3IfWA-GWLUph2a7pRduprdEST9yy-lKddSf7hCHhCcPZ7080-eu%7EKkDJ32CUIjV3B3NXmifaPw52QLBtPq1TSuBVFw%7EZZyp46loeTtPT0UdyObmKrfk5wxE8tuaRo-ynomZza91klPAFkRDXAgKNZf7Dx1Q8mCRnjdS9QQuYx6T0Fay5E4nH4M1608ZFOpbEZGUD1ZXl1g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=bsLcxojtmXYbsL8depOSD5q2MJ1Wdbm5AoFvyJ0loTdTRQmp3Mt32gPXK79b3uTKl2z96KDvYGmZO8lNJ3QOA0%7EKtsXDUL3NiiBcNzWemGSJtQIRwzQmSPMY%7EgCVe5lS--1%7ECYiuLDMTSriXjQra8ErtMotUN7G78KmuoSs%7EcZnVz6Jfn7rk55SNvtLOimYcdfyy5tz3jod6I5QL1TPa%7EbKR8orpLmzdI1-D0acPGpvDhAVr0Qei4F-c8PE4VOtmqg2tdiWe%7E7EyzaY-NJZea7FvAQ4j90l19Ea1YNMysNDYlZDQUVWyN26pmVhFYwXmT-1-nIPFdhpHmteVRj8C7A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "\u001b[35m[\u001b[0m#113c7e 180MiB/8.8GiB\u001b[36m(1%)\u001b[0m CN:16 DL:\u001b[32m210MiB\u001b[0m ETA:\u001b[33m42s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mCVe5lS--1%7ECYiuLDMTSriXjQra8ErtMotUN7G78KmuoSs%7EcZnVz6Jfn7rk55SNvtLOimYcdfyy5tz3jod6I5QL1TPa%7EbKR8orpLmzdI1-D0acPGpvDhAVr0Qei4F-c8PE4VOtmqg2tdiWe%7E7EyzaY-NJZea7FvAQ4j90l19Ea1YNMysNDYlZDQUVWyN26pmVhFYwXmT-1-nIPFdhpHmteVRj8C7A__&Key-Pair-Id=KVTP0A1DKRTAX_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=bsLcxojtmXYbsL8depOSD5q2MJ1Wdbm5AoFvyJ0loTdTRQmp3Mt32gPXK79b3uTKl2z96KDvYGmZO8lNJ3QOA0%7EKtsXDUL3NiiBcNzWemGSJtQIRwzQmSPMY%7Eg\n",
            "\u001b[35m[\u001b[0m#a17340 188MiB/8.8GiB\u001b[36m(2%)\u001b[0m CN:16 DL:\u001b[32m221MiB\u001b[0m ETA:\u001b[33m40s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083828&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4Mjh9fX1dfQ__&Signature=cALycggqm0aNzr6u4Ds8bGF7PvaaDC0zEhRN0sTTWeEPEGbHhwk37JSmsZqGIcy0W4RHhvwSCOmujj3BdVBLXRr0hi2cdnDVGG7DIHWEF7iMS2NCYVsi0JIUfAYz-8i7NttM8qMbjm19jR2XOFLtUJEAte7vkEsqm-XdEtiEULKSi76vB-jFydM-1YmcDk5Tw3k7cw%7EZHsMmRKls%7EvsSei-IS3aAwO1GpFhnPDklc6CgMzkPCvmn6fYKOPOet9rzdwJwVIFhzAOp7xM9GvwvKVTsjf5Ma1A45nwMzqTfvf66VngUjdlLY8GQW3ZAWc-dOp0eapNfDGEZu4AsbBjhyQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:09 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=ymjWSYjwM6RxZKhSxLDDqka4Sgi7frBgUiyellsSH6nQT1hpP-McVIyR8g%7EEHp%7E3MxQMNMqxlwBYgwCBr7h-gwNbgu26xMpSpWEChs2D2KiLEdI6g64ogkwAIkaUxppKRh2CVwK2iC5OdfgXJV2b5G%7EKoQakiuIz8IKNSiPJjGWYLM23MYNgj5P7hknBgxPABDG7DBo6v898XyWMnM5w0kLYj8AAzaRZRiJ9H7f71NRb3oHZe7IBEy8WVN0l1%7E35RHzIEoQGzrvFpGjUNbzwEwjsPUSknxrf4ySM0ym-usLJNLNuXOq8-zP5quJMkaMGvnI5LYF2kjt8aQRbI28yrA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#f0ab05 23MiB/8.8GiB\u001b[36m(0%)\u001b[0m CN:16 DL:\u001b[32m37MiB\u001b[0m ETA:\u001b[33m4m3s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mm\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c3f07c8ec31edebe5ff80ed32d175475a044a0517d84c5eff6f6247ad0a4432d?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00007-of-00009.bin%3B+filename%3D%22pytorch_model-00007-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzNmMDdjOGVjMzFlZGViZTVmZjgwZWQzMmQxNzU0NzVhMDQ0YTA1MTdkODRjNWVmZjZmNjI0N2FkMGE0NDMyZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=jLU5ITQv3H%7EYi3ELIEcQtkCMizIPhean8BRe9BD4iXOCHRRtrwb34vb-tvzFnrlhB3kyjzINwvKuQZx7Un%7EWMcJn3g9F4rFg3SSF0rw8Pp3ddND%7ExFXXoAlFbrCEnxpjPHBIxIXjAgMd3IfWA-GWLUph2a7pRduprdEST9yy-lKddSf7hCHhCcPZ7080-eu%7EKkDJ32CUIjV3B3NXmifaPw52QLBtPq1TSuBVFw%7EZZyp46loeTtPT0UdyObmKrfk5wxE8tuaRo-ynomZza91klPAFkRDXAgKNZf7Dx1Q8mCRnjdS9QQuYx6T0Fay5E4nH4M1608ZFOpbEZGUD1ZXl1g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=bsLcxojtmXYbsL8depOSD5q2MJ1Wdbm5AoFvyJ0loTdTRQmp3Mt32gPXK79b3uTKl2z96KDvYGmZO8lNJ3QOA0%7EKtsXDUL3NiiBcNzWemGSJtQIRwzQmSPMY%7EgCVe5lS--1%7ECYiuLDMTSriXjQra8ErtMotUN7G78KmuoSs%7EcZnVz6Jfn7rk55SNvtLOimYcdfyy5tz3jod6I5QL1TPa%7EbKR8orpLmzdI1-D0acPGpvDhAVr0Qei4F-c8PE4VOtmqg2tdiWe%7E7EyzaY-NJZea7FvAQ4j90l19Ea1YNMysNDYlZDQUVWyN26pmVhFYwXmT-1-nIPFdhpHmteVRj8C7A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082057&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIwNTd9fX1dfQ__&Signature=o3otmJ4ZJ77pmsQKQhIqMR0af7XZsJkZQdsY8sb0YrKKaP%7E8TgB8lRiNBDc3zyc7-f1muG-QNXKOAMy0y%7EHNRkoe1BxMLkfUgD2PFo3qArAQUdVruxmDjv1ZKG0SuzG-9U8ZL9%7EfJgR3FM7-Fo0cTPmiE9F0rFTInoFpjhGAqujzCuEM2wsTW9gTa9hJiQYSy8zu4hKaACFyg3hebVRC8x3-WiA0-fsr1P4boOYWq5tUpOllAdCufLz-E2REyGKiwz-klvaf%7Ejh7SOq1q2S-chRkV0N6Rp92gt44L2KQcZBzgKm-pD7jTwzegzk5tluhk7rvBmpRE7RqGjjFagQzyw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081878&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE4Nzh9fX1dfQ__&Signature=VW58NhsTRXgZ8qoHpt%7E99vSYM1DYicsobVK5Li2-YsZfpMPtLB96SaubS5HEz44jYwtPvRLpbl23lMiPrq34OYuPeGpqI2eiJuQOU6k0BIoQXwzqoiFP%7EpRMg4K3SgKAJlGT3mVwrI3sGXYNwUvdZD1vc2ANrgANF3fcc%7EUetGKVoxmjw4Zv516R-LAQBPUNe8DGXBCV4JVXnrQ0pFNTXWJeZgqhcaQkgCCp4fHQmrFleY8oqq2v1SejncVJj56isysUP3CZ3xnt3aJjhTz2G4xGsWjK8mfljlkfPTFoF2wfdf8tvsQBthxN8Qwg5dDkdUhM33k2A9wg5IlfZVZKnA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687081550&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODE1NTB9fX1dfQ__&Signature=ZtCkNRQY-Sn6jtTO3AzuYnx5KhfNT5UFpx-lvoGGIZj1HgCevLNECu60Fe559VvL6j664YFYgiMixyJU1R6%7ERrios8SH1bIq6o8jWhZ28DUdd4x9-3BN8fa57c1j-Y58B78kmzHyJTVa6XciJnXI8V69SnEL00MWRzfFQH9efMbKxLaZCoipcMxDiWJhwc6WYW6Aom343B7Yjcbkzz-KZ9xOVzvJfXWO7g1ynCu8jiixkwlQMJHVsHhllBeWXDmAUCodRLvKoiQTCcN4HqU-mVigIS%7E5bMgYzTGKiRHPMLQJDnaLG1uxLGuN2kqv43jDHfP9DMpa47Y%7EUfsyQxFrsw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=g6BE7qHAu0WVo-4YPe4Ee-N9k2yaJkDAXIrdtS849vS-OXm6KcoucGVgtA-5wjOH2tCdFu5bEGW-9ByGRetzyD-26Rn0k3iZ2S7hyCf9GNqvJLxpZsGa3RQJjsjl%7EYAiyX2B%7Ej7g8HC0ELZhbmu6aUyqwIIoxuw2apmDeLt0S62AaaIbbr3SmP0%7EB6kIJvw4XCe6WZNqdyeFQhRGDhX3VOHuOAydcfUNB6fUAEJ54e8d1hIFO63lBAGoQAhCvUnZoCZLOHlxLJzskHPxDOuP32C3nNqn-%7Eaonlv6W4wMVQOWrDPk3Kz15MfHqbigLz3FUfcRZVqgNCfO6OokvNzSwA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082537&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI1Mzd9fX1dfQ__&Signature=EmKywoctY5AI6xnes8TdFymVBxLCHwSmFo936kVdCddfEYbWO8%7E2brnf8U9d51xbldWdYdCnSFCXBolsXqvyDAncQww5fk5T88lCB99%7ElNUrfrbkEIM-%7EKpVFqyAqfNiBA5qBimIkJ%7EQyvfeMEi7UiAhztbXtHCVELh0E5dw4lxJfH%7EANj9HdCu1uen0Z8fPS1BIgA%7EVGKMPXocj4U1syrw7qMwRBOSIi3342GhUPV2jvQfDyOqf9rxk2y4QXailqm1r1KbIm7OueGIKvIluPlUx9XxS0YBho4KWKnVhYH51IgbM98y67ai4WDgSB812bjLrIrUazCpZ5otmxH0m9g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=yxvktjsn5MiynAlkxKLcumpcMATnG0WW%7ECKT1kHZLaRk4XN7Tbu9QlBr5rR5CkWV1J1FUo3IWwq8mnHepKkPqVv1xJ4J6X32p6CmEirlsd7uYnqL9RyJWQ1KBHMskGZxV-E1p2Vxiu36LeAOKvc1gxVLVeL5WaaIOBou3y0e6INefpDkiJ-IMDu1oBRhvRRqnyJvrK8Yys-eAzCFfBb81xDCFiUrlotgN6Pe0R0CQlUihR3h-REMOoPbayQuZhgmjqfdpfjdZFL7aB0uO5AHGcLjC5%7E5vax7K1SMMixwwVjlo-655wmmEpvbF-KaofAGsEZ7nrQsE9UNr2-WrVaTJQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/7af5021fe9afb9b4186a59a91e75aa60570c66fba43390b634387ee54bd73223?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00008-of-00009.bin%3B+filename%3D%22pytorch_model-00008-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvN2FmNTAyMWZlOWFmYjliNDE4NmE1OWE5MWU3NWFhNjA1NzBjNjZmYmE0MzM5MGI2MzQzODdlZTU0YmQ3MzIyMz9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=bsLcxojtmXYbsL8depOSD5q2MJ1Wdbm5AoFvyJ0loTdTRQmp3Mt32gPXK79b3uTKl2z96KDvYGmZO8lNJ3QOA0%7EKtsXDUL3NiiBcNzWemGSJtQIRwzQmSPMY%7EgCVe5lS--1%7ECYiuLDMTSriXjQra8ErtMotUN7G78KmuoSs%7EcZnVz6Jfn7rk55SNvtLOimYcdfyy5tz3jod6I5QL1TPa%7EbKR8orpLmzdI1-D0acPGpvDhAVr0Qei4F-c8PE4VOtmqg2tdiWe%7E7EyzaY-NJZea7FvAQ4j90l19Ea1YNMysNDYlZDQUVWyN26pmVhFYwXmT-1-nIPFdhpHmteVRj8C7A__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083824&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM4MjR9fX1dfQ__&Signature=KAJ7vxy19seH-6l9LRqNYUXpzb2mTVeARqfdeo7PVpEdW-Shk3WNhQVbxxqplMeMLbm2kWzQXWwrQ13aY7qPgHMNhYDSqfRGvQZRD1qmU%7EX5TMyK3lnJq0R0c9Uk6nfHWqkudp8T8ubd2fskieiKJrs153ptxdwjcG5QxVk3r01Jto8XNxPcJyfURnnwSDaH2WCX3uM%7EulmLh%7EYFqZqzv3Ybm4d1KA2saDoj8c3LyX54LZ4ZeNFOn9LK5KMlYG5i2JUEJ01Ms34vrG4Y%7EPNsZ9Pug5xzgGR%7EGnNqSeEhCz-fFf7CdEPrTFVxaJLcTVoZok3oE76eJq61v3euY8W%7E1g__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083909&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MDl9fX1dfQ__&Signature=HKAxCmoJnTjOdqg%7EF5SOOUz2sq2crgvDYX-fkvMMxIosFJKJs8vkJlH9dfXoaNmKrKbu1HzeVfjndBPIJLCK3A2cUVkD8lE%7EVyOOWSoXJMUwqqIQKXraXWt6Mnt%7EAKZEjHAFMOMiXFQCe%7E9cFVtfoezTO%7EfrFkZn9uaU6hIIuSO0OkX3eHuKzVV4a-ahfbDxNZgVLcS2M%7EmQ9U1JlmNhM806tEDMfm3-jdA4etkTyOodSh1nX7vjLwXWsqxXEH3dMXnTl%7ESKDWPZkRRU78IsFXRPm7lwaISz85s3ebEmNXqn1k0E8SfwJFWhD5Hjx04tadeGY0e2qpiQ3L23ozkMfA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=yxvktjsn5MiynAlkxKLcumpcMATnG0WW%7ECKT1kHZLaRk4XN7Tbu9QlBr5rR5CkWV1J1FUo3IWwq8mnHepKkPqVv1xJ4J6X32p6CmEirlsd7uYnqL9RyJWQ1KBHMskGZxV-E1p2Vxiu36LeAOKvc1gxVLVeL5WaaIOBou3y0e6INefpDkiJ-IMDu1oBRhvRRqnyJvrK8Yys-eAzCFfBb81xDCFiUrlotgN6Pe0R0CQlUihR3h-REMOoPbayQuZhgmjqfdpfjdZFL7aB0uO5AHGcLjC5%7E5vax7K1SMMixwwVjlo-655wmmEpvbF-KaofAGsEZ7nrQsE9UNr2-WrVaTJQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#f0ab05 127MiB/8.8GiB\u001b[36m(1%)\u001b[0m CN:16 DL:\u001b[32m78MiB\u001b[0m ETA:\u001b[33m1m53s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:25:10 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083910&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTB9fX1dfQ__&Signature=yxvktjsn5MiynAlkxKLcumpcMATnG0WW%7ECKT1kHZLaRk4XN7Tbu9QlBr5rR5CkWV1J1FUo3IWwq8mnHepKkPqVv1xJ4J6X32p6CmEirlsd7uYnqL9RyJWQ1KBHMskGZxV-E1p2Vxiu36LeAOKvc1gxVLVeL5WaaIOBou3y0e6INefpDkiJ-IMDu1oBRhvRRqnyJvrK8Yys-eAzCFfBb81xDCFiUrlotgN6Pe0R0CQlUihR3h-REMOoPbayQuZhgmjqfdpfjdZFL7aB0uO5AHGcLjC5%7E5vax7K1SMMixwwVjlo-655wmmEpvbF-KaofAGsEZ7nrQsE9UNr2-WrVaTJQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:11 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/2fc011e39d4a9c5958630b1bb946dec9ca54adbccc52804fed545490f995b20b?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00004-of-00009.bin%3B+filename%3D%22pytorch_model-00004-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083566&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvMmZjMDExZTM5ZDRhOWM1OTU4NjMwYjFiYjk0NmRlYzljYTU0YWRiY2NjNTI4MDRmZWQ1NDU0OTBmOTk1YjIwYj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM1NjZ9fX1dfQ__&Signature=tUyi5IIc-QmOhBDGFiJGQ2%7EFNxiUtVQE8hxEloc2xcSDi7Q1vRQkHPlY3tVnVLNsxFd9Y9V%7E0oyl6ALS1PP1gigDgaFFKK0ZNbBe5rGvTNNoJ3EPZkIBKSpnMitxbKVCaZ2EKr5LKMkJU52VQqBWDI-%7EQDsS1YSJ6fQYLTbI%7EdODApj1G8t2Ymph9mbGuz2COnsvwyCL8pfjWQUOTrzbjaI43kadqRMgwkKA0pfHtUVyJ0PqT6QK0uxppUYYschml01WvexuIL-V-ws%7E-sdj3cYTameWysyz9t4p48sdXI9eCb12m5Ux1wNtoBZtwEIwMBFbxq0B1lqnjMNMVrpjRg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:11 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083911&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTF9fX1dfQ__&Signature=sKAC0D9wRtY%7EBTgkDN4Thq2QsHS9PLjFzAnu8CSOESJoTisWndllX4OByCZTudtaseJVvz0vX-u6sOwi7TLkR4IfPlRXfiZwUavQHThtr2nZo7tNJbsSsF%7EFocyyz7t7T5PkOOIiS728ZywVCPkaJ2cYSG2iHKYIsAWSP28fMWEGHdsAqM8txaecSJURWe8b8pGswZxG-GeqEnnqKOkSlo077-to7zJOTjpe6zasHyAOxnmJuBf7h0OfnTJEM8v2aDbS2XHHINnfvlFtwC7gsINGim0THx3U7UrYk4ZRvoEWjVLyRMgGgj6sgxBqFXjwO2uQO5AB-fUrugQY6nUjzg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:25:11 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/ae277c7c5aef0d0fea9e9e6b13113dfef0078a36c78805bebe14d32e9ab887d1?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00005-of-00009.bin%3B+filename%3D%22pytorch_model-00005-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083912&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYWUyNzdjN2M1YWVmMGQwZmVhOWU5ZTZiMTMxMTNkZmVmMDA3OGEzNmM3ODgwNWJlYmUxNGQzMmU5YWI4ODdkMT9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5MTJ9fX1dfQ__&Signature=RPW9uBqx%7Ea93SMBl%7EvOSGE9yAof18EfdMpKwiE-8fLmR4NXqocZUfzxmqH8ilstHzR5pN7eOqo2Adf274SSi9AxsfX3JkWgvgLbG96ZGyBeSKTPWSKJRw80WLTFOx1%7EwwXDEP9zlqeMgHsRf74HrDl9R9EY6xkqt9uU2qJYG0GpFb7gkThMrG5CIP%7EGzmRGVvyyMhboXhZUWJbGgYTVEeTPeTRAXVGASqcZZPhyv6QYvsc-afFFu6kxTM9wKnnxkFh4clxLzPyYX0N%7EKSGjEHaXHciTf7RRLg8YSHU51WG3hmtxoNifQGbMB3HENeszWcFCSogXl4ZoLw7wkuF7Fiw__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m7s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mmmm\n",
            "===============================================================================\n",
            "[#925cf4 8.0GiB/8.8GiB(90%) CN:16 DL:106MiB ETA:7s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00003-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m7s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#e34534 7.2GiB/8.8GiB(82%) CN:16 DL:96MiB ETA:16s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00004-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              16s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#113c7e 8.5GiB/8.8GiB(97%) CN:16 DL:108MiB ETA:2s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00001-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m2s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#a17340 8.7GiB/8.8GiB(98%) CN:16 DL:106MiB ETA:1s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00002-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m1s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#f0ab05 7.9GiB/8.8GiB(89%) CN:16 DL:102MiB ETA:9s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00008-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m9s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#c9ee63 7.8GiB/8.8GiB(89%) CN:16 DL:101MiB ETA:9s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00007-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m9s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#1adf34 8.4GiB/8.8GiB(95%) CN:16 DL:108MiB ETA:4s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00006-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            " *** Download Progress Summary as of Thu Jun 15 10:26:11 2023 ***              m4s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "===============================================================================\n",
            "[#039afc 8.3GiB/8.8GiB(94%) CN:16 DL:106MiB ETA:4s]\n",
            "FILE: models/tiiuae_falcon-40b/pytorch_model-00005-of-00009.bin\n",
            "-------------------------------------------------------------------------------\n",
            "\n",
            "\u001b[35m[\u001b[0m#039afc 8.5GiB/8.8GiB\u001b[36m(96%)\u001b[0m CN:16 DL:\u001b[32m111MiB\u001b[0m ETA:\u001b[33m2s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mm\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00002-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "a17340|\u001b[1;32mOK\u001b[0m  |   143MiB/s|models/tiiuae_falcon-40b/pytorch_model-00002-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model-00009-of-00009.bin -d models/tiiuae_falcon-40b -o pytorch_model-00009-of-00009.bin\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#7 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082403&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0MDN9fX1dfQ__&Signature=YQE2qLSV-J0Pq%7ESUm8NUk6tsxqsfAFgCnoiAt0CTahBeSwwjemF5DW4mk7K3qpepLNgWCrAkXBTnZe6Cvt5%7EZMA5eTc98sbZueen1h4azQCWXwSVDlAwMd3DSGxzhjNtVPgqERJ8a2Pe2H4yPr9OaEq3MtGw7UyIXYNWuIriNWGh7WUDkkZgQ9Vac2yTW7l6BGjVAQQ6hM-ZsD86ncaunJlCnQm1kvkvCT3NgQIWYE6tfIyw1c46i0nBC%7E32xlIOizNVbfh%7EbvbmL0ffjPCc9gxbZOk3NVrCc9yw7Va92NqVQGdsHjhderaJZgMsJ6TPe-eAFgqTGyptFUhRO76-8w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#10 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083453&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NTN9fX1dfQ__&Signature=B7YJ6UzwaRyLF7WiQ6m4K6vdWlYf74nWdlSluIluo-cOWlxXl8vK7CgAgKBal8h2pxyFmqKWFGu3c40Y%7EMTm%7EABDNZNY3w2eW1xOl8-letDwwToTvRjQ7VU8BsZ6Quu5j1-A6norLt82g%7EKtRFOaOYvvGseayQnmdvpvXnI4BzUcMJRw5DKyTvMhcW1T-NH4wtnSvTZTfQB%7EmLWXdMy7FmuAbsSOfA17HBpjpLYIVcNKn1Y33sdIOq3MZFBBppx%7EEo0cQLwgWgS76Kj0kXkozVavL55B0Fio2J8Om%7EF8Ber7EfgTUbLuwMM2enNFV6JHW2vrJoJqgXZkl-AWo9%7EbxA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#9 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#18 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082482&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0ODJ9fX1dfQ__&Signature=yvVY2x1BDjQUb8omusRSoIPCGpr0pLeOrF9kTz0jA%7EF2kqb91zC6ZXfR5ivEMMedJqL%7EPlGwnQp23n-e1OhTloAPPEVWXtNVKV0Jsy%7Ev7gkOhNfPxv5zWDihcdS1X4OBWy0BF6L3do8ufLkhGDZlMcUHXS%7E2Qb05Fw5bDgU63cY7vxde%7Emw52LgpmoZegearUReqK8nasj%7EBk1cn5v3tdVKjZgwRl2eZkLalXSgfVITnxJdM3yMqjtZhe0ujD4JmpvR8PwycEdmCNo50MBVWBM%7E9kQPbPO9BLbeBfP81VlwWtgEHTLQGiQ7yo6cng-zcBBGsk51Yco9VTMkclupzxA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#22 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083453&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NTN9fX1dfQ__&Signature=B7YJ6UzwaRyLF7WiQ6m4K6vdWlYf74nWdlSluIluo-cOWlxXl8vK7CgAgKBal8h2pxyFmqKWFGu3c40Y%7EMTm%7EABDNZNY3w2eW1xOl8-letDwwToTvRjQ7VU8BsZ6Quu5j1-A6norLt82g%7EKtRFOaOYvvGseayQnmdvpvXnI4BzUcMJRw5DKyTvMhcW1T-NH4wtnSvTZTfQB%7EmLWXdMy7FmuAbsSOfA17HBpjpLYIVcNKn1Y33sdIOq3MZFBBppx%7EEo0cQLwgWgS76Kj0kXkozVavL55B0Fio2J8Om%7EF8Ber7EfgTUbLuwMM2enNFV6JHW2vrJoJqgXZkl-AWo9%7EbxA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#23 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#20 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:12 [\u001b[1;32mNOTICE\u001b[0m] CUID#19 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082315&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODIzMTV9fX1dfQ__&Signature=sXoyZXM0CkKEw7GPUKPDhG5umbtWjoBo23vNR27%7ElDvKhbTCutHJhLukTj4SqfWOPhA7W6IXc7OyvAPRJ1DvJVUZDbddMHSwk%7EWSl%7EhIwLEowYdr-%7EGYiCeaFkj2egf8rZeWgveB5WAUKXm2cfpk8FIJ%7EWtbadZOo26k1huQdyHIQJNoz1%7ELVESJnTpByD%7EUFXzixmzaNw8-9Emo0pdCrKNyx0XNKmMjR0-PXGzudfEBr-SPFVt0ODEIt2TQL5WZWbawfSkw-feJ%7EJ8sh0McgaGCuVQOwqEzRNhDQCWdvt1iGCZyGiRGYbc7hUQvm9eOsq08x%7EIFSV0vdWFNS3eaGg__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#925cf4 8.5GiB/8.8GiB\u001b[36m(96%)\u001b[0m CN:16 DL:\u001b[32m133MiB\u001b[0m ETA:\u001b[33m2s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#11 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#15 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083016&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODMwMTZ9fX1dfQ__&Signature=zoWfqaGBaR5N5zRQRJLHjR0m7wqI5z-9JxaYhj5r1FsLN0NjbTvsq56OlnuFZ6SseUC0HtzRQiPj9ZyfhC0NTB-8ki7oIxSZbgVVMRUU4wYCSxUn3WVtYV93vyHE75qHu0NPSWE1pb%7E4kIjkmJxeAjTp8q2QFIh2ZnKTjTzaNriJ22BdMyerEuXm6rWyER3c0kqXIS6aAfS0kgqMI5bghiy7wBPD793PbWtfOSe1CWEJpMV5E9pbofLTJgJs355i7s2SJ8Idbt93Rhpz7cvdRLPyRBYR1FPizIno1mMxX17fpXefMU1uJFc%7Erakt7ak0SxOzTcwOBWMzPgUaD4ESdQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#12 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#e34534 7.6GiB/8.8GiB\u001b[36m(86%)\u001b[0m CN:16 DL:\u001b[32m112MiB\u001b[0m ETA:\u001b[33m10s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00001-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "113c7e|\u001b[1;32mOK\u001b[0m  |   141MiB/s|models/tiiuae_falcon-40b/pytorch_model-00001-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            " 33%|██████████████▋                             | 6/18 [01:04<02:08, 10.72s/it]Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/pytorch_model.bin.index.json -d models/tiiuae_falcon-40b -o pytorch_model.bin.index.json\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#14 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082482&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0ODJ9fX1dfQ__&Signature=yvVY2x1BDjQUb8omusRSoIPCGpr0pLeOrF9kTz0jA%7EF2kqb91zC6ZXfR5ivEMMedJqL%7EPlGwnQp23n-e1OhTloAPPEVWXtNVKV0Jsy%7Ev7gkOhNfPxv5zWDihcdS1X4OBWy0BF6L3do8ufLkhGDZlMcUHXS%7E2Qb05Fw5bDgU63cY7vxde%7Emw52LgpmoZegearUReqK8nasj%7EBk1cn5v3tdVKjZgwRl2eZkLalXSgfVITnxJdM3yMqjtZhe0ujD4JmpvR8PwycEdmCNo50MBVWBM%7E9kQPbPO9BLbeBfP81VlwWtgEHTLQGiQ7yo6cng-zcBBGsk51Yco9VTMkclupzxA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#f0ab05 8.2GiB/8.8GiB\u001b[36m(93%)\u001b[0m CN:16 DL:\u001b[32m109MiB\u001b[0m ETA:\u001b[33m5s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model.bin.index.json\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "b0a95a|\u001b[1;32mOK\u001b[0m  |   5.3MiB/s|models/tiiuae_falcon-40b/pytorch_model.bin.index.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/special_tokens_map.json -d models/tiiuae_falcon-40b -o special_tokens_map.json\n",
            "\u001b[35m[\u001b[0m#c9ee63 8.1GiB/8.8GiB\u001b[36m(92%)\u001b[0m CN:16 DL:\u001b[32m104MiB\u001b[0m ETA:\u001b[33m6s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\u001b[35m[\u001b[0m#039afc 8.7GiB/8.8GiB\u001b[36m(98%)\u001b[0m CN:16 DL:\u001b[32m114MiB\u001b[0m ETA:\u001b[33m1s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/special_tokens_map.json\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "72abd1|\u001b[1;32mOK\u001b[0m  |        n/a|models/tiiuae_falcon-40b/special_tokens_map.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/tokenizer.json -d models/tiiuae_falcon-40b -o tokenizer.json\n",
            "\u001b[35m[\u001b[0m#cc5131 66MiB/7.0GiB\u001b[36m(0%)\u001b[0m CN:16 DL:\u001b[32m120MiB\u001b[0m ETA:\u001b[33m59s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#17 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082403&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0MDN9fX1dfQ__&Signature=YQE2qLSV-J0Pq%7ESUm8NUk6tsxqsfAFgCnoiAt0CTahBeSwwjemF5DW4mk7K3qpepLNgWCrAkXBTnZe6Cvt5%7EZMA5eTc98sbZueen1h4azQCWXwSVDlAwMd3DSGxzhjNtVPgqERJ8a2Pe2H4yPr9OaEq3MtGw7UyIXYNWuIriNWGh7WUDkkZgQ9Vac2yTW7l6BGjVAQQ6hM-ZsD86ncaunJlCnQm1kvkvCT3NgQIWYE6tfIyw1c46i0nBC%7E32xlIOizNVbfh%7EbvbmL0ffjPCc9gxbZOk3NVrCc9yw7Va92NqVQGdsHjhderaJZgMsJ6TPe-eAFgqTGyptFUhRO76-8w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#13 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687082403&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODI0MDN9fX1dfQ__&Signature=YQE2qLSV-J0Pq%7ESUm8NUk6tsxqsfAFgCnoiAt0CTahBeSwwjemF5DW4mk7K3qpepLNgWCrAkXBTnZe6Cvt5%7EZMA5eTc98sbZueen1h4azQCWXwSVDlAwMd3DSGxzhjNtVPgqERJ8a2Pe2H4yPr9OaEq3MtGw7UyIXYNWuIriNWGh7WUDkkZgQ9Vac2yTW7l6BGjVAQQ6hM-ZsD86ncaunJlCnQm1kvkvCT3NgQIWYE6tfIyw1c46i0nBC%7E32xlIOizNVbfh%7EbvbmL0ffjPCc9gxbZOk3NVrCc9yw7Va92NqVQGdsHjhderaJZgMsJ6TPe-eAFgqTGyptFUhRO76-8w__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#16 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083453&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM0NTN9fX1dfQ__&Signature=B7YJ6UzwaRyLF7WiQ6m4K6vdWlYf74nWdlSluIluo-cOWlxXl8vK7CgAgKBal8h2pxyFmqKWFGu3c40Y%7EMTm%7EABDNZNY3w2eW1xOl8-letDwwToTvRjQ7VU8BsZ6Quu5j1-A6norLt82g%7EKtRFOaOYvvGseayQnmdvpvXnI4BzUcMJRw5DKyTvMhcW1T-NH4wtnSvTZTfQB%7EmLWXdMy7FmuAbsSOfA17HBpjpLYIVcNKn1Y33sdIOq3MZFBBppx%7EEo0cQLwgWgS76Kj0kXkozVavL55B0Fio2J8Om%7EF8Ber7EfgTUbLuwMM2enNFV6JHW2vrJoJqgXZkl-AWo9%7EbxA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\n",
            "06/15 10:26:13 [\u001b[1;32mNOTICE\u001b[0m] CUID#21 - Redirecting to https://cdn-lfs.huggingface.co/repos/a3/72/a37288fd25de45b6a8bbb79801907289f6c12bb870db68f75b0024193da1f047/c5c794d3106945f514b5501904506485377c95ce2cef030373c7cbe7459ef610?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model-00009-of-00009.bin%3B+filename%3D%22pytorch_model-00009-of-00009.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1687083973&Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9jZG4tbGZzLmh1Z2dpbmdmYWNlLmNvL3JlcG9zL2EzLzcyL2EzNzI4OGZkMjVkZTQ1YjZhOGJiYjc5ODAxOTA3Mjg5ZjZjMTJiYjg3MGRiNjhmNzViMDAyNDE5M2RhMWYwNDcvYzVjNzk0ZDMxMDY5NDVmNTE0YjU1MDE5MDQ1MDY0ODUzNzdjOTVjZTJjZWYwMzAzNzNjN2NiZTc0NTllZjYxMD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoiLCJDb25kaXRpb24iOnsiRGF0ZUxlc3NUaGFuIjp7IkFXUzpFcG9jaFRpbWUiOjE2ODcwODM5NzN9fX1dfQ__&Signature=V5Zu-IOxrL8dTeGkiu7e8XDXf0EkezeqUFf7akJi1AW8i9Ux7iQaqnWfhnmVljFIMoc0JIzDEATAN%7EVGsUMsCxnyj22lqPZpPQxbWyx-RApqzkYOauchM7usi7KnSuLUNML4FxtK-UjpXJB%7E8FVCEZNvManSimih1MYhk7zHoslFAb%7E%7ERrAgCWoNFosT-etLBWoljNmtqKXDTkB1hyecoWKePcB9JijvQ2YlFjly3adfsAksw-tO%7EtfSU4iaaPBI3KOJcgm28n3--WVNpGC5444Ts8AeHpXCcmsogpBX2eudwz9sqsXztVyx30scXTQWVP04zHKGcRgg6W-Izol3rQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "\u001b[35m[\u001b[0m#105934 800KiB/2.6MiB\u001b[36m(29%)\u001b[0m CN:3 DL:\u001b[32m0.9MiB\u001b[0m ETA:\u001b[33m2s\u001b[0m\u001b[35m]\u001b[0m\u001b[0mm\n",
            "06/15 10:26:14 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00005-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "039afc|\u001b[1;32mOK\u001b[0m  |   139MiB/s|models/tiiuae_falcon-40b/pytorch_model-00005-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "Running: aria2c -c -x 16 -s 16 -k 1M https://huggingface.co/tiiuae/falcon-40b/resolve/main/tokenizer_config.json -d models/tiiuae_falcon-40b -o tokenizer_config.json\n",
            "\n",
            "06/15 10:26:14 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\n",
            "06/15 10:26:14 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/tokenizer_config.json\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "256d92|\u001b[1;32mOK\u001b[0m  |        n/a|models/tiiuae_falcon-40b/tokenizer_config.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[35m[\u001b[0m#e34534 8.3GiB/8.8GiB\u001b[36m(94%)\u001b[0m CN:16 DL:\u001b[32m142MiB\u001b[0m ETA:\u001b[33m3s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:16 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00006-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "1adf34|\u001b[1;32mOK\u001b[0m  |   135MiB/s|models/tiiuae_falcon-40b/pytorch_model-00006-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[35m[\u001b[0m#925cf4 8.8GiB/8.8GiB\u001b[36m(99%)\u001b[0m CN:1 DL:\u001b[32m115MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:16 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00007-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "c9ee63|\u001b[1;32mOK\u001b[0m  |   135MiB/s|models/tiiuae_falcon-40b/pytorch_model-00007-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "06/15 10:26:17 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00008-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "f0ab05|\u001b[1;32mOK\u001b[0m  |   134MiB/s|models/tiiuae_falcon-40b/pytorch_model-00008-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "06/15 10:26:17 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/tokenizer.json\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "105934|\u001b[1;32mOK\u001b[0m  |   771KiB/s|models/tiiuae_falcon-40b/tokenizer.json\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "06/15 10:26:17 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00003-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "925cf4|\u001b[1;32mOK\u001b[0m  |   133MiB/s|models/tiiuae_falcon-40b/pytorch_model-00003-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[35m[\u001b[0m#cc5131 1.3GiB/7.0GiB\u001b[36m(19%)\u001b[0m CN:16 DL:\u001b[32m313MiB\u001b[0m ETA:\u001b[33m18s\u001b[0m\u001b[35m]\u001b[0m\u001b[0m\n",
            "06/15 10:26:17 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00004-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "e34534|\u001b[1;32mOK\u001b[0m  |   132MiB/s|models/tiiuae_falcon-40b/pytorch_model-00004-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\u001b[35m[\u001b[0m#cc5131 6.9GiB/7.0GiB\u001b[36m(97%)\u001b[0m CN:16 DL:\u001b[32m516MiB\u001b[0m\u001b[35m]\u001b[0m\u001b[0m0m\u001b[35m]\u001b[0m\u001b[0mm\n",
            "06/15 10:26:30 [\u001b[1;32mNOTICE\u001b[0m] Download complete: models/tiiuae_falcon-40b/pytorch_model-00009-of-00009.bin\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "cc5131|\u001b[1;32mOK\u001b[0m  |   448MiB/s|models/tiiuae_falcon-40b/pytorch_model-00009-of-00009.bin\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "100%|███████████████████████████████████████████| 18/18 [01:21<00:00,  4.55s/it]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!python /content/llm-playground/helper/download-model.py tiiuae/falcon-40b\n",
        "# /content/llm-playground/models/tiiuae_falcon-40b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydCdRNI9gJ78"
      },
      "outputs": [],
      "source": [
        "# Downloading shards: 100%|████████████████████████| 9/9 [19:19<00:00, 128.85s/it]\n",
        "# Loading checkpoint shards: 100%|██████████████████| 9/9 [01:30<00:00, 10.08s/it]\n",
        "# (OK):download completed.\n",
        "# 100%|████████████████████████████████████████████████████████████████████| 18/18 [01:44<00:00,  5.81s/it]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqQyPp5HbAm"
      },
      "source": [
        "### HF Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6wC1lP23B-4",
        "outputId": "a8e21deb-0b20-4a3e-a46b-7c832b5948e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# For axolotl push_dataset_to_hub\n",
        "import os\n",
        "from huggingface_hub import notebook_login, login\n",
        "# Colab:\n",
        "# notebook_login()\n",
        "# RunPod:\n",
        "login(os.environ.get(\"HUGGINGFACE_TOKEN\"), add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AigYR_DY7X"
      },
      "source": [
        "### Update axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQYJdcd7DY7X"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn5aNACDY7X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl axolotl-update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdY5tXtYDY7X"
      },
      "outputs": [],
      "source": [
        "!cp -r axolotl-update/* axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsKmIUmDY7Y"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljX3Yir2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adru8lVlzhLi"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaorAF2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9eIEonzhLi"
      },
      "outputs": [],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1Ahx1QG5xm"
      },
      "source": [
        "### Init Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBBF-lS2HVPG",
        "outputId": "d192b8a4-66b0-4f1f-fe58-fbcbff34704a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/utensil/axolotl-trained into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "!python /content/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_FP2VeXH0Un"
      },
      "outputs": [],
      "source": [
        "!ls /content/axolotl-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G86XUuQ1gJ8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOXz9ZCJgJ8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYU6HmYduRa"
      },
      "source": [
        "### Reinstall PyTorch with CUDA 11.8 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6v1nl6duRa",
        "outputId": "3ab826a5-514f-4419-98d0-e472496c35a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (2.0.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp39-cp39-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
            "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.5.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0A0aPWJzhLk",
        "outputId": "9d2fbcf9-e903-49f4-dbf6-95b19ad695fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1+cu118\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPSrhKPHIrS"
      },
      "source": [
        "### Reinstall deepspeed (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PIACM6duRa",
        "outputId": "5522ec22-c17f-4dc1-ee74-65583b0921d1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu118\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+52907a66, 52907a66, master\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-u6vhWHvt6"
      },
      "outputs": [],
      "source": [
        "!yes|pip uninstall deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIpqugJOHuhh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!TORCH_CUDA_ARCH_LIST=\"3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\" DS_BUILD_OPS=1 DS_BUILD_SPARSE_ATTN=0 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" # --global-option=\"bdist_wheel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8JzOr94dDm1"
      },
      "outputs": [],
      "source": [
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcKtKd_MdDm1"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tw4OcVHQdK"
      },
      "source": [
        "### Init Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvyUmZdktu0",
        "outputId": "1722afd2-e9ce-4c41-f8ca-fe80ed36f0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC48y25Lkqa5"
      },
      "outputs": [],
      "source": [
        "# Try no config\n",
        "# !accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5E8urQEXiL",
        "outputId": "5b0be00c-2074-46da-c5ca-21e4d4f96efb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": \"auto\",\n",
        "          \"eps\": \"auto\",\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "      \"type\": \"WarmupDecayLR\",\n",
        "      \"params\": {\n",
        "        \"total_num_steps\": \"auto\",\n",
        "        \"warmup_min_lr\": \"auto\",\n",
        "        \"warmup_max_lr\": \"auto\",\n",
        "        \"warmup_num_steps\": \"auto\"\n",
        "       }\n",
        "    },\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZJw_gCDY7J",
        "scrolled": true,
        "outputId": "90f58129-0df9-4c17-b23e-b85dc4c19cd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "# optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "# lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWOnrpzEr-1",
        "outputId": "ba3fb65b-b164-4406-8cbd-ff70423cbc26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=true\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9sbxRAElkU",
        "outputId": "1b037884-0780-431b-c9fb-aaa3f05f28d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing scripts/ft.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/ft.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import logging\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  logging.info('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  logging.info('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  logging.info('setup_trainer_ex before')\n",
        "  logging.info(f'cfg.some_config = {cfg.some_config}')\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  logging.info('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6CD_zZHUpG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqrhYq-DY7J"
      },
      "source": [
        "## #1 DS Rountine Attempt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsRPtribgJ8O",
        "outputId": "4fd1b0bf-3fc1-4fee-82dc-0c7cdc0c9ef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_DISABLE_PROGRESS_BARS=1\n"
          ]
        }
      ],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_oAE22wgJ8Q",
        "outputId": "c1aefd26-ad3a-4f0a-8ca3-dd38dc39fb57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=true\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m66yoSyzK7uC",
        "outputId": "34787713-e911-4ca5-c7b9-2bc4cc33e843"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joQh6mGBm3_J",
        "outputId": "dbf4947e-efd5-4a22-c19b-8077a77e7763"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
            "    type: \"alpaca:chat\"\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 1\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "# optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "# lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwPlElbm73f"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jICMPJuomFsx",
        "scrolled": true,
        "outputId": "6caccb5c-ea50-472b-d80d-70192d364cda"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_2ml2d0fa/none_5ic8klds/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_2ml2d0fa/none_5ic8klds/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Downloading readme: 100%|██████████████████████| 533/533 [00:00<00:00, 3.31MB/s]\n",
            "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
            "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
            "Downloading data: 100%|████████████████████| 3.19M/3.19M [00:00<00:00, 85.2MB/s]\u001b[A\n",
            "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  3.06it/s]\n",
            "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1817.29it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 561.64it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 303.01it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:14<00:00,  8.27s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:17<00:00,  8.64s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'dense_4h_to_h', 'query_key_value', 'dense']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "[2023-06-15 10:35:39,250] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-15 10:35:39,250] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-15 10:35:39,250] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "[2023-06-15 10:35:42,559] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-15 10:35:42,559] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "Parameter Offload: Total persistent parameters: 1982464 in 242 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_103557-lvo6se55\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstill-resonance-36\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/lvo6se55\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1963, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "Traceback (most recent call last):\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7fc78c287200>\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1963, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7feff40903c0>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstill-resonance-36\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/lvo6se55\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230615_103557-lvo6se55/logs\u001b[0m\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1221 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 1222) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_10:36:04\n",
            "  host      : b2d0cfd20a3b\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 1222)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfs-AbKDY7K"
      },
      "source": [
        "## #2 ACC mbs 32 2xA100 minotaur OOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gNWb1VKgJ8S",
        "outputId": "ff716c49-559d-4cb9-9d5a-8e1315d8491b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=false\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wcOH4Ftmjg5n",
        "outputId": "3cce7e78-8b4b-42cc-c201-a03f107b4700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hf/ARC-Challenge.jsonl\n",
        "      - hf/ARC-Easy.jsonl\n",
        "      - hf/riddle_sense.jsonl\n",
        "    type: explainchoice:chat\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hf/gsm8k.jsonl\n",
        "      - hf/winogrande.jsonl\n",
        "    type: alpaca_chat.load_qa\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/n_task.jsonl\n",
        "      - custom/misconceptions.jsonl\n",
        "      - custom/context_insensitivity.jsonl\n",
        "    type: alpaca_chat\n",
        "  - path: camel-ai/math\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/biology\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/physics\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/chemistry\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/in_context_qa.jsonl\n",
        "    type: context_qa\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/in_context_qa.jsonl\n",
        "    type: context_qa.load_404\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/jokes_explained_500up.jsonl\n",
        "    type: sharegpt_jokes\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/classify-self-chat.sharegpt.jsonl\n",
        "      - custom/coding-self-chat.sharegpt.jsonl\n",
        "      - custom/prose-gpt4.sharegpt.jsonl\n",
        "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
        "    type: sharegpt_simple.load_role\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - openai/tldr.jsonl\n",
        "    type: summarizetldr:chat\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hellaswag/hellaswag.jsonl\n",
        "    type: explainchoice:chat\n",
        "  - path: metaeval/ScienceQA_text_only\n",
        "    type: concisechoice:chat\n",
        "  - path: teknium/GPT4-LLM-Cleaned\n",
        "    type: alpaca_chat\n",
        "  - path: teknium/GPTeacher-General-Instruct\n",
        "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
        "    type: gpteacher:chat\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
        "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
        "    type: alpaca_chat\n",
        "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
        "    type: alpaca_chat\n",
        "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
        "    type: sharegpt:chat\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 32\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1u8RtbDY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EeOND66dDnF"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnz7AF94LJTa"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR8QPcw3DY7K",
        "scrolled": true,
        "outputId": "48462411-ed39-4fa5-804b-090fde6aa7ee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_227_egdi/none_ygwpum_e/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_227_egdi/none_ygwpum_e/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Downloading readme: 100%|██████████████████| 3.10k/3.10k [00:00<00:00, 8.61MB/s]\n",
            "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7...\n",
            "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
            "Downloading data:   0%|                              | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   3%|▌                    | 3.28M/117M [00:00<00:03, 32.8MB/s]\u001b[A\n",
            "Downloading data:   9%|█▊                   | 10.3M/117M [00:00<00:01, 54.7MB/s]\u001b[A\n",
            "Downloading data:  16%|███▍                 | 19.1M/117M [00:00<00:01, 69.9MB/s]\u001b[A\n",
            "Downloading data:  24%|████▉                | 27.7M/117M [00:00<00:01, 76.4MB/s]\u001b[A\n",
            "Downloading data:  30%|██████▎              | 35.3M/117M [00:00<00:01, 64.4MB/s]\u001b[A\n",
            "Downloading data:  38%|███████▉             | 44.0M/117M [00:00<00:01, 71.0MB/s]\u001b[A\n",
            "Downloading data:  44%|█████████▎           | 51.9M/117M [00:00<00:00, 73.6MB/s]\u001b[A\n",
            "Downloading data:  51%|██████████▋          | 59.5M/117M [00:00<00:01, 52.3MB/s]\u001b[A\n",
            "Downloading data:  56%|███████████▊         | 65.6M/117M [00:01<00:00, 51.9MB/s]\u001b[A\n",
            "Downloading data:  64%|█████████████▎       | 74.3M/117M [00:01<00:00, 60.2MB/s]\u001b[A\n",
            "Downloading data:  71%|██████████████▉      | 82.9M/117M [00:01<00:00, 66.9MB/s]\u001b[A\n",
            "Downloading data:  78%|████████████████▍    | 91.7M/117M [00:01<00:00, 72.3MB/s]\u001b[A\n",
            "Downloading data:  86%|██████████████████▉   | 100M/117M [00:01<00:00, 76.5MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████████████████| 117M/117M [00:01<00:00, 62.8MB/s]\u001b[A\n",
            "\n",
            "Downloading data:   0%|                              | 0.00/116M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   2%|▎                    | 1.77M/116M [00:00<00:06, 17.7MB/s]\u001b[A\n",
            "Downloading data:   7%|█▌                   | 8.48M/116M [00:00<00:02, 46.8MB/s]\u001b[A\n",
            "Downloading data:  15%|███                  | 17.0M/116M [00:00<00:01, 64.5MB/s]\u001b[A\n",
            "Downloading data:  22%|████▌                | 25.3M/116M [00:00<00:01, 71.5MB/s]\u001b[A\n",
            "Downloading data:  29%|██████               | 33.6M/116M [00:00<00:01, 75.7MB/s]\u001b[A\n",
            "Downloading data:  36%|███████▌             | 41.8M/116M [00:00<00:00, 78.0MB/s]\u001b[A\n",
            "Downloading data:  43%|████████▉            | 49.6M/116M [00:00<00:01, 41.5MB/s]\u001b[A\n",
            "Downloading data:  50%|██████████▍          | 58.0M/116M [00:01<00:01, 49.8MB/s]\u001b[A\n",
            "Downloading data:  56%|███████████▋         | 64.7M/116M [00:01<00:01, 39.1MB/s]\u001b[A\n",
            "Downloading data:  63%|█████████████▏       | 73.3M/116M [00:01<00:00, 48.0MB/s]\u001b[A\n",
            "Downloading data:  69%|██████████████▍      | 80.1M/116M [00:01<00:00, 51.9MB/s]\u001b[A\n",
            "Downloading data:  76%|███████████████▉     | 88.4M/116M [00:01<00:00, 59.2MB/s]\u001b[A\n",
            "Downloading data:  83%|█████████████████▎   | 96.0M/116M [00:01<00:00, 55.9MB/s]\u001b[A\n",
            "Downloading data:  90%|███████████████████▋  | 104M/116M [00:01<00:00, 62.5MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████████████████| 116M/116M [00:02<00:00, 54.0MB/s]\u001b[A\n",
            "\n",
            "Downloading data:   0%|                              | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   3%|▌                    | 3.38M/117M [00:00<00:03, 33.8MB/s]\u001b[A\n",
            "Downloading data:   9%|█▉                   | 11.0M/117M [00:00<00:01, 58.8MB/s]\u001b[A\n",
            "Downloading data:  14%|███                  | 16.9M/117M [00:00<00:01, 53.9MB/s]\u001b[A\n",
            "Downloading data:  22%|████▌                | 25.3M/117M [00:00<00:01, 65.3MB/s]\u001b[A\n",
            "Downloading data:  27%|█████▊               | 32.0M/117M [00:00<00:01, 61.5MB/s]\u001b[A\n",
            "Downloading data:  34%|███████              | 39.2M/117M [00:00<00:01, 64.8MB/s]\u001b[A\n",
            "Downloading data:  39%|████████▏            | 45.7M/117M [00:00<00:01, 58.7MB/s]\u001b[A\n",
            "Downloading data:  44%|█████████▎           | 51.8M/117M [00:00<00:01, 55.2MB/s]\u001b[A\n",
            "Downloading data:  51%|██████████▊          | 60.0M/117M [00:00<00:00, 62.8MB/s]\u001b[A\n",
            "Downloading data:  57%|███████████▉         | 66.5M/117M [00:01<00:00, 53.1MB/s]\u001b[A\n",
            "Downloading data:  64%|█████████████▍       | 74.7M/117M [00:01<00:00, 60.6MB/s]\u001b[A\n",
            "Downloading data:  70%|██████████████▌      | 81.2M/117M [00:01<00:00, 46.3MB/s]\u001b[A\n",
            "Downloading data:  77%|████████████████     | 89.5M/117M [00:01<00:00, 54.5MB/s]\u001b[A\n",
            "Downloading data:  84%|█████████████████▌   | 97.9M/117M [00:01<00:00, 61.6MB/s]\u001b[A\n",
            "Downloading data:  91%|███████████████████▉  | 106M/117M [00:01<00:00, 66.0MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████████████████| 117M/117M [00:02<00:00, 55.9MB/s]\u001b[A\n",
            "\n",
            "Downloading data:   0%|                              | 0.00/117M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   3%|▌                    | 3.49M/117M [00:00<00:03, 34.9MB/s]\u001b[A\n",
            "Downloading data:  10%|██                   | 11.4M/117M [00:00<00:01, 60.8MB/s]\u001b[A\n",
            "Downloading data:  15%|███                  | 17.5M/117M [00:00<00:01, 58.1MB/s]\u001b[A\n",
            "Downloading data:  22%|████▌                | 25.4M/117M [00:00<00:01, 66.3MB/s]\u001b[A\n",
            "Downloading data:  29%|█████▉               | 33.5M/117M [00:00<00:01, 71.6MB/s]\u001b[A\n",
            "Downloading data:  35%|███████▍             | 41.4M/117M [00:00<00:01, 74.0MB/s]\u001b[A\n",
            "Downloading data:  42%|████████▋            | 48.8M/117M [00:00<00:00, 73.9MB/s]\u001b[A\n",
            "Downloading data:  49%|██████████▏          | 57.1M/117M [00:00<00:00, 76.5MB/s]\u001b[A\n",
            "Downloading data:  55%|███████████▌         | 64.7M/117M [00:00<00:00, 71.7MB/s]\u001b[A\n",
            "Downloading data:  62%|█████████████        | 72.9M/117M [00:01<00:00, 74.7MB/s]\u001b[A\n",
            "Downloading data:  69%|██████████████▍      | 80.5M/117M [00:01<00:00, 45.4MB/s]\u001b[A\n",
            "Downloading data:  76%|███████████████▉     | 89.0M/117M [00:01<00:00, 53.6MB/s]\u001b[A\n",
            "Downloading data:  82%|█████████████████▏   | 96.0M/117M [00:01<00:00, 47.6MB/s]\u001b[A\n",
            "Downloading data:  89%|███████████████████▌  | 104M/117M [00:01<00:00, 54.7MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████████████████| 117M/117M [00:02<00:00, 56.1MB/s]\u001b[A\n",
            "\n",
            "Downloading data:   0%|                              | 0.00/115M [00:00<?, ?B/s]\u001b[A\n",
            "Downloading data:   3%|▌                    | 3.05M/115M [00:00<00:03, 30.5MB/s]\u001b[A\n",
            "Downloading data:   9%|█▉                   | 10.3M/115M [00:00<00:01, 55.3MB/s]\u001b[A\n",
            "Downloading data:  16%|███▍                 | 18.8M/115M [00:00<00:01, 68.9MB/s]\u001b[A\n",
            "Downloading data:  23%|████▉                | 27.1M/115M [00:00<00:01, 74.2MB/s]\u001b[A\n",
            "Downloading data:  30%|██████▎              | 34.5M/115M [00:00<00:01, 66.7MB/s]\u001b[A\n",
            "Downloading data:  37%|███████▊             | 42.7M/115M [00:00<00:01, 71.6MB/s]\u001b[A\n",
            "Downloading data:  44%|█████████▎           | 50.9M/115M [00:00<00:00, 74.9MB/s]\u001b[A\n",
            "Downloading data:  52%|██████████▊          | 59.4M/115M [00:00<00:00, 78.1MB/s]\u001b[A\n",
            "Downloading data:  58%|████████████▎        | 67.3M/115M [00:01<00:00, 63.6MB/s]\u001b[A\n",
            "Downloading data:  65%|█████████████▋       | 75.5M/115M [00:01<00:00, 68.4MB/s]\u001b[A\n",
            "Downloading data:  72%|███████████████      | 82.8M/115M [00:01<00:00, 54.4MB/s]\u001b[A\n",
            "Downloading data:  79%|████████████████▍    | 90.6M/115M [00:01<00:00, 59.9MB/s]\u001b[A\n",
            "Downloading data:  85%|█████████████████▊   | 98.0M/115M [00:01<00:00, 63.5MB/s]\u001b[A\n",
            "Downloading data:  92%|████████████████████▏ | 106M/115M [00:01<00:00, 68.0MB/s]\u001b[A\n",
            "Downloading data: 100%|██████████████████████| 115M/115M [00:01<00:00, 61.5MB/s]\u001b[A\n",
            "Downloading data files: 100%|█████████████████████| 1/1 [00:12<00:00, 12.95s/it]\n",
            "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 436.32it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7. Subsequent calls will reuse this data.\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 60.09it/s]\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 46.17it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.34s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense', 'dense_4h_to_h', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.40s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'query_key_value', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_104236-bsxk4bbg\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mroyal-morning-37\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/bsxk4bbg\u001b[0m\n",
            "  0%|                                                 | 0/19557 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 739, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 646, in forward\n",
            "    outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 411, in forward\n",
            "    mlp_output = self.mlp(ln_mlp)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 356, in forward\n",
            "    x = self.act(self.dense_h_to_4h(x))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/tuners/lora.py\", line 826, in forward\n",
            "    result = result.clone()\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 5.22 GiB (GPU 1; 79.15 GiB total capacity; 62.46 GiB already allocated; 4.65 GiB free; 72.83 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 739, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 646, in forward\n",
            "    outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 411, in forward\n",
            "    mlp_output = self.mlp(ln_mlp)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 357, in forward\n",
            "    x = self.dense_4h_to_h(x)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/tuners/lora.py\", line 839, in forward\n",
            "    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.16 GiB (GPU 0; 79.15 GiB total capacity; 73.42 GiB already allocated; 20.44 MiB free; 77.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1729 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 1730) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_10:42:52\n",
            "  host      : b2d0cfd20a3b\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 1730)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbjGJ2GDY7L"
      },
      "source": [
        "## #3 2xA100 mbs 16 OOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00vedG_seEDL"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ltGgTqeEDM"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6hvCkF0DY7L",
        "scrolled": true,
        "outputId": "f42f2be1-1d49-43ae-d752-236d7434a389"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 16\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwPMxEdeLLDu"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XsSrgMvDY7L",
        "scrolled": true,
        "outputId": "31626314-0ad5-4bfa-d3a3-02dc70a51232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g4owamzf/none_wnl8yvmc/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g4owamzf/none_wnl8yvmc/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 69.22it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards:   0%|                          | 0/9 [00:00<?, ?it/s]WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 63.28it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:13<00:00,  8.22s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.39s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'query_key_value', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'query_key_value', 'dense', 'dense_h_to_4h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_105038-3o7uv7sq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdashing-resonance-38\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/3o7uv7sq\u001b[0m\n",
            "  0%|                                                 | 0/39111 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.1978, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 1/39111 [00:32<349:36:18, 32.18s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 739, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 646, in forward\n",
            "    outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 411, in forward\n",
            "    mlp_output = self.mlp(ln_mlp)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 356, in forward\n",
            "    x = self.act(self.dense_h_to_4h(x))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/tuners/lora.py\", line 826, in forward\n",
            "    result = result.clone()\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.61 GiB (GPU 0; 79.15 GiB total capacity; 57.21 GiB already allocated; 1.28 GiB free; 75.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:         train/epoch 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train/global_step 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: train/learning_rate 2e-05\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train/loss 1.1978\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mdashing-resonance-38\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/3o7uv7sq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230615_105038-3o7uv7sq/logs\u001b[0m\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2209 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 2208) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_10:51:34\n",
            "  host      : b2d0cfd20a3b\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 2208)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwP73VWDY7L"
      },
      "source": [
        "## #4 4xA100 mbs 16 OOM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMk0VCrho7ZB",
        "outputId": "f7011b6f-b9c7-408f-81dd-4c8cdab423e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-azUQO0dDnH",
        "outputId": "05564d57-e8b2-4575-f8d4-7f5fc7cfd099"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=false\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGjKHqolo7ZB",
        "outputId": "ea410ede-653b-41fc-84c4-002701e67e15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_DISABLE_PROGRESS_BARS=1\n"
          ]
        }
      ],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_tDMBqDY7L",
        "scrolled": true,
        "outputId": "49a28117-ce9d-4f53-c517-8fb8f16805af"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 12\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFuqSRNLL5_"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VIorWx3DY7M",
        "scrolled": true,
        "outputId": "2743ca03-976a-419b-abc6-4d4bd009ec98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `4`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_cza9brmj/none_8bcip2zh/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_cza9brmj/none_8bcip2zh/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_cza9brmj/none_8bcip2zh/attempt_0/3/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_cza9brmj/none_8bcip2zh/attempt_0/2/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 61.88it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 58.05it/s]\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 57.74it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 65.12it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.67s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'query_key_value', 'dense_4h_to_h', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.70s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.71s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'query_key_value', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:19<00:00,  8.86s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense', 'query_key_value', 'dense_h_to_4h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
            "INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_141904-wfq71g10\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmisunderstood-leaf-39\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/wfq71g10\u001b[0m\n",
            "  0%|                                                 | 0/19557 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 782, in forward\n",
            "    loss = loss_fct(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.59 GiB (GPU 2; 79.15 GiB total capacity; 72.43 GiB already allocated; 1.67 GiB free; 75.60 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1277 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1278 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 1280 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 1279) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_14:19:22\n",
            "  host      : 48ca1761a330\n",
            "  rank      : 2 (local_rank: 2)\n",
            "  exitcode  : 1 (pid: 1279)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TrgHEoizhLx"
      },
      "source": [
        "## #5 mbs 12"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NdJGjFTdDnI"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcOtbqlCdDnI",
        "outputId": "2c27d69c-2f24-4ca1-c3bf-2e24e0c3deaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 12\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "KNM9aJoto7ZD",
        "outputId": "2f7e6d29-cc34-4072-f2e4-5fe8a2227e83"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: jupyternotify in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (0.1.15)\n",
            "Requirement already satisfied: ipython in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyternotify) (8.14.0)\n",
            "Requirement already satisfied: jupyter in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyternotify) (1.0.0)\n",
            "Requirement already satisfied: backcall in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (0.2.0)\n",
            "Requirement already satisfied: decorator in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (5.1.1)\n",
            "Requirement already satisfied: jedi>=0.16 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (0.18.2)\n",
            "Requirement already satisfied: matplotlib-inline in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (0.1.6)\n",
            "Requirement already satisfied: pickleshare in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (3.0.38)\n",
            "Requirement already satisfied: pygments>=2.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (2.15.1)\n",
            "Requirement already satisfied: stack-data in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (0.6.2)\n",
            "Requirement already satisfied: traitlets>=5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (5.9.0)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (4.6.3)\n",
            "Requirement already satisfied: pexpect>4.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipython->jupyternotify) (4.8.0)\n",
            "Requirement already satisfied: notebook in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (6.5.4)\n",
            "Requirement already satisfied: qtconsole in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (5.4.3)\n",
            "Requirement already satisfied: jupyter-console in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (6.6.3)\n",
            "Requirement already satisfied: nbconvert in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (7.5.0)\n",
            "Requirement already satisfied: ipykernel in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (6.23.2)\n",
            "Requirement already satisfied: ipywidgets in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter->jupyternotify) (8.0.6)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jedi>=0.16->ipython->jupyternotify) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pexpect>4.3->ipython->jupyternotify) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython->jupyternotify) (0.2.6)\n",
            "Requirement already satisfied: comm>=0.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (0.1.3)\n",
            "Requirement already satisfied: debugpy>=1.6.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (1.6.7)\n",
            "Requirement already satisfied: jupyter-client>=6.1.12 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (8.2.0)\n",
            "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (5.3.1)\n",
            "Requirement already satisfied: nest-asyncio in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (1.5.6)\n",
            "Requirement already satisfied: packaging in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (23.1)\n",
            "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (5.9.5)\n",
            "Requirement already satisfied: pyzmq>=20 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (25.1.0)\n",
            "Requirement already satisfied: tornado>=6.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipykernel->jupyter->jupyternotify) (6.3.2)\n",
            "Requirement already satisfied: widgetsnbextension~=4.0.7 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipywidgets->jupyter->jupyternotify) (4.0.7)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.7 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from ipywidgets->jupyter->jupyternotify) (3.0.7)\n",
            "Requirement already satisfied: beautifulsoup4 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (4.12.2)\n",
            "Requirement already satisfied: bleach!=5.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (6.0.0)\n",
            "Requirement already satisfied: defusedxml in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (0.7.1)\n",
            "Requirement already satisfied: importlib-metadata>=3.6 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (6.6.0)\n",
            "Requirement already satisfied: jinja2>=3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (3.1.2)\n",
            "Requirement already satisfied: jupyterlab-pygments in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (0.2.2)\n",
            "Requirement already satisfied: markupsafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (2.1.3)\n",
            "Requirement already satisfied: mistune<3,>=2.0.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (2.0.5)\n",
            "Requirement already satisfied: nbclient>=0.5.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (0.8.0)\n",
            "Requirement already satisfied: nbformat>=5.7 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (5.9.0)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (1.5.0)\n",
            "Requirement already satisfied: tinycss2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbconvert->jupyter->jupyternotify) (1.2.1)\n",
            "Requirement already satisfied: argon2-cffi in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (21.3.0)\n",
            "Requirement already satisfied: ipython-genutils in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (0.2.0)\n",
            "Requirement already satisfied: Send2Trash>=1.8.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (1.8.2)\n",
            "Requirement already satisfied: terminado>=0.8.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (0.17.1)\n",
            "Requirement already satisfied: prometheus-client in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (0.17.0)\n",
            "Requirement already satisfied: nbclassic>=0.4.7 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from notebook->jupyter->jupyternotify) (1.0.0)\n",
            "Requirement already satisfied: qtpy>=2.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from qtconsole->jupyter->jupyternotify) (2.3.1)\n",
            "Requirement already satisfied: executing>=1.2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from stack-data->ipython->jupyternotify) (1.2.0)\n",
            "Requirement already satisfied: asttokens>=2.1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from stack-data->ipython->jupyternotify) (2.2.1)\n",
            "Requirement already satisfied: pure-eval in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from stack-data->ipython->jupyternotify) (0.2.2)\n",
            "Requirement already satisfied: six in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from asttokens>=2.1.0->stack-data->ipython->jupyternotify) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bleach!=5.0.0->nbconvert->jupyter->jupyternotify) (0.5.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from importlib-metadata>=3.6->nbconvert->jupyter->jupyternotify) (3.15.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-client>=6.1.12->ipykernel->jupyter->jupyternotify) (2.8.2)\n",
            "Requirement already satisfied: platformdirs>=2.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-core!=5.0.*,>=4.12->ipykernel->jupyter->jupyternotify) (3.5.3)\n",
            "Requirement already satisfied: jupyter-server>=1.8 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (2.6.0)\n",
            "Requirement already satisfied: notebook-shim>=0.2.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (0.2.3)\n",
            "Requirement already satisfied: fastjsonschema in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbformat>=5.7->nbconvert->jupyter->jupyternotify) (2.17.1)\n",
            "Requirement already satisfied: jsonschema>=2.6 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nbformat>=5.7->nbconvert->jupyter->jupyternotify) (4.17.3)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from argon2-cffi->notebook->jupyter->jupyternotify) (21.2.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from beautifulsoup4->nbconvert->jupyter->jupyternotify) (2.4.1)\n",
            "Requirement already satisfied: attrs>=17.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (23.1.0)\n",
            "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (0.19.3)\n",
            "Requirement already satisfied: anyio>=3.1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (3.7.0)\n",
            "Requirement already satisfied: jupyter-events>=0.6.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (0.6.3)\n",
            "Requirement already satisfied: jupyter-server-terminals in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (0.4.4)\n",
            "Requirement already satisfied: overrides in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (7.3.1)\n",
            "Requirement already satisfied: websocket-client in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (1.5.3)\n",
            "Requirement already satisfied: cffi>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from argon2-cffi-bindings->argon2-cffi->notebook->jupyter->jupyternotify) (1.15.1)\n",
            "Requirement already satisfied: idna>=2.8 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (3.4)\n",
            "Requirement already satisfied: sniffio>=1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from anyio>=3.1.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (1.1.1)\n",
            "Requirement already satisfied: pycparser in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->notebook->jupyter->jupyternotify) (2.21)\n",
            "Requirement already satisfied: python-json-logger>=2.0.4 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (2.0.7)\n",
            "Requirement already satisfied: pyyaml>=5.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (6.0)\n",
            "Requirement already satisfied: rfc3339-validator in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (0.1.4)\n",
            "Requirement already satisfied: rfc3986-validator>=0.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jupyter-events>=0.6.0->jupyter-server>=1.8->nbclassic>=0.4.7->notebook->jupyter->jupyternotify) (0.1.1)\n",
            "Requirement already satisfied: fqdn in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (1.5.1)\n",
            "Requirement already satisfied: isoduration in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (20.11.0)\n",
            "Requirement already satisfied: jsonpointer>1.13 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (2.3)\n",
            "Requirement already satisfied: uri-template in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (1.2.0)\n",
            "Requirement already satisfied: webcolors>=1.11 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (1.13)\n",
            "Requirement already satisfied: arrow>=0.15.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from isoduration->jsonschema>=2.6->nbformat>=5.7->nbconvert->jupyter->jupyternotify) (1.2.3)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/cphyc/jupyter-notify.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7j0Wga8io7ZD",
        "outputId": "363b4a45-0c3d-45d0-f80b-d60047e71eae"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": [
              "if (!(\"Notification\" in window)) {\n",
              "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
              "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
              "    Notification.requestPermission(function (permission) {\n",
              "        if(!('permission' in Notification)) {\n",
              "            Notification.permission = permission;\n",
              "        }\n",
              "    })\n",
              "}\n",
              "\n",
              "if(!window.jQuery) {\n",
              "    var jq = document.createElement('script');\n",
              "    jq.src = \"//ajax.googleapis.com/ajax/libs/jquery/2.1.4/jquery.min.js\";\n",
              "    document.getElementsByTagName('head')[0].appendChild(jq);\n",
              "}\n",
              "\n",
              "// Detect if the window is out of focus.\n",
              "window.jupyterNotifyIsInBackground = undefined;\n",
              "(function() {\n",
              "    // Check document.hidden support\n",
              "    var hidden;\n",
              "    if (typeof document.hidden !== \"undefined\") { // Opera 12.10 and Firefox 18 and later support\n",
              "      hidden = \"hidden\";\n",
              "    } else if (typeof document.msHidden !== \"undefined\") {\n",
              "      hidden = \"msHidden\";\n",
              "    } else if (typeof document.webkitHidden !== \"undefined\") {\n",
              "      hidden = \"webkitHidden\";\n",
              "    }\n",
              "\n",
              "    // Set initial background state\n",
              "    if (document[hidden]) {\n",
              "      window.jupyterNotifyIsInBackground = true;\n",
              "    } else {\n",
              "      window.jupyterNotifyIsInBackground = false;\n",
              "    }\n",
              "\n",
              "    window.addEventListener('blur', function() { window.jupyterNotifyIsInBackground = true; }, false);\n",
              "    window.addEventListener('focus', function() { window.jupyterNotifyIsInBackground = false; }, false);\n",
              "})();\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "%load_ext jupyternotify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWWi1zMYo7ZD"
      },
      "outputs": [],
      "source": [
        "%autonotify -a 30 -o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJG6JK2zhLx",
        "scrolled": true,
        "outputId": "15cbe47a-a91a-4ac9-b46c-ec7f7b40d826"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `4`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_wmntf1j0/none_3vzrck5b/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_wmntf1j0/none_3vzrck5b/attempt_0/3/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_wmntf1j0/none_3vzrck5b/attempt_0/2/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_wmntf1j0/none_3vzrck5b/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 58.28it/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 59.54it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 60.05it/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 61.15it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:14<00:00,  8.33s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'dense_h_to_4h', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.77s/it]\n",
            "Loading checkpoint shards:  89%|████████████████  | 8/9 [01:18<00:09,  9.34s/it]INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'dense', 'query_key_value', 'dense_4h_to_h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:21<00:00,  9.04s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_h_to_4h', 'dense_4h_to_h', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:25<00:00,  9.53s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense_4h_to_h', 'dense_h_to_4h', 'dense']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=4, worker_count=3, timeout=0:30:00)\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 2, key: store_based_barrier_key:1 (world_size=4, worker_count=3, timeout=0:30:00)\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_143431-hf14kgww\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcool-yogurt-40\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/hf14kgww\u001b[0m\n",
            "  0%|                                                 | 0/26073 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.1595, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 1/26073 [00:29<215:10:01, 29.71s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "{'loss': 1.0741, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 2/26073 [00:50<175:46:33, 24.27s/it]Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "      File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "      File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "        outputs = model(**inputs)outputs = model(**inputs)\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "        return forward_call(*args, **kwargs)return forward_call(*args, **kwargs)\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "        return model_forward(*args, **kwargs)return model_forward(*args, **kwargs)\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "      File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "      File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]    \n",
            "return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "        return forward_call(*args, **kwargs)return forward_call(*args, **kwargs)\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "        return self.base_model(return self.base_model(\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "        return forward_call(*args, **kwargs)return forward_call(*args, **kwargs)\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "        output = old_forward(*args, **kwargs)output = old_forward(*args, **kwargs)\n",
            "\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "        transformer_outputs = self.transformer(transformer_outputs = self.transformer(\n",
            "\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 646, in forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 646, in forward\n",
            "    outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    outputs = torch.utils.checkpoint.checkpoint(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 249, in checkpoint\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return CheckpointFunction.apply(function, preserve, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 506, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 107, in forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    outputs = run_function(*args)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 411, in forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 411, in forward\n",
            "    mlp_output = self.mlp(ln_mlp)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    mlp_output = self.mlp(ln_mlp)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 357, in forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 357, in forward\n",
            "    x = self.dense_4h_to_h(x)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    x = self.dense_4h_to_h(x)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/tuners/lora.py\", line 1019, in forward\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/tuners/lora.py\", line 1019, in forward\n",
            "    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    self.lora_A[self.active_adapter](self.lora_dropout[self.active_adapter](x))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "    return forward_call(*args, **kwargs)\n",
            "      File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/linear.py\", line 114, in forward\n",
            "return F.linear(input, self.weight, self.bias)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.15 GiB (GPU 1; 79.15 GiB total capacity; 65.72 GiB already allocated; 372.44 MiB free; 76.38 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "    return F.linear(input, self.weight, self.bias)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 1.52 GiB (GPU 3; 79.15 GiB total capacity; 67.92 GiB already allocated; 2.44 MiB free; 77.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2517 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2519 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 2518) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2023-06-15_14:35:37\n",
            "  host      : 48ca1761a330\n",
            "  rank      : 3 (local_rank: 3)\n",
            "  exitcode  : 1 (pid: 2520)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_14:35:37\n",
            "  host      : 48ca1761a330\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 2518)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"176ab60c-f662-42d9-baa3-221e14a602dc\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"176ab60c-f662-42d9-baa3-221e14a602dc\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"176ab60c-f662-42d9-baa3-221e14a602dc\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"176ab60c-f662-42d9-baa3-221e14a602dc\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnqGCBezhLy"
      },
      "source": [
        "## #6 mbs 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHaDkOkvdDnI",
        "outputId": "75d5b5a0-2579-4c89-eb22-9c30e64bb057"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 10\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aEj2fAgzhLz",
        "outputId": "085848f7-0823-46f7-eaff-8e01e67bb9b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `4`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_senvjawa/none_6iavc_e9/attempt_0/2/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_senvjawa/none_6iavc_e9/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_senvjawa/none_6iavc_e9/attempt_0/3/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_senvjawa/none_6iavc_e9/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 36.21it/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 49.75it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 54.06it/s]\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 60.41it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:16<00:00,  8.47s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'dense_4h_to_h', 'dense', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:16<00:00,  8.52s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'dense_4h_to_h', 'dense', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:17<00:00,  8.58s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense_4h_to_h', 'dense', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.77s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'dense', 'query_key_value']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
            "INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_144241-68cv56hf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclassic-monkey-41\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/68cv56hf\u001b[0m\n",
            "  0%|                                                 | 0/31287 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.199, 'learning_rate': 2e-05, 'epoch': 0.0}                           \n",
            "  0%|                                     | 1/31287 [00:25<217:24:02, 25.02s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "{'loss': 1.0305, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 2/31287 [00:42<177:13:27, 20.39s/it]Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 782, in forward\n",
            "    loss = loss_fct(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 3.80 GiB (GPU 3; 79.15 GiB total capacity; 62.20 GiB already allocated; 1.44 GiB free; 74.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3391 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3392 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3393 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 3 (pid: 3394) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_14:43:40\n",
            "  host      : 48ca1761a330\n",
            "  rank      : 3 (local_rank: 3)\n",
            "  exitcode  : 1 (pid: 3394)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"9d2ea516-6e38-4dd7-b0bf-d637b07a9675\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"9d2ea516-6e38-4dd7-b0bf-d637b07a9675\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"9d2ea516-6e38-4dd7-b0bf-d637b07a9675\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"9d2ea516-6e38-4dd7-b0bf-d637b07a9675\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivJrMd8zhLz"
      },
      "source": [
        "## #7 mbs 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99raQX83zhLz",
        "outputId": "0da3b95f-5cab-424a-f287-9fe18de03ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 8\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubZ7fVBVzhLz",
        "outputId": "e68c0e35-7106-46f5-d409-8150aa794be1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `4`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ppbio_p4/none_p5xky5f6/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ppbio_p4/none_p5xky5f6/attempt_0/3/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ppbio_p4/none_p5xky5f6/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ppbio_p4/none_p5xky5f6/attempt_0/2/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 62.21it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 39.61it/s]\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 59.86it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 56.58it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:14<00:00,  8.33s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'query_key_value', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.74s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'query_key_value', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.74s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_h_to_4h', 'dense_4h_to_h', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.77s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'query_key_value', 'dense_4h_to_h', 'dense']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_145155-0xvcv9wq\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mbumbling-butterfly-42\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/0xvcv9wq\u001b[0m\n",
            "  0%|                                                 | 0/39111 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.2136, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 1/39111 [00:17<189:48:14, 17.47s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "{'loss': 1.0936, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.0397, 'learning_rate': 6e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.1016, 'learning_rate': 8e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.1942, 'learning_rate': 0.0001, 'epoch': 0.0}                         \n",
            "  0%|                                     | 5/39111 [01:40<244:24:56, 22.50s/it]\n",
            "  0%|                                                   | 0/132 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▋                                          | 2/132 [00:08<09:05,  4.19s/it]\u001b[A\n",
            "  2%|▉                                          | 3/132 [00:11<08:04,  3.75s/it]\u001b[A\n",
            "  3%|█▎                                         | 4/132 [00:15<07:50,  3.67s/it]\u001b[A\n",
            "  4%|█▋                                         | 5/132 [00:25<12:30,  5.91s/it]\u001b[A\n",
            "  5%|█▉                                         | 6/132 [00:33<14:23,  6.85s/it]\u001b[A\n",
            "  5%|██▎                                        | 7/132 [00:46<18:25,  8.84s/it]\u001b[A\n",
            "  6%|██▌                                        | 8/132 [00:49<14:27,  7.00s/it]\u001b[A\n",
            "  7%|██▉                                        | 9/132 [00:55<13:14,  6.46s/it]\u001b[A\n",
            "  8%|███▏                                      | 10/132 [00:57<10:49,  5.33s/it]\u001b[A\n",
            "  8%|███▌                                      | 11/132 [01:05<11:50,  5.87s/it]\u001b[A\n",
            "  9%|███▊                                      | 12/132 [01:09<10:38,  5.32s/it]\u001b[A\n",
            " 10%|████▏                                     | 13/132 [01:12<09:12,  4.64s/it]\u001b[A\n",
            " 11%|████▍                                     | 14/132 [01:17<09:13,  4.69s/it]\u001b[A\n",
            " 11%|████▊                                     | 15/132 [01:23<10:11,  5.23s/it]\u001b[A\n",
            " 12%|█████                                     | 16/132 [01:28<10:08,  5.25s/it]\u001b[A\n",
            " 13%|█████▍                                    | 17/132 [01:37<12:02,  6.28s/it]\u001b[A\n",
            " 14%|█████▋                                    | 18/132 [01:42<11:02,  5.82s/it]\u001b[A\n",
            " 14%|██████                                    | 19/132 [01:46<10:06,  5.37s/it]\u001b[A\n",
            " 15%|██████▎                                   | 20/132 [01:54<11:33,  6.19s/it]\u001b[A\n",
            " 16%|██████▋                                   | 21/132 [02:05<14:13,  7.69s/it]\u001b[A\n",
            " 17%|███████                                   | 22/132 [02:08<11:23,  6.22s/it]\u001b[A\n",
            " 17%|███████▎                                  | 23/132 [02:33<21:25, 11.79s/it]\u001b[A\n",
            " 18%|███████▋                                  | 24/132 [02:36<16:23,  9.10s/it]\u001b[A\n",
            " 19%|███████▉                                  | 25/132 [02:42<14:51,  8.33s/it]\u001b[A\n",
            " 20%|████████▎                                 | 26/132 [02:46<12:27,  7.05s/it]\u001b[A\n",
            " 20%|████████▌                                 | 27/132 [02:52<11:29,  6.56s/it]\u001b[A\n",
            " 21%|████████▉                                 | 28/132 [02:58<11:19,  6.54s/it]\u001b[A\n",
            " 22%|█████████▏                                | 29/132 [03:04<10:38,  6.20s/it]\u001b[A\n",
            " 23%|█████████▌                                | 30/132 [03:16<13:26,  7.90s/it]\u001b[A\n",
            " 23%|█████████▊                                | 31/132 [03:22<12:33,  7.46s/it]\u001b[A\n",
            " 24%|██████████▏                               | 32/132 [03:33<14:22,  8.62s/it]\u001b[A\n",
            " 25%|██████████▌                               | 33/132 [03:39<12:42,  7.71s/it]\u001b[A\n",
            " 26%|██████████▊                               | 34/132 [03:45<11:57,  7.32s/it]\u001b[A\n",
            " 27%|███████████▏                              | 35/132 [03:51<11:16,  6.97s/it]\u001b[A\n",
            " 27%|███████████▍                              | 36/132 [03:57<10:18,  6.44s/it]\u001b[A\n",
            " 28%|███████████▊                              | 37/132 [04:01<09:12,  5.81s/it]\u001b[A\n",
            " 29%|████████████                              | 38/132 [04:09<09:56,  6.35s/it]\u001b[A\n",
            " 30%|████████████▍                             | 39/132 [04:15<10:00,  6.45s/it]\u001b[A\n",
            " 30%|████████████▋                             | 40/132 [04:22<10:01,  6.54s/it]\u001b[A\n",
            " 31%|█████████████                             | 41/132 [04:24<08:00,  5.29s/it]\u001b[A\n",
            " 32%|█████████████▎                            | 42/132 [04:37<11:14,  7.49s/it]\u001b[A\n",
            " 33%|█████████████▋                            | 43/132 [04:46<11:37,  7.83s/it]\u001b[A\n",
            " 33%|██████████████                            | 44/132 [04:49<09:32,  6.50s/it]\u001b[A\n",
            " 34%|██████████████▎                           | 45/132 [04:55<09:07,  6.30s/it]\u001b[A\n",
            " 35%|██████████████▋                           | 46/132 [05:00<08:38,  6.03s/it]\u001b[A\n",
            " 36%|██████████████▉                           | 47/132 [05:08<09:25,  6.66s/it]\u001b[A\n",
            " 36%|███████████████▎                          | 48/132 [05:11<07:38,  5.46s/it]\u001b[A^C\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4277 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4278 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4279 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4280 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1883, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2184, in _maybe_log_save_evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2930, in evaluate\n",
            "    output = eval_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3119, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3362, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 654, in forward\n",
            "    outputs = block(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 255, in forward\n",
            "    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 201, in _split_heads\n",
            "    k = qkv[:, :, :, [-2]]\n",
            "KeyboardInterrupt\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1883, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2184, in _maybe_log_save_evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2930, in evaluate\n",
            "    output = eval_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3119, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3362, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 654, in forward\n",
            "    outputs = block(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 255, in forward\n",
            "    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 201, in _split_heads\n",
            "    k = qkv[:, :, :, [-2]]\n",
            "KeyboardInterrupt\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"f4aab5ca-bf09-48b1-8046-e882879c5022\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"f4aab5ca-bf09-48b1-8046-e882879c5022\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"f4aab5ca-bf09-48b1-8046-e882879c5022\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"f4aab5ca-bf09-48b1-8046-e882879c5022\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaiP0pUzhL0"
      },
      "source": [
        "## #8 No early_stopping_patience"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzy_ZDY4zhL0",
        "outputId": "6305f489-e8b7-4e0b-fe53-834c498f9dd7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/ARC-Challenge.jsonl\n",
            "      - hf/ARC-Easy.jsonl\n",
            "      - hf/riddle_sense.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hf/gsm8k.jsonl\n",
            "      - hf/winogrande.jsonl\n",
            "    type: alpaca_chat.load_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/n_task.jsonl\n",
            "      - custom/misconceptions.jsonl\n",
            "      - custom/context_insensitivity.jsonl\n",
            "    type: alpaca_chat\n",
            "  - path: camel-ai/math\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/biology\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/physics\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: camel-ai/chemistry\n",
            "    type: alpaca_chat.load_camel_ai\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/in_context_qa.jsonl\n",
            "    type: context_qa.load_404\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/jokes_explained_500up.jsonl\n",
            "    type: sharegpt_jokes\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - custom/classify-self-chat.sharegpt.jsonl\n",
            "      - custom/coding-self-chat.sharegpt.jsonl\n",
            "      - custom/prose-gpt4.sharegpt.jsonl\n",
            "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
            "    type: sharegpt_simple.load_role\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - openai/tldr.jsonl\n",
            "    type: summarizetldr:chat\n",
            "  - path: winglian/evals\n",
            "    data_files:\n",
            "      - hellaswag/hellaswag.jsonl\n",
            "    type: explainchoice:chat\n",
            "  - path: metaeval/ScienceQA_text_only\n",
            "    type: concisechoice:chat\n",
            "  - path: teknium/GPT4-LLM-Cleaned\n",
            "    type: alpaca_chat\n",
            "  - path: teknium/GPTeacher-General-Instruct\n",
            "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
            "    type: gpteacher:chat\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
            "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
            "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
            "    type: alpaca_chat\n",
            "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
            "    type: sharegpt:chat\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 8\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "# early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mV9bkd2o7ZH",
        "outputId": "09610f02-5fe9-4b52-f51b-6785df746437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `4`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_zvrgnmqx/none_h1wvgnqp/attempt_0/3/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_zvrgnmqx/none_h1wvgnqp/attempt_0/2/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_zvrgnmqx/none_h1wvgnqp/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_zvrgnmqx/none_h1wvgnqp/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /content/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 61.29it/s]\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "  0%|                                                     | 0/1 [00:00<?, ?it/s]INFO:root:loading model and peft_config...\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 63.89it/s]\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 59.07it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--9a63aa2c07ace8350a0e8b32ab913f2a-9cb79670bc460bd7/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|█████████████████████████████████████████████| 1/1 [00:00<00:00, 36.28it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:14<00:00,  8.26s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.35s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'query_key_value', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:17<00:00,  8.65s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'query_key_value', 'dense_h_to_4h', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.69s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'query_key_value', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 2\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 3\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 2: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 3: Completed store-based barrier for key:store_based_barrier_key:1 with 4 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_150540-dfoiiils\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvisionary-frog-43\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/dfoiiils\u001b[0m\n",
            "  0%|                                                 | 0/39111 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.2136, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "  0%|                                     | 1/39111 [00:17<188:38:39, 17.36s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "{'loss': 1.0936, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.0399, 'learning_rate': 6e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.1019, 'learning_rate': 8e-05, 'epoch': 0.0}                          \n",
            "{'loss': 1.1939, 'learning_rate': 0.0001, 'epoch': 0.0}                         \n",
            "  0%|                                     | 5/39111 [01:40<244:43:42, 22.53s/it]\n",
            "  0%|                                                   | 0/132 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▋                                          | 2/132 [00:08<09:04,  4.19s/it]\u001b[A\n",
            "  2%|▉                                          | 3/132 [00:11<08:03,  3.75s/it]\u001b[A\n",
            "  3%|█▎                                         | 4/132 [00:15<07:49,  3.67s/it]\u001b[A\n",
            "  4%|█▋                                         | 5/132 [00:25<12:25,  5.87s/it]\u001b[A\n",
            "  5%|█▉                                         | 6/132 [00:33<14:01,  6.68s/it]\u001b[A\n",
            "  5%|██▎                                        | 7/132 [00:46<18:09,  8.71s/it]\u001b[A\n",
            "  6%|██▌                                        | 8/132 [00:49<14:16,  6.91s/it]\u001b[A\n",
            "  7%|██▉                                        | 9/132 [00:54<13:07,  6.40s/it]\u001b[A\n",
            "  8%|███▏                                      | 10/132 [00:57<10:44,  5.29s/it]\u001b[A\n",
            "  8%|███▌                                      | 11/132 [01:04<11:47,  5.84s/it]\u001b[A\n",
            "  9%|███▊                                      | 12/132 [01:08<10:35,  5.30s/it]\u001b[A\n",
            " 10%|████▏                                     | 13/132 [01:11<09:10,  4.63s/it]\u001b[A\n",
            " 11%|████▍                                     | 14/132 [01:16<09:12,  4.68s/it]\u001b[A\n",
            " 11%|████▊                                     | 15/132 [01:22<10:11,  5.22s/it]\u001b[A\n",
            " 12%|█████                                     | 16/132 [01:28<10:08,  5.25s/it]\u001b[A\n",
            " 13%|█████▍                                    | 17/132 [01:36<12:02,  6.28s/it]\u001b[A\n",
            " 14%|█████▋                                    | 18/132 [01:41<11:02,  5.81s/it]\u001b[A\n",
            " 14%|██████                                    | 19/132 [01:45<10:06,  5.36s/it]\u001b[A\n",
            " 15%|██████▎                                   | 20/132 [01:54<11:33,  6.19s/it]\u001b[A\n",
            " 16%|██████▋                                   | 21/132 [02:05<14:13,  7.69s/it]\u001b[A\n",
            " 17%|███████                                   | 22/132 [02:08<11:24,  6.22s/it]\u001b[A\n",
            " 17%|███████▎                                  | 23/132 [02:32<21:25, 11.80s/it]\u001b[A\n",
            " 18%|███████▋                                  | 24/132 [02:35<16:23,  9.11s/it]\u001b[A\n",
            " 19%|███████▉                                  | 25/132 [02:42<14:51,  8.33s/it]\u001b[A\n",
            " 20%|████████▎                                 | 26/132 [02:46<12:27,  7.05s/it]\u001b[A\n",
            " 20%|████████▌                                 | 27/132 [02:51<11:28,  6.56s/it]\u001b[A\n",
            " 21%|████████▉                                 | 28/132 [02:58<11:19,  6.54s/it]\u001b[A\n",
            " 22%|█████████▏                                | 29/132 [03:03<10:38,  6.20s/it]\u001b[A\n",
            " 23%|█████████▌                                | 30/132 [03:15<13:15,  7.80s/it]\u001b[A\n",
            " 23%|█████████▊                                | 31/132 [03:21<12:26,  7.39s/it]\u001b[A\n",
            " 24%|██████████▏                               | 32/132 [03:32<14:16,  8.57s/it]\u001b[A\n",
            " 25%|██████████▌                               | 33/132 [03:38<12:39,  7.67s/it]\u001b[A\n",
            " 26%|██████████▊                               | 34/132 [03:44<11:54,  7.29s/it]\u001b[A\n",
            " 27%|███████████▏                              | 35/132 [03:50<11:13,  6.95s/it]\u001b[A\n",
            " 27%|███████████▍                              | 36/132 [03:56<10:16,  6.42s/it]\u001b[A\n",
            " 28%|███████████▊                              | 37/132 [04:00<09:10,  5.80s/it]\u001b[A\n",
            " 29%|████████████                              | 38/132 [04:08<09:55,  6.34s/it]\u001b[A\n",
            " 30%|████████████▍                             | 39/132 [04:14<09:59,  6.45s/it]\u001b[A\n",
            " 30%|████████████▋                             | 40/132 [04:21<10:01,  6.54s/it]\u001b[A\n",
            " 31%|█████████████                             | 41/132 [04:23<08:00,  5.28s/it]\u001b[A\n",
            " 32%|█████████████▎                            | 42/132 [04:36<11:13,  7.48s/it]\u001b[A\n",
            " 33%|█████████████▋                            | 43/132 [04:45<11:36,  7.83s/it]\u001b[A\n",
            " 33%|██████████████                            | 44/132 [04:48<09:31,  6.50s/it]\u001b[A\n",
            " 34%|██████████████▎                           | 45/132 [04:54<09:07,  6.30s/it]\u001b[A\n",
            " 35%|██████████████▋                           | 46/132 [04:59<08:38,  6.03s/it]\u001b[A\n",
            " 36%|██████████████▉                           | 47/132 [05:07<09:25,  6.65s/it]\u001b[A\n",
            " 36%|███████████████▎                          | 48/132 [05:10<07:38,  5.46s/it]\u001b[A\n",
            " 37%|███████████████▌                          | 49/132 [05:16<07:41,  5.56s/it]\u001b[A\n",
            " 38%|███████████████▉                          | 50/132 [05:22<07:43,  5.65s/it]\u001b[A\n",
            " 39%|████████████████▏                         | 51/132 [05:26<07:00,  5.20s/it]\u001b[A\n",
            " 39%|████████████████▌                         | 52/132 [05:30<06:28,  4.86s/it]\u001b[A\n",
            " 40%|████████████████▊                         | 53/132 [05:38<07:33,  5.75s/it]\u001b[A\n",
            " 41%|█████████████████▏                        | 54/132 [05:42<06:47,  5.23s/it]\u001b[A\n",
            " 42%|█████████████████▌                        | 55/132 [05:48<07:07,  5.55s/it]\u001b[A\n",
            " 42%|█████████████████▊                        | 56/132 [05:53<06:49,  5.39s/it]\u001b[A\n",
            " 43%|██████████████████▏                       | 57/132 [06:00<07:18,  5.85s/it]\u001b[A\n",
            " 44%|██████████████████▍                       | 58/132 [06:05<07:03,  5.73s/it]\u001b[A\n",
            " 45%|██████████████████▊                       | 59/132 [06:10<06:40,  5.48s/it]\u001b[A\n",
            " 45%|███████████████████                       | 60/132 [06:14<05:46,  4.81s/it]\u001b[A\n",
            " 46%|███████████████████▍                      | 61/132 [06:18<05:36,  4.73s/it]\u001b[A\n",
            " 47%|███████████████████▋                      | 62/132 [06:25<06:26,  5.52s/it]\u001b[A\n",
            " 48%|████████████████████                      | 63/132 [06:31<06:22,  5.55s/it]\u001b[A\n",
            " 48%|████████████████████▎                     | 64/132 [06:37<06:28,  5.72s/it]\u001b[A\n",
            " 49%|████████████████████▋                     | 65/132 [06:41<05:40,  5.08s/it]\u001b[A\n",
            " 50%|█████████████████████                     | 66/132 [06:48<06:12,  5.64s/it]\u001b[A\n",
            " 51%|█████████████████████▎                    | 67/132 [06:52<05:38,  5.21s/it]\u001b[A\n",
            " 52%|█████████████████████▋                    | 68/132 [07:01<06:38,  6.23s/it]\u001b[A\n",
            " 52%|█████████████████████▉                    | 69/132 [07:04<05:40,  5.41s/it]\u001b[A\n",
            " 53%|██████████████████████▎                   | 70/132 [07:11<06:09,  5.97s/it]\u001b[A\n",
            " 54%|██████████████████████▌                   | 71/132 [07:16<05:31,  5.43s/it]\u001b[A\n",
            " 55%|██████████████████████▉                   | 72/132 [07:21<05:20,  5.35s/it]\u001b[A\n",
            " 55%|███████████████████████▏                  | 73/132 [07:27<05:30,  5.60s/it]\u001b[A\n",
            " 56%|███████████████████████▌                  | 74/132 [07:31<04:52,  5.04s/it]\u001b[A\n",
            " 57%|███████████████████████▊                  | 75/132 [07:37<05:11,  5.46s/it]\u001b[A\n",
            " 58%|████████████████████████▏                 | 76/132 [07:41<04:43,  5.06s/it]\u001b[A\n",
            " 58%|████████████████████████▌                 | 77/132 [07:46<04:37,  5.04s/it]\u001b[A\n",
            " 59%|████████████████████████▊                 | 78/132 [07:50<04:19,  4.81s/it]\u001b[A\n",
            " 60%|█████████████████████████▏                | 79/132 [07:55<04:10,  4.73s/it]\u001b[A\n",
            " 61%|█████████████████████████▍                | 80/132 [08:05<05:29,  6.33s/it]\u001b[A\n",
            " 61%|█████████████████████████▊                | 81/132 [08:09<04:54,  5.77s/it]\u001b[A\n",
            " 62%|██████████████████████████                | 82/132 [08:15<04:43,  5.66s/it]\u001b[A\n",
            " 63%|██████████████████████████▍               | 83/132 [08:21<04:36,  5.65s/it]\u001b[A\n",
            " 64%|██████████████████████████▋               | 84/132 [08:27<04:39,  5.82s/it]\u001b[A\n",
            " 64%|███████████████████████████               | 85/132 [08:36<05:18,  6.77s/it]\u001b[A\n",
            " 65%|███████████████████████████▎              | 86/132 [08:38<04:11,  5.48s/it]\u001b[A\n",
            " 66%|███████████████████████████▋              | 87/132 [08:46<04:37,  6.16s/it]\u001b[A\n",
            " 67%|████████████████████████████              | 88/132 [08:49<03:49,  5.21s/it]\u001b[A\n",
            " 67%|████████████████████████████▎             | 89/132 [08:57<04:22,  6.10s/it]\u001b[A\n",
            " 68%|████████████████████████████▋             | 90/132 [09:02<04:00,  5.73s/it]\u001b[A\n",
            " 69%|████████████████████████████▉             | 91/132 [09:13<04:55,  7.21s/it]\u001b[A\n",
            " 70%|█████████████████████████████▎            | 92/132 [09:21<05:07,  7.69s/it]\u001b[A\n",
            " 70%|█████████████████████████████▌            | 93/132 [09:27<04:33,  7.02s/it]\u001b[A\n",
            " 71%|█████████████████████████████▉            | 94/132 [09:36<04:53,  7.72s/it]\u001b[A\n",
            " 72%|██████████████████████████████▏           | 95/132 [09:44<04:50,  7.85s/it]\u001b[A\n",
            " 73%|██████████████████████████████▌           | 96/132 [09:47<03:50,  6.41s/it]\u001b[A\n",
            " 73%|██████████████████████████████▊           | 97/132 [09:54<03:45,  6.43s/it]\u001b[A\n",
            " 74%|███████████████████████████████▏          | 98/132 [10:00<03:37,  6.38s/it]\u001b[A\n",
            " 75%|███████████████████████████████▌          | 99/132 [10:07<03:38,  6.62s/it]\u001b[A\n",
            " 76%|███████████████████████████████          | 100/132 [10:14<03:36,  6.76s/it]\u001b[A\n",
            " 77%|███████████████████████████████▎         | 101/132 [10:18<03:03,  5.93s/it]\u001b[A\n",
            " 77%|███████████████████████████████▋         | 102/132 [10:22<02:38,  5.27s/it]\u001b[A\n",
            " 78%|███████████████████████████████▉         | 103/132 [10:29<02:49,  5.83s/it]\u001b[A\n",
            " 79%|████████████████████████████████▎        | 104/132 [10:33<02:24,  5.17s/it]\u001b[A\n",
            " 80%|████████████████████████████████▌        | 105/132 [10:42<02:46,  6.18s/it]\u001b[A\n",
            " 80%|████████████████████████████████▉        | 106/132 [10:46<02:25,  5.59s/it]\u001b[A\n",
            " 81%|█████████████████████████████████▏       | 107/132 [10:51<02:15,  5.40s/it]\u001b[A\n",
            " 82%|█████████████████████████████████▌       | 108/132 [11:04<03:04,  7.71s/it]\u001b[A\n",
            " 83%|█████████████████████████████████▊       | 109/132 [11:06<02:18,  6.04s/it]\u001b[A\n",
            " 83%|██████████████████████████████████▏      | 110/132 [11:17<02:46,  7.58s/it]\u001b[A\n",
            " 84%|██████████████████████████████████▍      | 111/132 [11:20<02:12,  6.29s/it]\u001b[A\n",
            " 85%|██████████████████████████████████▊      | 112/132 [11:33<02:46,  8.33s/it]\u001b[A\n",
            " 86%|███████████████████████████████████      | 113/132 [11:38<02:15,  7.15s/it]\u001b[A\n",
            " 86%|███████████████████████████████████▍     | 114/132 [11:43<01:57,  6.51s/it]\u001b[A\n",
            " 87%|███████████████████████████████████▋     | 115/132 [11:46<01:35,  5.63s/it]\u001b[A\n",
            " 88%|████████████████████████████████████     | 116/132 [11:52<01:29,  5.62s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▎    | 117/132 [11:58<01:27,  5.80s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▋    | 118/132 [12:04<01:19,  5.70s/it]\u001b[A\n",
            " 90%|████████████████████████████████████▉    | 119/132 [12:09<01:10,  5.43s/it]\u001b[A\n",
            " 91%|█████████████████████████████████████▎   | 120/132 [12:19<01:21,  6.81s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▌   | 121/132 [12:22<01:04,  5.86s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▉   | 122/132 [12:28<00:58,  5.86s/it]\u001b[A\n",
            " 93%|██████████████████████████████████████▏  | 123/132 [12:31<00:45,  5.02s/it]\u001b[A\n",
            " 94%|██████████████████████████████████████▌  | 124/132 [12:37<00:41,  5.20s/it]\u001b[A\n",
            " 95%|██████████████████████████████████████▊  | 125/132 [12:40<00:32,  4.60s/it]\u001b[A\n",
            " 95%|███████████████████████████████████████▏ | 126/132 [12:47<00:32,  5.47s/it]\u001b[A\n",
            " 96%|███████████████████████████████████████▍ | 127/132 [12:54<00:29,  5.87s/it]\u001b[A\n",
            " 97%|███████████████████████████████████████▊ | 128/132 [13:02<00:25,  6.34s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████ | 129/132 [13:07<00:18,  6.16s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████▍| 130/132 [13:11<00:10,  5.43s/it]\u001b[A\n",
            " 99%|████████████████████████████████████████▋| 131/132 [13:14<00:04,  4.71s/it]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.8758234977722168, 'eval_runtime': 810.1351, 'eval_samples_per_second': 5.202, 'eval_steps_per_second': 0.163, 'epoch': 0.0}\n",
            "  0%|                                     | 5/39111 [15:10<244:43:42, 22.53s/it]\n",
            "100%|█████████████████████████████████████████| 132/132 [13:26<00:00,  6.49s/it]\u001b[A\n",
            "{'loss': 1.0173, 'learning_rate': 0.00012, 'epoch': 0.0}                        \u001b[A\n",
            "{'loss': 1.217, 'learning_rate': 0.00014, 'epoch': 0.0}                         \n",
            "{'loss': 0.9514, 'learning_rate': 0.00016, 'epoch': 0.0}                        \n",
            "{'loss': 0.8572, 'learning_rate': 0.00018, 'epoch': 0.0}                        \n",
            "{'loss': 1.1514, 'learning_rate': 0.0002, 'epoch': 0.0}                         \n",
            "  0%|                                    | 10/39111 [16:35<849:32:22, 78.22s/it]\n",
            "  0%|                                                   | 0/132 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▋                                          | 2/132 [00:07<08:31,  3.93s/it]\u001b[A\n",
            "  2%|▉                                          | 3/132 [00:11<07:55,  3.69s/it]\u001b[A\n",
            "  3%|█▎                                         | 4/132 [00:14<07:45,  3.64s/it]\u001b[A\n",
            "  4%|█▋                                         | 5/132 [00:24<12:09,  5.75s/it]\u001b[A\n",
            "  5%|█▉                                         | 6/132 [00:32<13:39,  6.51s/it]\u001b[A\n",
            "  5%|██▎                                        | 7/132 [00:45<17:54,  8.60s/it]\u001b[A\n",
            "  6%|██▌                                        | 8/132 [00:48<14:07,  6.83s/it]\u001b[A\n",
            "  7%|██▉                                        | 9/132 [00:53<13:00,  6.35s/it]\u001b[A\n",
            "  8%|███▏                                      | 10/132 [00:56<10:40,  5.25s/it]\u001b[A\n",
            "  8%|███▌                                      | 11/132 [01:03<11:43,  5.82s/it]\u001b[A\n",
            "  9%|███▊                                      | 12/132 [01:07<10:33,  5.28s/it]\u001b[A\n",
            " 10%|████▏                                     | 13/132 [01:10<09:09,  4.62s/it]\u001b[A\n",
            " 11%|████▍                                     | 14/132 [01:15<09:11,  4.68s/it]\u001b[A\n",
            " 11%|████▊                                     | 15/132 [01:22<10:09,  5.21s/it]\u001b[A\n",
            " 12%|█████                                     | 16/132 [01:27<10:07,  5.24s/it]\u001b[A\n",
            " 13%|█████▍                                    | 17/132 [01:36<12:01,  6.28s/it]\u001b[A\n",
            " 14%|█████▋                                    | 18/132 [01:40<11:02,  5.81s/it]\u001b[A\n",
            " 14%|██████                                    | 19/132 [01:45<10:05,  5.36s/it]\u001b[A\n",
            " 15%|██████▎                                   | 20/132 [01:53<11:32,  6.19s/it]\u001b[A\n",
            " 16%|██████▋                                   | 21/132 [02:04<14:13,  7.69s/it]\u001b[A\n",
            " 17%|███████                                   | 22/132 [02:07<11:23,  6.22s/it]\u001b[A\n",
            " 17%|███████▎                                  | 23/132 [02:30<20:43, 11.41s/it]\u001b[A\n",
            " 18%|███████▋                                  | 24/132 [02:33<15:54,  8.84s/it]\u001b[A\n",
            " 19%|███████▉                                  | 25/132 [02:40<14:31,  8.15s/it]\u001b[A\n",
            " 20%|████████▎                                 | 26/132 [02:44<12:13,  6.92s/it]\u001b[A\n",
            " 20%|████████▌                                 | 27/132 [02:49<11:19,  6.47s/it]\u001b[A\n",
            " 21%|████████▉                                 | 28/132 [02:56<11:13,  6.47s/it]\u001b[A\n",
            " 22%|█████████▏                                | 29/132 [03:01<10:34,  6.16s/it]\u001b[A\n",
            " 23%|█████████▌                                | 30/132 [03:12<13:12,  7.77s/it]\u001b[A\n",
            " 23%|█████████▊                                | 31/132 [03:19<12:24,  7.37s/it]\u001b[A\n",
            " 24%|██████████▏                               | 32/132 [03:30<14:15,  8.55s/it]\u001b[A\n",
            " 25%|██████████▌                               | 33/132 [03:36<12:38,  7.66s/it]\u001b[A\n",
            " 26%|██████████▊                               | 34/132 [03:42<11:53,  7.28s/it]\u001b[A\n",
            " 27%|███████████▏                              | 35/132 [03:48<11:13,  6.95s/it]\u001b[A\n",
            " 27%|███████████▍                              | 36/132 [03:54<10:16,  6.42s/it]\u001b[A\n",
            " 28%|███████████▊                              | 37/132 [03:58<09:11,  5.80s/it]\u001b[A\n",
            " 29%|████████████                              | 38/132 [04:06<09:55,  6.34s/it]\u001b[A\n",
            " 30%|████████████▍                             | 39/132 [04:12<09:59,  6.45s/it]\u001b[A\n",
            " 30%|████████████▋                             | 40/132 [04:19<10:01,  6.54s/it]\u001b[A\n",
            " 31%|█████████████                             | 41/132 [04:21<08:00,  5.28s/it]\u001b[A\n",
            " 32%|█████████████▎                            | 42/132 [04:34<11:13,  7.49s/it]\u001b[A\n",
            " 33%|█████████████▋                            | 43/132 [04:43<11:37,  7.83s/it]\u001b[A\n",
            " 33%|██████████████                            | 44/132 [04:46<09:32,  6.50s/it]\u001b[A\n",
            " 34%|██████████████▎                           | 45/132 [04:52<09:07,  6.30s/it]\u001b[A\n",
            " 35%|██████████████▋                           | 46/132 [04:57<08:38,  6.03s/it]\u001b[A\n",
            " 36%|██████████████▉                           | 47/132 [05:05<09:25,  6.66s/it]\u001b[A\n",
            " 36%|███████████████▎                          | 48/132 [05:08<07:38,  5.46s/it]\u001b[A\n",
            " 37%|███████████████▌                          | 49/132 [05:14<07:41,  5.56s/it]\u001b[A\n",
            " 38%|███████████████▉                          | 50/132 [05:20<07:43,  5.66s/it]\u001b[A\n",
            " 39%|████████████████▏                         | 51/132 [05:24<07:00,  5.20s/it]\u001b[A\n",
            " 39%|████████████████▌                         | 52/132 [05:28<06:29,  4.86s/it]\u001b[A\n",
            " 40%|████████████████▊                         | 53/132 [05:36<07:33,  5.75s/it]\u001b[A\n",
            " 41%|█████████████████▏                        | 54/132 [05:40<06:48,  5.23s/it]\u001b[A\n",
            " 42%|█████████████████▌                        | 55/132 [05:46<07:07,  5.56s/it]\u001b[A\n",
            " 42%|█████████████████▊                        | 56/132 [05:51<06:50,  5.40s/it]\u001b[A\n",
            " 43%|██████████████████▏                       | 57/132 [05:58<07:19,  5.86s/it]\u001b[A\n",
            " 44%|██████████████████▍                       | 58/132 [06:03<07:04,  5.73s/it]\u001b[A\n",
            " 45%|██████████████████▊                       | 59/132 [06:08<06:40,  5.49s/it]\u001b[A\n",
            " 45%|███████████████████                       | 60/132 [06:12<05:46,  4.82s/it]\u001b[A\n",
            " 46%|███████████████████▍                      | 61/132 [06:16<05:36,  4.74s/it]\u001b[A\n",
            " 47%|███████████████████▋                      | 62/132 [06:24<06:26,  5.52s/it]\u001b[A\n",
            " 48%|████████████████████                      | 63/132 [06:29<06:23,  5.56s/it]\u001b[A\n",
            " 48%|████████████████████▎                     | 64/132 [06:35<06:29,  5.73s/it]\u001b[A\n",
            " 49%|████████████████████▋                     | 65/132 [06:39<05:40,  5.08s/it]\u001b[A\n",
            " 50%|█████████████████████                     | 66/132 [06:46<06:12,  5.64s/it]\u001b[A\n",
            " 51%|█████████████████████▎                    | 67/132 [06:50<05:38,  5.20s/it]\u001b[A\n",
            " 52%|█████████████████████▋                    | 68/132 [06:59<06:38,  6.23s/it]\u001b[A\n",
            " 52%|█████████████████████▉                    | 69/132 [07:02<05:40,  5.41s/it]\u001b[A\n",
            " 53%|██████████████████████▎                   | 70/132 [07:09<06:09,  5.97s/it]\u001b[A\n",
            " 54%|██████████████████████▌                   | 71/132 [07:14<05:31,  5.43s/it]\u001b[A\n",
            " 55%|██████████████████████▉                   | 72/132 [07:19<05:20,  5.35s/it]\u001b[A\n",
            " 55%|███████████████████████▏                  | 73/132 [07:25<05:30,  5.60s/it]\u001b[A\n",
            " 56%|███████████████████████▌                  | 74/132 [07:29<04:52,  5.04s/it]\u001b[A\n",
            " 57%|███████████████████████▊                  | 75/132 [07:35<05:11,  5.46s/it]\u001b[A\n",
            " 58%|████████████████████████▏                 | 76/132 [07:39<04:43,  5.06s/it]\u001b[A\n",
            " 58%|████████████████████████▌                 | 77/132 [07:44<04:37,  5.05s/it]\u001b[A\n",
            " 59%|████████████████████████▊                 | 78/132 [07:48<04:19,  4.81s/it]\u001b[A\n",
            " 60%|█████████████████████████▏                | 79/132 [07:53<04:11,  4.74s/it]\u001b[A\n",
            " 61%|█████████████████████████▍                | 80/132 [08:03<05:29,  6.34s/it]\u001b[A\n",
            " 61%|█████████████████████████▊                | 81/132 [08:08<04:54,  5.77s/it]\u001b[A\n",
            " 62%|██████████████████████████                | 82/132 [08:13<04:43,  5.67s/it]\u001b[A\n",
            " 63%|██████████████████████████▍               | 83/132 [08:19<04:36,  5.65s/it]\u001b[A\n",
            " 64%|██████████████████████████▋               | 84/132 [08:25<04:39,  5.82s/it]\u001b[A\n",
            " 64%|███████████████████████████               | 85/132 [08:34<05:18,  6.78s/it]\u001b[A\n",
            " 65%|███████████████████████████▎              | 86/132 [08:36<04:12,  5.48s/it]\u001b[A\n",
            " 66%|███████████████████████████▋              | 87/132 [08:44<04:37,  6.17s/it]\u001b[A\n",
            " 67%|████████████████████████████              | 88/132 [08:47<03:49,  5.21s/it]\u001b[A\n",
            " 67%|████████████████████████████▎             | 89/132 [08:55<04:22,  6.11s/it]\u001b[A\n",
            " 68%|████████████████████████████▋             | 90/132 [09:00<04:01,  5.74s/it]\u001b[A\n",
            " 69%|████████████████████████████▉             | 91/132 [09:11<04:55,  7.21s/it]\u001b[A\n",
            " 70%|█████████████████████████████▎            | 92/132 [09:20<05:07,  7.69s/it]\u001b[A\n",
            " 70%|█████████████████████████████▌            | 93/132 [09:25<04:33,  7.02s/it]\u001b[A\n",
            " 71%|█████████████████████████████▉            | 94/132 [09:34<04:53,  7.72s/it]\u001b[A\n",
            " 72%|██████████████████████████████▏           | 95/132 [09:43<04:50,  7.85s/it]\u001b[A\n",
            " 73%|██████████████████████████████▌           | 96/132 [09:46<03:50,  6.41s/it]\u001b[A\n",
            " 73%|██████████████████████████████▊           | 97/132 [09:52<03:45,  6.43s/it]\u001b[A\n",
            " 74%|███████████████████████████████▏          | 98/132 [09:58<03:36,  6.38s/it]\u001b[A\n",
            " 75%|███████████████████████████████▌          | 99/132 [10:05<03:38,  6.62s/it]\u001b[A\n",
            " 76%|███████████████████████████████          | 100/132 [10:13<03:36,  6.76s/it]\u001b[A\n",
            " 77%|███████████████████████████████▎         | 101/132 [10:17<03:03,  5.94s/it]\u001b[A\n",
            " 77%|███████████████████████████████▋         | 102/132 [10:20<02:38,  5.28s/it]\u001b[A\n",
            " 78%|███████████████████████████████▉         | 103/132 [10:27<02:49,  5.84s/it]\u001b[A\n",
            " 79%|████████████████████████████████▎        | 104/132 [10:31<02:24,  5.18s/it]\u001b[A\n",
            " 80%|████████████████████████████████▌        | 105/132 [10:40<02:47,  6.19s/it]\u001b[A\n",
            " 80%|████████████████████████████████▉        | 106/132 [10:44<02:25,  5.59s/it]\u001b[A\n",
            " 81%|█████████████████████████████████▏       | 107/132 [10:49<02:15,  5.40s/it]\u001b[A\n",
            " 82%|█████████████████████████████████▌       | 108/132 [11:02<03:04,  7.71s/it]\u001b[A\n",
            " 83%|█████████████████████████████████▊       | 109/132 [11:04<02:19,  6.05s/it]\u001b[A\n",
            " 83%|██████████████████████████████████▏      | 110/132 [11:15<02:46,  7.58s/it]\u001b[A\n",
            " 84%|██████████████████████████████████▍      | 111/132 [11:19<02:12,  6.29s/it]\u001b[A\n",
            " 85%|██████████████████████████████████▊      | 112/132 [11:32<02:46,  8.33s/it]\u001b[A\n",
            " 86%|███████████████████████████████████      | 113/132 [11:36<02:15,  7.16s/it]\u001b[A\n",
            " 86%|███████████████████████████████████▍     | 114/132 [11:41<01:57,  6.51s/it]\u001b[A\n",
            " 87%|███████████████████████████████████▋     | 115/132 [11:45<01:35,  5.63s/it]\u001b[A\n",
            " 88%|████████████████████████████████████     | 116/132 [11:50<01:30,  5.63s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▎    | 117/132 [11:56<01:27,  5.80s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▋    | 118/132 [12:02<01:19,  5.70s/it]\u001b[A\n",
            " 90%|████████████████████████████████████▉    | 119/132 [12:07<01:10,  5.43s/it]\u001b[A\n",
            " 91%|█████████████████████████████████████▎   | 120/132 [12:17<01:21,  6.81s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▌   | 121/132 [12:20<01:04,  5.86s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▉   | 122/132 [12:26<00:58,  5.86s/it]\u001b[A\n",
            " 93%|██████████████████████████████████████▏  | 123/132 [12:29<00:45,  5.02s/it]\u001b[A\n",
            " 94%|██████████████████████████████████████▌  | 124/132 [12:35<00:41,  5.20s/it]\u001b[A\n",
            " 95%|██████████████████████████████████████▊  | 125/132 [12:38<00:32,  4.60s/it]\u001b[A\n",
            " 95%|███████████████████████████████████████▏ | 126/132 [12:46<00:32,  5.47s/it]\u001b[A\n",
            " 96%|███████████████████████████████████████▍ | 127/132 [12:52<00:29,  5.87s/it]\u001b[A\n",
            " 97%|███████████████████████████████████████▊ | 128/132 [13:00<00:25,  6.34s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████ | 129/132 [13:06<00:18,  6.16s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████▍| 130/132 [13:09<00:10,  5.43s/it]\u001b[A\n",
            " 99%|████████████████████████████████████████▋| 131/132 [13:12<00:04,  4.70s/it]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.8401325941085815, 'eval_runtime': 808.2868, 'eval_samples_per_second': 5.213, 'eval_steps_per_second': 0.163, 'epoch': 0.0}\n",
            "  0%|                                    | 10/39111 [30:03<849:32:22, 78.22s/it]\n",
            "100%|█████████████████████████████████████████| 132/132 [13:25<00:00,  6.49s/it]\u001b[A\n",
            "{'loss': 0.8225, 'learning_rate': 0.00019999999967722936, 'epoch': 0.0}         \u001b[A\n",
            "{'loss': 0.8483, 'learning_rate': 0.00019999999870891745, 'epoch': 0.0}         \n",
            "{'loss': 0.7848, 'learning_rate': 0.0001999999970950643, 'epoch': 0.0}          \n",
            "{'loss': 0.8681, 'learning_rate': 0.00019999999483566984, 'epoch': 0.0}         \n",
            "{'loss': 0.9066, 'learning_rate': 0.0001999999919307342, 'epoch': 0.0}          \n",
            "  0%|                                    | 15/39111 [31:51<965:08:36, 88.87s/it]\n",
            "  0%|                                                   | 0/132 [00:00<?, ?it/s]\u001b[A\n",
            "  2%|▋                                          | 2/132 [00:07<08:30,  3.93s/it]\u001b[A\n",
            "  2%|▉                                          | 3/132 [00:11<07:55,  3.68s/it]\u001b[A\n",
            "  3%|█▎                                         | 4/132 [00:14<07:45,  3.63s/it]\u001b[A\n",
            "  4%|█▋                                         | 5/132 [00:24<12:09,  5.74s/it]\u001b[A\n",
            "  5%|█▉                                         | 6/132 [00:32<13:39,  6.51s/it]\u001b[A\n",
            "  5%|██▎                                        | 7/132 [00:45<17:54,  8.60s/it]\u001b[A\n",
            "  6%|██▌                                        | 8/132 [00:48<14:07,  6.83s/it]\u001b[A\n",
            "  7%|██▉                                        | 9/132 [00:53<13:01,  6.35s/it]\u001b[A\n",
            "  8%|███▏                                      | 10/132 [00:56<10:40,  5.25s/it]\u001b[A\n",
            "  8%|███▌                                      | 11/132 [01:03<11:44,  5.82s/it]\u001b[A\n",
            "  9%|███▊                                      | 12/132 [01:07<10:33,  5.28s/it]\u001b[A\n",
            " 10%|████▏                                     | 13/132 [01:10<09:09,  4.62s/it]\u001b[A\n",
            " 11%|████▍                                     | 14/132 [01:15<09:12,  4.68s/it]\u001b[A\n",
            " 11%|████▊                                     | 15/132 [01:22<10:10,  5.22s/it]\u001b[A\n",
            " 12%|█████                                     | 16/132 [01:27<10:08,  5.25s/it]\u001b[A\n",
            " 13%|█████▍                                    | 17/132 [01:36<12:02,  6.28s/it]\u001b[A\n",
            " 14%|█████▋                                    | 18/132 [01:40<11:02,  5.82s/it]\u001b[A\n",
            " 14%|██████                                    | 19/132 [01:45<10:06,  5.37s/it]\u001b[A\n",
            " 15%|██████▎                                   | 20/132 [01:53<11:33,  6.19s/it]\u001b[A\n",
            " 16%|██████▋                                   | 21/132 [02:04<14:14,  7.69s/it]\u001b[A\n",
            " 17%|███████                                   | 22/132 [02:07<11:24,  6.22s/it]\u001b[A\n",
            " 17%|███████▎                                  | 23/132 [02:30<20:44, 11.42s/it]\u001b[A\n",
            " 18%|███████▋                                  | 24/132 [02:33<15:55,  8.85s/it]\u001b[A\n",
            " 19%|███████▉                                  | 25/132 [02:40<14:32,  8.15s/it]\u001b[A\n",
            " 20%|████████▎                                 | 26/132 [02:44<12:13,  6.92s/it]\u001b[A\n",
            " 20%|████████▌                                 | 27/132 [02:49<11:19,  6.47s/it]\u001b[A\n",
            " 21%|████████▉                                 | 28/132 [02:56<11:13,  6.48s/it]\u001b[A\n",
            " 22%|█████████▏                                | 29/132 [03:01<10:34,  6.16s/it]\u001b[A\n",
            " 23%|█████████▌                                | 30/132 [03:13<13:12,  7.77s/it]\u001b[A\n",
            " 23%|█████████▊                                | 31/132 [03:19<12:24,  7.37s/it]\u001b[A\n",
            " 24%|██████████▏                               | 32/132 [03:30<14:15,  8.56s/it]\u001b[A\n",
            " 25%|██████████▌                               | 33/132 [03:36<12:38,  7.66s/it]\u001b[A\n",
            " 26%|██████████▊                               | 34/132 [03:42<11:53,  7.29s/it]\u001b[A\n",
            " 27%|███████████▏                              | 35/132 [03:48<11:14,  6.95s/it]\u001b[A\n",
            " 27%|███████████▍                              | 36/132 [03:54<10:16,  6.43s/it]\u001b[A\n",
            " 28%|███████████▊                              | 37/132 [03:58<09:11,  5.80s/it]\u001b[A\n",
            " 29%|████████████                              | 38/132 [04:06<09:56,  6.34s/it]\u001b[A\n",
            " 30%|████████████▍                             | 39/132 [04:12<10:00,  6.45s/it]\u001b[A\n",
            " 30%|████████████▋                             | 40/132 [04:19<10:01,  6.54s/it]\u001b[A\n",
            " 31%|█████████████                             | 41/132 [04:21<08:00,  5.28s/it]\u001b[A\n",
            " 32%|█████████████▎                            | 42/132 [04:34<11:13,  7.49s/it]\u001b[A\n",
            " 33%|█████████████▋                            | 43/132 [04:43<11:37,  7.83s/it]\u001b[A\n",
            " 33%|██████████████                            | 44/132 [04:46<09:32,  6.50s/it]\u001b[A\n",
            " 34%|██████████████▎                           | 45/132 [04:52<09:08,  6.30s/it]\u001b[A\n",
            " 35%|██████████████▋                           | 46/132 [04:57<08:38,  6.03s/it]\u001b[A\n",
            " 36%|██████████████▉                           | 47/132 [05:05<09:26,  6.66s/it]\u001b[A\n",
            " 36%|███████████████▎                          | 48/132 [05:08<07:38,  5.46s/it]\u001b[A\n",
            " 37%|███████████████▌                          | 49/132 [05:14<07:41,  5.56s/it]\u001b[A\n",
            " 38%|███████████████▉                          | 50/132 [05:20<07:43,  5.66s/it]\u001b[A\n",
            " 39%|████████████████▏                         | 51/132 [05:24<07:01,  5.20s/it]\u001b[A\n",
            " 39%|████████████████▌                         | 52/132 [05:28<06:29,  4.87s/it]\u001b[A\n",
            " 40%|████████████████▊                         | 53/132 [05:36<07:34,  5.75s/it]\u001b[A\n",
            " 41%|█████████████████▏                        | 54/132 [05:40<06:48,  5.23s/it]\u001b[A\n",
            " 42%|█████████████████▌                        | 55/132 [05:46<07:07,  5.55s/it]\u001b[A\n",
            " 42%|█████████████████▊                        | 56/132 [05:51<06:49,  5.39s/it]\u001b[A\n",
            " 43%|██████████████████▏                       | 57/132 [05:58<07:19,  5.86s/it]\u001b[A\n",
            " 44%|██████████████████▍                       | 58/132 [06:04<07:04,  5.73s/it]\u001b[A\n",
            " 45%|██████████████████▊                       | 59/132 [06:08<06:40,  5.49s/it]\u001b[A\n",
            " 45%|███████████████████                       | 60/132 [06:12<05:46,  4.82s/it]\u001b[A\n",
            " 46%|███████████████████▍                      | 61/132 [06:16<05:36,  4.73s/it]\u001b[A\n",
            " 47%|███████████████████▋                      | 62/132 [06:24<06:26,  5.52s/it]\u001b[A\n",
            " 48%|████████████████████                      | 63/132 [06:29<06:22,  5.55s/it]\u001b[A\n",
            " 48%|████████████████████▎                     | 64/132 [06:35<06:28,  5.72s/it]\u001b[A\n",
            " 49%|████████████████████▋                     | 65/132 [06:39<05:39,  5.07s/it]\u001b[A\n",
            " 50%|█████████████████████                     | 66/132 [06:46<06:12,  5.64s/it]\u001b[A\n",
            " 51%|█████████████████████▎                    | 67/132 [06:50<05:38,  5.20s/it]\u001b[A\n",
            " 52%|█████████████████████▋                    | 68/132 [06:59<06:38,  6.23s/it]\u001b[A\n",
            " 52%|█████████████████████▉                    | 69/132 [07:02<05:40,  5.41s/it]\u001b[A\n",
            " 53%|██████████████████████▎                   | 70/132 [07:09<06:09,  5.97s/it]\u001b[A\n",
            " 54%|██████████████████████▌                   | 71/132 [07:14<05:31,  5.43s/it]\u001b[A\n",
            " 55%|██████████████████████▉                   | 72/132 [07:19<05:20,  5.35s/it]\u001b[A\n",
            " 55%|███████████████████████▏                  | 73/132 [07:25<05:30,  5.60s/it]\u001b[A\n",
            " 56%|███████████████████████▌                  | 74/132 [07:29<04:52,  5.04s/it]\u001b[A\n",
            " 57%|███████████████████████▊                  | 75/132 [07:35<05:11,  5.46s/it]\u001b[A\n",
            " 58%|████████████████████████▏                 | 76/132 [07:39<04:43,  5.07s/it]\u001b[A\n",
            " 58%|████████████████████████▌                 | 77/132 [07:44<04:37,  5.04s/it]\u001b[A\n",
            " 59%|████████████████████████▊                 | 78/132 [07:49<04:19,  4.81s/it]\u001b[A\n",
            " 60%|█████████████████████████▏                | 79/132 [07:53<04:11,  4.74s/it]\u001b[A\n",
            " 61%|█████████████████████████▍                | 80/132 [08:03<05:29,  6.34s/it]\u001b[A\n",
            " 61%|█████████████████████████▊                | 81/132 [08:08<04:54,  5.77s/it]\u001b[A\n",
            " 62%|██████████████████████████                | 82/132 [08:13<04:43,  5.67s/it]\u001b[A\n",
            " 63%|██████████████████████████▍               | 83/132 [08:19<04:36,  5.65s/it]\u001b[A\n",
            " 64%|██████████████████████████▋               | 84/132 [08:25<04:39,  5.82s/it]\u001b[A\n",
            " 64%|███████████████████████████               | 85/132 [08:34<05:18,  6.77s/it]\u001b[A\n",
            " 65%|███████████████████████████▎              | 86/132 [08:36<04:12,  5.48s/it]\u001b[A\n",
            " 66%|███████████████████████████▋              | 87/132 [08:44<04:37,  6.17s/it]\u001b[A\n",
            " 67%|████████████████████████████              | 88/132 [08:47<03:49,  5.21s/it]\u001b[A\n",
            " 67%|████████████████████████████▎             | 89/132 [08:55<04:22,  6.11s/it]\u001b[A\n",
            " 68%|████████████████████████████▋             | 90/132 [09:00<04:01,  5.75s/it]\u001b[A\n",
            " 69%|████████████████████████████▉             | 91/132 [09:11<04:55,  7.22s/it]\u001b[A\n",
            " 70%|█████████████████████████████▎            | 92/132 [09:20<05:07,  7.69s/it]\u001b[A\n",
            " 70%|█████████████████████████████▌            | 93/132 [09:25<04:34,  7.03s/it]\u001b[A\n",
            " 71%|█████████████████████████████▉            | 94/132 [09:34<04:53,  7.72s/it]\u001b[A\n",
            " 72%|██████████████████████████████▏           | 95/132 [09:43<04:50,  7.85s/it]\u001b[A\n",
            " 73%|██████████████████████████████▌           | 96/132 [09:46<03:50,  6.41s/it]\u001b[A\n",
            " 73%|██████████████████████████████▊           | 97/132 [09:52<03:44,  6.43s/it]\u001b[A\n",
            " 74%|███████████████████████████████▏          | 98/132 [09:58<03:37,  6.38s/it]\u001b[A\n",
            " 75%|███████████████████████████████▌          | 99/132 [10:06<03:38,  6.62s/it]\u001b[A\n",
            " 76%|███████████████████████████████          | 100/132 [10:13<03:36,  6.76s/it]\u001b[A\n",
            " 77%|███████████████████████████████▎         | 101/132 [10:17<03:03,  5.94s/it]\u001b[A\n",
            " 77%|███████████████████████████████▋         | 102/132 [10:20<02:38,  5.28s/it]\u001b[A\n",
            " 78%|███████████████████████████████▉         | 103/132 [10:28<02:49,  5.83s/it]\u001b[A\n",
            " 79%|████████████████████████████████▎        | 104/132 [10:31<02:24,  5.18s/it]\u001b[A\n",
            " 80%|████████████████████████████████▌        | 105/132 [10:40<02:46,  6.18s/it]\u001b[A\n",
            " 80%|████████████████████████████████▉        | 106/132 [10:44<02:25,  5.58s/it]\u001b[A\n",
            " 81%|█████████████████████████████████▏       | 107/132 [10:49<02:14,  5.40s/it]\u001b[A\n",
            " 82%|█████████████████████████████████▌       | 108/132 [11:02<03:04,  7.70s/it]\u001b[A\n",
            " 83%|█████████████████████████████████▊       | 109/132 [11:04<02:18,  6.04s/it]\u001b[A\n",
            " 83%|██████████████████████████████████▏      | 110/132 [11:15<02:46,  7.57s/it]\u001b[A\n",
            " 84%|██████████████████████████████████▍      | 111/132 [11:19<02:12,  6.29s/it]\u001b[A\n",
            " 85%|██████████████████████████████████▊      | 112/132 [11:32<02:46,  8.34s/it]\u001b[A\n",
            " 86%|███████████████████████████████████      | 113/132 [11:36<02:15,  7.16s/it]\u001b[A\n",
            " 86%|███████████████████████████████████▍     | 114/132 [11:41<01:57,  6.51s/it]\u001b[A\n",
            " 87%|███████████████████████████████████▋     | 115/132 [11:45<01:35,  5.63s/it]\u001b[A\n",
            " 88%|████████████████████████████████████     | 116/132 [11:50<01:30,  5.63s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▎    | 117/132 [11:57<01:27,  5.81s/it]\u001b[A\n",
            " 89%|████████████████████████████████████▋    | 118/132 [12:02<01:19,  5.70s/it]\u001b[A\n",
            " 90%|████████████████████████████████████▉    | 119/132 [12:07<01:10,  5.43s/it]\u001b[A\n",
            " 91%|█████████████████████████████████████▎   | 120/132 [12:17<01:21,  6.82s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▌   | 121/132 [12:20<01:04,  5.86s/it]\u001b[A\n",
            " 92%|█████████████████████████████████████▉   | 122/132 [12:26<00:58,  5.85s/it]\u001b[A\n",
            " 93%|██████████████████████████████████████▏  | 123/132 [12:29<00:45,  5.02s/it]\u001b[A\n",
            " 94%|██████████████████████████████████████▌  | 124/132 [12:35<00:41,  5.20s/it]\u001b[A\n",
            " 95%|██████████████████████████████████████▊  | 125/132 [12:38<00:32,  4.60s/it]\u001b[A\n",
            " 95%|███████████████████████████████████████▏ | 126/132 [12:46<00:32,  5.47s/it]\u001b[A\n",
            " 96%|███████████████████████████████████████▍ | 127/132 [12:53<00:29,  5.87s/it]\u001b[A\n",
            " 97%|███████████████████████████████████████▊ | 128/132 [13:00<00:25,  6.34s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████ | 129/132 [13:06<00:18,  6.16s/it]\u001b[A\n",
            " 98%|████████████████████████████████████████▍| 130/132 [13:09<00:10,  5.43s/it]\u001b[A\n",
            " 99%|████████████████████████████████████████▋| 131/132 [13:12<00:04,  4.71s/it]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 0.7737265825271606, 'eval_runtime': 808.3726, 'eval_samples_per_second': 5.213, 'eval_steps_per_second': 0.163, 'epoch': 0.0}\n",
            "  0%|                                    | 15/39111 [45:20<965:08:36, 88.87s/it]\n",
            "100%|█████████████████████████████████████████| 132/132 [13:25<00:00,  6.49s/it]\u001b[A\n",
            "{'loss': 0.7656, 'learning_rate': 0.00019999998838025727, 'epoch': 0.0}         \u001b[A\n",
            "{'loss': 0.7605, 'learning_rate': 0.00019999998418423917, 'epoch': 0.0}         \n",
            "{'loss': 0.7971, 'learning_rate': 0.0001999999793426799, 'epoch': 0.0}          \n",
            "  0%|                                  | 18/39111 [46:21<1761:36:15, 162.22s/it]Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2636, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 576, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/operations.py\", line 564, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/amp/autocast_mode.py\", line 14, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1156, in forward\n",
            "    output = self._run_ddp_forward(*inputs, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/parallel/distributed.py\", line 1110, in _run_ddp_forward\n",
            "    return module_to_run(*inputs[0], **kwargs[0])  # type: ignore[index]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 785, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 782, in forward\n",
            "    loss = loss_fct(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/loss.py\", line 1174, in forward\n",
            "    return F.cross_entropy(input, target, weight=self.weight,\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/functional.py\", line 3029, in cross_entropy\n",
            "    return torch._C._nn.cross_entropy_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index, label_smoothing)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 4.62 GiB (GPU 2; 79.15 GiB total capacity; 69.88 GiB already allocated; 2.44 MiB free; 76.40 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 5246 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 5247 closing signal SIGTERM\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 5249 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 2 (pid: 5248) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_15:52:16\n",
            "  host      : 48ca1761a330\n",
            "  rank      : 2 (local_rank: 2)\n",
            "  exitcode  : 1 (pid: 5248)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"f9d3d5af-bcbd-4928-923d-3295071c9b2a\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"f9d3d5af-bcbd-4928-923d-3295071c9b2a\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": [
              "$(document).ready(\n",
              "    function() {\n",
              "        function appendUniqueDiv(){\n",
              "            // append a div with our uuid so we can check that it's already\n",
              "            // been sent and avoid duplicates on page reload\n",
              "            var notifiedDiv = document.createElement(\"div\")\n",
              "            notifiedDiv.id = \"f9d3d5af-bcbd-4928-923d-3295071c9b2a\"\n",
              "            element.append(notifiedDiv)\n",
              "        }\n",
              "\n",
              "        // only send notifications if the pageload is complete; this will\n",
              "        // help stop extra notifications when a saved notebook is loaded,\n",
              "        // which during testing gives us state \"interactive\", not \"complete\"\n",
              "        if (document.readyState === 'complete') {\n",
              "            // check for the div that signifies that the notification\n",
              "            // was already sent\n",
              "            if (document.getElementById(\"f9d3d5af-bcbd-4928-923d-3295071c9b2a\") === null) {\n",
              "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell Execution Has Finished!!\", \"autonotify_after\": \"30\", \"autonotify_output\": true, \"only_in_background\": false};\n",
              "\n",
              "                // We have a notification but the window is active\n",
              "                if (notificationPayload.only_in_background && !window.jupyterNotifyIsInBackground) {\n",
              "                    appendUniqueDiv();\n",
              "                    return;\n",
              "                }\n",
              "                if (Notification.permission !== 'denied') {\n",
              "                    if (Notification.permission !== 'granted') { \n",
              "                        Notification.requestPermission(function (permission) {\n",
              "                            if(!('permission' in Notification)) {\n",
              "                                Notification.permission = permission\n",
              "                            }\n",
              "                        })\n",
              "                    }\n",
              "                    if (Notification.permission === 'granted') {\n",
              "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
              "                    appendUniqueDiv()\n",
              "                    notification.onclick = function () {\n",
              "                        window.focus();\n",
              "                        this.close();\n",
              "                        };\n",
              "                    } \n",
              "                }     \n",
              "            }\n",
              "        }\n",
              "    }\n",
              ")\n"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "noYMg25xo7ZH"
      },
      "source": [
        "## #9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgIm_x1po7ZH"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9gVy7Gvo7ZH"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWDQTL_vGSL"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqeHYEGDY7Q"
      },
      "source": [
        "### Upload checkpoints to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkekyF8DKerI"
      },
      "outputs": [],
      "source": [
        "%cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8SpVYpNY1y"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_79rGoKkA_"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK8IWmrtKvlJ"
      },
      "outputs": [],
      "source": [
        "# ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyQDH-YDY7R"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5TnMCeLDY7M"
      },
      "source": [
        "## Below are ad hoc cells handling issues during training\n",
        "\n",
        "current out dir:\n",
        "\n",
        "```\n",
        "/content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCBx0UZDY7M"
      },
      "source": [
        "### Force release VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0VdzsqSDY7M"
      },
      "outputs": [],
      "source": [
        "# First interupt the kernel, wait a few seconds then run this to kill finetune to release VRAM\n",
        "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7StF_PDY7N"
      },
      "source": [
        "### Clean the finetuned model and all checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4c-7KHDY7N"
      },
      "outputs": [],
      "source": [
        "# Only run this to start over\n",
        "!rm -rf /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6s4xi9jDY7N"
      },
      "source": [
        "### Zip the prepared dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-aBbwEDY7N"
      },
      "outputs": [],
      "source": [
        "!apt install zip\n",
        "!zip -r last_run_prepared.zip -xi last_run_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFC20jMDY7N"
      },
      "source": [
        "### Monitoring GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgPaXew2DY7O"
      },
      "outputs": [],
      "source": [
        "# Run this in a seperate terminal\n",
        "!nvitop -m full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFlPnbXDY7O"
      },
      "source": [
        "### Fix DISK FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5CWZu7DY7O"
      },
      "outputs": [],
      "source": [
        "%cd /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqYxcpZDY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h|grep G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIJxrv1DY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h /root/.local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25FTebpHDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/Trash/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K4d-2ocDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/wandb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOoZLugDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17SexQnqDY7P"
      },
      "source": [
        "### Check who is using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE8A5crNDY7P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install lsof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNCASirDY7Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!lsof /dev/nvidia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZv_qjgDY7Y"
      },
      "source": [
        "### A new bash without tmux etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkJAXwTDY7Z"
      },
      "outputs": [],
      "source": [
        "!bash --norc --noprofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3s54THDY7Z"
      },
      "source": [
        "### Clean up all checkpoints but last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fSoQ8hteEDX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJtsGJ0oeEDX"
      },
      "outputs": [],
      "source": [
        "!cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk '{print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdO1EybEDY7Z"
      },
      "outputs": [],
      "source": [
        "!cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk 'NR > 0 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyk_zXK5eEDX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}