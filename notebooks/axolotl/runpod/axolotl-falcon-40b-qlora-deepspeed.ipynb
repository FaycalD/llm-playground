{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/notebooks/axolotl/runpod/axolotl-falcon-40b-qlora-deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yoSiRvDY7G"
      },
      "source": [
        "# Finetuning falcon-40b\n",
        "\n",
        "- Axolotl+QLoRA\n",
        "- minotaur datasets\n",
        "- deepspeed ZeRO 3 8xGPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpbQuaxDY7H"
      },
      "source": [
        "<!-- https://jupyterlab.readthedocs.io/en/stable/user/commands.html#commands-list -->\n",
        "<button data-commandLinker-command=\"apputils:change-theme\" data-commandlinker-args='{\"theme\": \"JupyterLab Dark\"}' href=\"#\">Dark theme</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/\"}' href=\"#\">llm-playground</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/\"}' href=\"#\">axolotl</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/notebooks/axolotl/runpod\"}' href=\"#\">Runpod notebooks</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples\"}' href=\"#\">axolotl configs</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/storage\"}' href=\"#\">Storage</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/axolotl-trained\"}' href=\"#\">axolotl-trained</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples/falcon/config-40b-qlora.yml\"}' href=\"#\">Edit qlora config</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/ds_config.json\"}' href=\"#\">Edit ds config</button>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZd-cf70HM2n"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrtaRkauKLL"
      },
      "source": [
        "### Set HF Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIyW4yWuHvd"
      },
      "outputs": [],
      "source": [
        "# %env HF_DATASETS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOfHUYQduRX"
      },
      "outputs": [],
      "source": [
        "#%env TRANSFORMERS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR9oblHTduRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaKgWKnduRY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJMsnPnOduRY"
      },
      "outputs": [],
      "source": [
        "!ln -s /content/cache /root/.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqQyPp5HbAm"
      },
      "source": [
        "### HF Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6wC1lP23B-4",
        "outputId": "9e80e244-b214-4dec-cfba-8d98d0aeba6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid.\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# For axolotl push_dataset_to_hub\n",
        "import os\n",
        "from huggingface_hub import notebook_login, login\n",
        "# Colab:\n",
        "# notebook_login()\n",
        "# RunPod:\n",
        "login(os.environ.get(\"HUGGINGFACE_TOKEN\"), add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AigYR_DY7X"
      },
      "source": [
        "### Update axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQYJdcd7DY7X",
        "outputId": "1244d551-db77-4320-a411-c602d6f28fcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn5aNACDY7X",
        "scrolled": true,
        "outputId": "2165b399-d984-4c59-d6b7-37c03b7e986a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'axolotl-update'...\n",
            "remote: Enumerating objects: 3152, done.\u001b[K\n",
            "remote: Counting objects: 100% (1383/1383), done.\u001b[K\n",
            "remote: Compressing objects: 100% (341/341), done.\u001b[K\n",
            "remote: Total 3152 (delta 1132), reused 1170 (delta 991), pack-reused 1769\u001b[K\n",
            "Receiving objects: 100% (3152/3152), 1.42 MiB | 5.36 MiB/s, done.\n",
            "Resolving deltas: 100% (1986/1986), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl axolotl-update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdY5tXtYDY7X"
      },
      "outputs": [],
      "source": [
        "!cp -r axolotl-update/* axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsKmIUmDY7Y",
        "outputId": "1e69b19b-e820-41e2-876f-130c1b0f7be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljX3Yir2DY7Y",
        "scrolled": true,
        "outputId": "50f1e011-dcd0-4d01-aa87-2203450c439a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes not staged for commit:\n",
            "  (use \"git add <file>...\" to update what will be committed)\n",
            "  (use \"git restore <file>...\" to discard changes in working directory)\n",
            "\t\u001b[31mmodified:   FAQS.md\u001b[m\n",
            "\t\u001b[31mmodified:   README.md\u001b[m\n",
            "\t\u001b[31mmodified:   docker/Dockerfile\u001b[m\n",
            "\t\u001b[31mmodified:   docker/Dockerfile-base\u001b[m\n",
            "\t\u001b[31mmodified:   examples/falcon/config-7b-lora.yml\u001b[m\n",
            "\t\u001b[31mmodified:   examples/falcon/config-7b.yml\u001b[m\n",
            "\t\u001b[31mmodified:   examples/gptq-lora-7b/README.md\u001b[m\n",
            "\t\u001b[31mmodified:   examples/gptq-lora-7b/config.yml\u001b[m\n",
            "\t\u001b[31mmodified:   examples/mpt-7b/config.yml\u001b[m\n",
            "\t\u001b[31mmodified:   requirements.txt\u001b[m\n",
            "\t\u001b[31mmodified:   scripts/finetune.py\u001b[m\n",
            "\t\u001b[31mmodified:   scripts/runpod-entrypoint.sh\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/datasets.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/flash_attn.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/prompt_strategies/alpaca_chat.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/prompters.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/utils/data.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/utils/models.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/utils/trainer.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/utils/validation.py\u001b[m\n",
            "\t\u001b[31mmodified:   src/axolotl/utils/wandb.py\u001b[m\n",
            "\t\u001b[31mmodified:   tests/test_prompt_tokenizers.py\u001b[m\n",
            "\t\u001b[31mmodified:   tests/test_validation.py\u001b[m\n",
            "\n",
            "Untracked files:\n",
            "  (use \"git add <file>...\" to include in what will be committed)\n",
            "\t\u001b[31mdocker-compose.yaml\u001b[m\n",
            "\t\u001b[31mexamples/cerebras/\u001b[m\n",
            "\t\u001b[31mexamples/falcon/config-7b-qlora.yml\u001b[m\n",
            "\t\u001b[31mexamples/falcon/ft.yml\u001b[m\n",
            "\t\u001b[31mexamples/falcon/lora.yml\u001b[m\n",
            "\t\u001b[31mexamples/falcon/qlora.yml\u001b[m\n",
            "\t\u001b[31mexamples/gptj/\u001b[m\n",
            "\t\u001b[31mexamples/huggyllama/\u001b[m\n",
            "\t\u001b[31mexamples/jeopardy-bot/\u001b[m\n",
            "\t\u001b[31mexamples/openllama-3b/\u001b[m\n",
            "\t\u001b[31mexamples/openllama/\u001b[m\n",
            "\t\u001b[31mexamples/pythia/\u001b[m\n",
            "\t\u001b[31mimage/axolotl-badge-web.png\u001b[m\n",
            "\t\u001b[31msrc/axolotl/monkeypatch/\u001b[m\n",
            "\t\u001b[31msrc/axolotl/prompt_strategies/context_qa.py\u001b[m\n",
            "\t\u001b[31msrc/axolotl/prompt_strategies/sharegpt_jokes.py\u001b[m\n",
            "\t\u001b[31msrc/axolotl/prompt_strategies/sharegpt_simple.py\u001b[m\n",
            "\t\u001b[31mtests/fixtures/alpaca/\u001b[m\n",
            "\t\u001b[31mtests/test_packed_dataset.py\u001b[m\n",
            "\n",
            "no changes added to commit (use \"git add\" and/or \"git commit -a\")\n"
          ]
        }
      ],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaorAF2DY7Y",
        "scrolled": true,
        "outputId": "01f52eec-42e7-4965-f26d-4364ddb3232c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obtaining file:///workspace/axolotl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-awf5nfb1/transformers_914ce9d55534468ea1a220819448f7a6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-awf5nfb1/transformers_914ce9d55534468ea1a220819448f7a6\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 3723329d014a7b144863e597ea4fe6de5e6a8279\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: bitsandbytes>=0.39.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg (from axolotl==0.1) (0.39.0)\n",
            "Requirement already satisfied: accelerate in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.21.0.dev0)\n",
            "Requirement already satisfied: addict in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (2.4.0)\n",
            "Requirement already satisfied: fire in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.5.0)\n",
            "Requirement already satisfied: PyYAML==6.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (6.0)\n",
            "Requirement already satisfied: datasets in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (2.12.0)\n",
            "Requirement already satisfied: sentencepiece in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.1.99)\n",
            "Requirement already satisfied: wandb in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.15.3)\n",
            "Requirement already satisfied: einops in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.6.1)\n",
            "Requirement already satisfied: xformers in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.0.20)\n",
            "Requirement already satisfied: bert-score==0.3.13 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.3.13)\n",
            "Requirement already satisfied: evaluate==0.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.4.0)\n",
            "Requirement already satisfied: rouge-score==0.1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (0.1.2)\n",
            "Requirement already satisfied: scipy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from axolotl==0.1) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.1)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.2)\n",
            "Requirement already satisfied: numpy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (1.24.3)\n",
            "Requirement already satisfied: requests in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from bert-score==0.3.13->axolotl==0.1) (23.1)\n",
            "Requirement already satisfied: dill in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.3.6)\n",
            "Requirement already satisfied: xxhash in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (3.2.0)\n",
            "Requirement already satisfied: multiprocess in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.70.14)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (2023.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.14.1)\n",
            "Requirement already satisfied: responses<0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from evaluate==0.4.0->axolotl==0.1) (0.18.0)\n",
            "Requirement already satisfied: absl-py in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (1.4.0)\n",
            "Requirement already satisfied: nltk in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from rouge-score==0.1.2->axolotl==0.1) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from scikit-learn==1.2.2->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: psutil in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from accelerate->axolotl==0.1) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets->axolotl==0.1) (12.0.0)\n",
            "Requirement already satisfied: aiohttp in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from datasets->axolotl==0.1) (3.8.4)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (2023.5.5)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (0.3.1)\n",
            "Requirement already satisfied: termcolor in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from fire->axolotl==0.1) (2.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (8.1.3)\n",
            "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (3.1.31)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.24.0)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.4.0)\n",
            "Requirement already satisfied: pathtools in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (0.1.2)\n",
            "Requirement already satisfied: setproctitle in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.3.2)\n",
            "Requirement already satisfied: setuptools in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (67.8.0)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (4.6.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.15.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from wandb->axolotl==0.1) (4.23.2)\n",
            "Requirement already satisfied: pyre-extensions==0.0.29 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from xformers->axolotl==0.1) (0.0.29)\n",
            "Requirement already satisfied: typing-inspect in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pyre-extensions==0.0.29->xformers->axolotl==0.1) (0.9.0)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.12)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.99)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.7.101 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.101)\n",
            "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (8.5.0.96)\n",
            "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.10.3.66)\n",
            "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (10.9.0.58)\n",
            "Requirement already satisfied: nvidia-curand-cu11==10.2.10.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (10.2.10.91)\n",
            "Requirement already satisfied: nvidia-cusolver-cu11==11.4.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.4.0.1)\n",
            "Requirement already satisfied: nvidia-cusparse-cu11==11.7.4.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.4.91)\n",
            "Requirement already satisfied: nvidia-nccl-cu11==2.14.3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.14.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu11==11.7.91 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (11.7.91)\n",
            "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.0.0)\n",
            "Requirement already satisfied: wheel in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (0.40.0)\n",
            "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.26.3)\n",
            "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from aiohttp->datasets->axolotl==0.1) (1.3.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (4.0.10)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2023.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from requests->bert-score==0.3.13->axolotl==0.1) (2023.5.7)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (4.39.4)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (9.5.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (5.12.0)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1) (5.0.0)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from importlib-resources>=3.2.0->matplotlib->bert-score==0.3.13->axolotl==0.1) (3.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.3.0)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1) (1.0.0)\n",
            "Installing collected packages: axolotl\n",
            "  Attempting uninstall: axolotl\n",
            "    Found existing installation: axolotl 0.1\n",
            "    Uninstalling axolotl-0.1:\n",
            "      Successfully uninstalled axolotl-0.1\n",
            "  Running setup.py develop for axolotl\n",
            "Successfully installed axolotl-0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adru8lVlzhLi",
        "outputId": "084d6af3-b595-4279-d599-0d49a4460b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu117\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+d755b9d6, d755b9d6, master\n",
            "torch cuda version ............... 11.7\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9eIEonzhLi",
        "outputId": "fc687682-4a63-4400-fb40-e769dd00bd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.1+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1Ahx1QG5xm"
      },
      "source": [
        "### Init Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBBF-lS2HVPG",
        "outputId": "400163fc-7e26-4927-d66a-900e7cc9d060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/axolotl-trained is already a clone of https://huggingface.co/utensil/axolotl-trained. Make sure you pull the latest changes with `repo.git_pull()`.\n"
          ]
        }
      ],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_FP2VeXH0Un",
        "outputId": "3402413c-9d4e-4d69-8c8d-6f9ded6b16c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "falcon-qlora-40b-gsm8k\n"
          ]
        }
      ],
      "source": [
        "!ls /content/axolotl-trained"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYU6HmYduRa"
      },
      "source": [
        "### Reinstall PyTorch with CUDA 11.8 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6v1nl6duRa",
        "outputId": "f0ec996a-0212-4080-a1aa-f647b0f57a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (2.0.1)\n",
            "Collecting torch\n",
            "  Using cached https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp39-cp39-linux_x86_64.whl (2267.3 MB)\n",
            "Requirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (4.6.2)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.3)\n",
            "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\n",
            "torchvision 0.15.1+cu118 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0A0aPWJzhLk",
        "outputId": "a3025d0a-f12e-4c87-836e-e71ba7abb80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1+cu118\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.1+cu118\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPSrhKPHIrS"
      },
      "source": [
        "### Reinstall deepspeed (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PIACM6duRa",
        "outputId": "fad3226f-d0a1-4253-9660-9d22ed1c0bab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu118\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+d755b9d6, d755b9d6, master\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-u6vhWHvt6"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIpqugJOHuhh"
      },
      "outputs": [],
      "source": [
        "# !TORCH_CUDA_ARCH_LIST=\"3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\" DS_BUILD_OPS=1 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" # --global-option=\"bdist_wheel\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tw4OcVHQdK"
      },
      "source": [
        "### Init Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvyUmZdktu0",
        "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mC48y25Lkqa5",
        "outputId": "2757a3c8-3790-4fd3-be39-03be8f533b35"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ],
      "source": [
        "# Try no config\n",
        "# !accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5E8urQEXiL",
        "outputId": "1a7ac7ab-c81d-4e10-ac9e-df10ed3181c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"cpu\",\n",
        "      \"pin_memory\": true\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"cpu\",\n",
        "      \"pin_memory\": true\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true,\n",
        "    \"sub_group_size\": 0,\n",
        "    \"reduce_bucket_size\": \"auto\",\n",
        "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "    \"stage3_param_persistence_threshold\": \"auto\",\n",
        "    \"stage3_max_live_parameters\": 0,\n",
        "    \"stage3_max_reuse_distance\": 0,\n",
        "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "  },\n",
        "  \"bf16\": {\n",
        "    \"enabled\": \"auto\"\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": \"auto\",\n",
        "    \"auto_cast\": false,\n",
        "    \"loss_scale\": 0,\n",
        "    \"initial_scale_power\": 32,\n",
        "    \"loss_scale_window\": 1000,\n",
        "    \"hysteresis\": 2,\n",
        "    \"min_loss_scale\": 1\n",
        "  },\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": \"auto\",\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.999\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": \"auto\"\n",
        "    }\n",
        "  },\n",
        "  \"train_batch_size\": \"auto\",\n",
        "  \"gradient_accumulation_steps\": \"auto\",\n",
        "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "  \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZJw_gCDY7J",
        "outputId": "30a5cd1b-eda1-4384-f9d5-090dd376961d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: tiiuae/falcon-40b\n",
        "base_model_config: tiiuae/falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9sbxRAElkU",
        "outputId": "807b0a21-be35-4592-838d-94c831552053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing scripts/ft.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile scripts/ft.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import logging\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  logging.info('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  logging.info('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  logging.info('setup_trainer_ex before')\n",
        "  logging.info(f'cfg.some_config = {cfg.some_config}')\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  logging.info('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWOnrpzEr-1",
        "outputId": "aae51ccf-bb6b-4f5d-d133-866e0aae9e5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=true\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6CD_zZHUpG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqrhYq-DY7J"
      },
      "source": [
        "## #1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m66yoSyzK7uC"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joQh6mGBm3_J"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwPlElbm73f"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jICMPJuomFsx",
        "scrolled": true,
        "outputId": "241a2589-fb00-414b-b27c-d5fc27e131ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "\n",
            "===================================BUG REPORT======================================================================BUG REPORT===================================\n",
            "\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            "  and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issuesand submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "\n",
            "================================================================================================================================================================\n",
            "\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0a1rckz0/none_qdcsr6ne/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_0a1rckz0/none_qdcsr6ne/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Downloading readme: 100%|██████████████████████| 533/533 [00:00<00:00, 1.74MB/s]\n",
            "Downloading and preparing dataset None/None to /root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n",
            "Downloading readme: 100%|██████████████████████| 533/533 [00:00<00:00, 2.31MB/s]\n",
            "Downloading data files:   0%|                             | 0/1 [00:00<?, ?it/s]\n",
            "Downloading data: 100%|████████████████████| 3.19M/3.19M [00:00<00:00, 33.9MB/s]\u001b[A\n",
            "Downloading data files: 100%|█████████████████████| 1/1 [00:00<00:00,  3.02it/s]\n",
            "Extracting data files: 100%|████████████████████| 1/1 [00:00<00:00, 1537.50it/s]\n",
            "Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 499.56it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 285.44it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.41s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'query_key_value', 'dense_h_to_4h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:22<00:00,  9.19s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'dense', 'query_key_value']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 15:40:36,087] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 15:40:36,087] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 1, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 15:40:52,409] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 15:40:52,409] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 15:40:52,409] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 361, in deepspeed_init\n",
            "    optimizer, lr_scheduler = deepspeed_optim_sched(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 307, in deepspeed_optim_sched\n",
            "    raise ValueError(\n",
            "ValueError: Found `optimizer` configured in the DeepSpeed config, but no `scheduler`. Please configure a scheduler in the DeepSpeed config.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 361, in deepspeed_init\n",
            "    optimizer, lr_scheduler = deepspeed_optim_sched(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 307, in deepspeed_optim_sched\n",
            "    raise ValueError(\n",
            "ValueError: Found `optimizer` configured in the DeepSpeed config, but no `scheduler`. Please configure a scheduler in the DeepSpeed config.\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3268 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3269) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_15:40:53\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 3269)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfs-AbKDY7K"
      },
      "source": [
        "## #2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1u8RtbDY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXasvnIEzhLu",
        "outputId": "bde2eae2-4c54-4be4-8216-56d1e935e7ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "  \"zero_optimization\": {\n",
        "    \"stage\": 3,\n",
        "    \"offload_optimizer\": {\n",
        "      \"device\": \"cpu\",\n",
        "      \"pin_memory\": true\n",
        "    },\n",
        "    \"offload_param\": {\n",
        "      \"device\": \"cpu\",\n",
        "      \"pin_memory\": true\n",
        "    },\n",
        "    \"overlap_comm\": true,\n",
        "    \"contiguous_gradients\": true,\n",
        "    \"sub_group_size\": 0,\n",
        "    \"reduce_bucket_size\": \"auto\",\n",
        "    \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "    \"stage3_param_persistence_threshold\": \"auto\",\n",
        "    \"stage3_max_live_parameters\": 0,\n",
        "    \"stage3_max_reuse_distance\": 0,\n",
        "    \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "  },\n",
        "  \"bf16\": {\n",
        "    \"enabled\": \"auto\"\n",
        "  },\n",
        "  \"fp16\": {\n",
        "    \"enabled\": \"auto\",\n",
        "    \"auto_cast\": false,\n",
        "    \"loss_scale\": 0,\n",
        "    \"initial_scale_power\": 32,\n",
        "    \"loss_scale_window\": 1000,\n",
        "    \"hysteresis\": 2,\n",
        "    \"min_loss_scale\": 1\n",
        "  },\n",
        "  \"optimizer\": {\n",
        "    \"type\": \"AdamW\",\n",
        "    \"params\": {\n",
        "      \"lr\": \"auto\",\n",
        "      \"betas\": [\n",
        "        0.9,\n",
        "        0.999\n",
        "      ],\n",
        "      \"eps\": 1e-8,\n",
        "      \"weight_decay\": \"auto\"\n",
        "    }\n",
        "  },\n",
        "  \"scheduler\": {\n",
        "    \"type\": \"OneCycle\",\n",
        "    \"params\": {\n",
        "      \"cycle_min_lr\": 0.00001,\n",
        "      \"cycle_max_lr\": 0.00003,\n",
        "      \"cycle_first_step_size\": 120\n",
        "    }\n",
        "  },\n",
        "  \"train_batch_size\": \"auto\",\n",
        "  \"gradient_accumulation_steps\": \"auto\",\n",
        "  \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "  \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnz7AF94LJTa"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR8QPcw3DY7K",
        "scrolled": true,
        "outputId": "e1534679-b8e6-4524-a05d-490d66e62a47"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_kya2vidn/none_ehuh7x6s/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_kya2vidn/none_ehuh7x6s/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 625.74it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 507.29it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.37s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense_4h_to_h', 'dense_h_to_4h', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:22<00:00,  9.13s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 15:52:40,847] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 15:52:40,847] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 15:52:40,847] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 15:53:03,427] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 15:53:03,428] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "Parameter Offload: Total persistent parameters: 1982464 in 242 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230613_155317-x0eyur59\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mruby-snowball-23\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/x0eyur59\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7f429e33f580>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7fbda9f0b0b0>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3607 closing signal SIGTERM\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mruby-snowball-23\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/x0eyur59\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 2 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230613_155317-x0eyur59/logs\u001b[0m\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3608) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_15:53:25\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 3608)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbjGJ2GDY7L"
      },
      "source": [
        "## #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6hvCkF0DY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwPMxEdeLLDu"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VoEOAyfzhLv"
      },
      "source": [
        "Config from https://github.com/AetherCortex/Llama-X/blob/main/src/configs/deepspeed_config.json, many thanks to Eric!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SRuxOMXtzhLv",
        "outputId": "5cec93d2-ae47-4386-8b3a-13d87b73d200"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": true,\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": 2e-5,\n",
        "          \"betas\": [\n",
        "            0.9,\n",
        "            0.999\n",
        "          ],\n",
        "          \"eps\": 1e-8,\n",
        "          \"weight_decay\": 0\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XsSrgMvDY7L",
        "scrolled": true,
        "outputId": "f89cb54e-54f7-4fbd-982c-2198b1653e64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_mo0prtyg/none_vbl0bzed/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_mo0prtyg/none_vbl0bzed/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 505.09it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 594.18it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:15<00:00,  8.39s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'query_key_value', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:22<00:00,  9.12s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'dense_4h_to_h', 'query_key_value', 'dense']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:03:22,182] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:03:22,182] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 16:03:22,183] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:03:42,021] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:03:42,021] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 273, in train\n",
            "    trainer = setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
            "  File \"/workspace/axolotl/src/axolotl/utils/trainer.py\", line 264, in setup_trainer\n",
            "    trainer = trainer_cls(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 343, in __init__\n",
            "    self.create_accelerator_and_postprocess()\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3820, in create_accelerator_and_postprocess\n",
            "    self.accelerator = Accelerator(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 286, in __init__\n",
            "    deepspeed_plugin.set_mixed_precision(mixed_precision)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/dataclasses.py\", line 651, in set_mixed_precision\n",
            "    raise ValueError(\n",
            "ValueError: `--mixed_precision` arg cannot be set to `bf16` when `fp16` is set in the DeepSpeed config file.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 273, in train\n",
            "    trainer = setup_trainer(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
            "  File \"/workspace/axolotl/src/axolotl/utils/trainer.py\", line 264, in setup_trainer\n",
            "    trainer = trainer_cls(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 343, in __init__\n",
            "    self.create_accelerator_and_postprocess()\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3820, in create_accelerator_and_postprocess\n",
            "    self.accelerator = Accelerator(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 286, in __init__\n",
            "    deepspeed_plugin.set_mixed_precision(mixed_precision)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/dataclasses.py\", line 651, in set_mixed_precision\n",
            "    raise ValueError(\n",
            "ValueError: `--mixed_precision` arg cannot be set to `bf16` when `fp16` is set in the DeepSpeed config file.\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4007) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2023-06-13_16:03:46\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 4008)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_16:03:46\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 4007)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwP73VWDY7L"
      },
      "source": [
        "## #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFGHaYx2zhLw",
        "outputId": "e2f088ef-98c1-4945-cd73-85fa6062c772"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": 2e-5,\n",
        "          \"betas\": [\n",
        "            0.9,\n",
        "            0.999\n",
        "          ],\n",
        "          \"eps\": 1e-8,\n",
        "          \"weight_decay\": 0\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_tDMBqDY7L",
        "scrolled": true,
        "outputId": "82b9032d-a55b-43bb-cd56-39d1be540d82"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: tiiuae/falcon-40b\n",
            "base_model_config: tiiuae/falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
            "    type: \"alpaca:chat\"\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 1\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFuqSRNLL5_"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VIorWx3DY7M",
        "scrolled": true,
        "outputId": "4fb34f92-1d05-46b2-9ecf-b9297591672a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic__sqryz8h/none_0u0tzqr0/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic__sqryz8h/none_0u0tzqr0/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 388.29it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 474.42it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:17<00:00,  8.67s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'dense', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:20<00:00,  8.92s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'query_key_value', 'dense', 'dense_h_to_4h']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:14:23,791] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:14:23,792] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 16:14:23,792] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:14:33,076] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:14:33,076] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 343, in deepspeed_init\n",
            "    hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 220, in trainer_config_finalize\n",
            "    raise ValueError(\n",
            "ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:\n",
            "- ds optimizer.params.lr=2e-05 vs hf learning_rate=0.0002\n",
            "- ds optimizer.params.weight_decay=0 vs hf weight_decay=0.01\n",
            "The easiest method is to set these DeepSpeed config values to 'auto'.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 343, in deepspeed_init\n",
            "    hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 220, in trainer_config_finalize\n",
            "    raise ValueError(\n",
            "ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:\n",
            "- ds optimizer.params.lr=2e-05 vs hf learning_rate=0.0002\n",
            "- ds optimizer.params.weight_decay=0 vs hf weight_decay=0.01\n",
            "The easiest method is to set these DeepSpeed config values to 'auto'.\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4339) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2023-06-13_16:14:35\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 4340)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_16:14:35\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 4339)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TrgHEoizhLx"
      },
      "source": [
        "## #5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vp20OUjzhLx",
        "outputId": "36dd3ca5-8fd9-4404-94d2-f99836c4212f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": [\n",
        "            0.9,\n",
        "            0.999\n",
        "          ],\n",
        "          \"eps\": 1e-8,\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WpJG6JK2zhLx",
        "outputId": "a39ccb75-1eb9-4e55-9330-3bc1b8114ce5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_f8kivh1t/none__4o5rna4/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_f8kivh1t/none__4o5rna4/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 507.48it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 623.78it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:17<00:00,  8.59s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_h_to_4h', 'dense_4h_to_h', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:18<00:00,  8.69s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_h_to_4h', 'query_key_value', 'dense_4h_to_h']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:23:28,965] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:23:28,966] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 16:23:28,966] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:23:31,051] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:23:31,051] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 361, in deepspeed_init\n",
            "    optimizer, lr_scheduler = deepspeed_optim_sched(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 307, in deepspeed_optim_sched\n",
            "    raise ValueError(\n",
            "ValueError: Found `optimizer` configured in the DeepSpeed config, but no `scheduler`. Please configure a scheduler in the DeepSpeed config.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1617, in _inner_training_loop\n",
            "    self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 361, in deepspeed_init\n",
            "    optimizer, lr_scheduler = deepspeed_optim_sched(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/deepspeed.py\", line 307, in deepspeed_optim_sched\n",
            "    raise ValueError(\n",
            "ValueError: Found `optimizer` configured in the DeepSpeed config, but no `scheduler`. Please configure a scheduler in the DeepSpeed config.\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 0 (pid: 4637) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "[1]:\n",
            "  time      : 2023-06-13_16:23:32\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 4638)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_16:23:32\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 0 (local_rank: 0)\n",
            "  exitcode  : 1 (pid: 4637)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnqGCBezhLy"
      },
      "source": [
        "## #6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAcxGgdvzhLy",
        "outputId": "ef5502ef-3e41-4096-c69f-0721cfab7919"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": [\n",
        "            0.9,\n",
        "            0.999\n",
        "          ],\n",
        "          \"eps\": 1e-8,\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "        \"type\": \"OneCycle\",\n",
        "        \"params\": {\n",
        "          \"cycle_min_lr\": 0.00001,\n",
        "          \"cycle_max_lr\": 0.00003,\n",
        "          \"cycle_first_step_size\": 120\n",
        "        }\n",
        "      },\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aEj2fAgzhLz",
        "outputId": "8b5d1764-bead-45ac-d20c-3cfc37079572"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ew53r__m/none_nkzpti8m/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_ew53r__m/none_nkzpti8m/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 525.73it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 602.28it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:23<00:00,  9.31s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'dense', 'query_key_value']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:30<00:00, 10.01s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:41:32,989] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:41:32,989] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:41:38,423] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:41:38,423] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 16:41:38,423] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "Parameter Offload: Total persistent parameters: 1982464 in 242 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230613_164153-c7tv5uak\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mclear-fire-24\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/c7tv5uak\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7fc9bc23df20>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7f1f880190b0>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mclear-fire-24\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/c7tv5uak\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230613_164153-c7tv5uak/logs\u001b[0m\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 4979 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 4980) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_16:42:01\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 4980)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivJrMd8zhLz"
      },
      "source": [
        "## #7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99raQX83zhLz",
        "outputId": "8a54e193-599e-4a1f-a767-810ed4ead0a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# 1b: tiiuae/falcon-rw-1b\n",
            "# 7b: tiiuae/falcon-7b\n",
            "# 40b: tiiuae/falcon-40b\n",
            "base_model: tiiuae/falcon-40b\n",
            "base_model_config: tiiuae/falcon-40b\n",
            "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
            "trust_remote_code: true\n",
            "model_type: AutoModelForCausalLM\n",
            "tokenizer_type: AutoTokenizer\n",
            "load_in_8bit: false\n",
            "# enable 4bit for QLoRA\n",
            "load_in_4bit: true\n",
            "gptq: false\n",
            "strict: false\n",
            "\n",
            "push_dataset_to_hub: utensil\n",
            "hf_use_auth_token: true\n",
            "\n",
            "datasets:\n",
            "  - path: QingyiSi/Alpaca-CoT\n",
            "    data_files:\n",
            "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
            "    type: \"alpaca:chat\"\n",
            "\n",
            "dataset_prepared_path: last_run_prepared\n",
            "val_set_size: 0.01\n",
            "# enable QLoRA\n",
            "adapter: qlora\n",
            "lora_model_dir:\n",
            "sequence_len: 2048\n",
            "max_packed_sequence_len:\n",
            "\n",
            "# hyperparameters from QLoRA paper Appendix B.2\n",
            "# \"We find hyperparameters to be largely robust across datasets\"\n",
            "lora_r: 64\n",
            "lora_alpha: 16\n",
            "# 0.1 for models up to 13B\n",
            "# 0.05 for 33B and 65B models\n",
            "lora_dropout: 0.05\n",
            "# add LoRA modules on all linear layers of the base model\n",
            "lora_target_modules:\n",
            "lora_target_linear: true\n",
            "lora_fan_in_fan_out:\n",
            "\n",
            "wandb_project: falcon-qlora\n",
            "wandb_watch:\n",
            "wandb_run_id:\n",
            "wandb_log_model:\n",
            "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "\n",
            "# QLoRA paper Table 9\n",
            "# - 16 for 7b & 13b\n",
            "# - 32 for 33b, 64 for 64b\n",
            "# Max size tested on A6000\n",
            "# - 7b: 40\n",
            "# - 40b: 4\n",
            "# decrease if OOM, increase for max VRAM utilization\n",
            "micro_batch_size: 1\n",
            "gradient_accumulation_steps: 1\n",
            "num_epochs: 3\n",
            "# Optimizer for QLoRA\n",
            "# optimizer: paged_adamw_32bit\n",
            "torchdistx_path:\n",
            "# lr_scheduler: cosine\n",
            "# QLoRA paper Table 9\n",
            "# - 2e-4 for 7b & 13b\n",
            "# - 1e-4 for 33b & 64b\n",
            "learning_rate: 0.0002\n",
            "train_on_inputs: false\n",
            "group_by_length: false\n",
            "bf16: true\n",
            "fp16: false\n",
            "tf32: true\n",
            "gradient_checkpointing: true\n",
            "# stop training after this many evaluation losses have increased in a row\n",
            "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
            "early_stopping_patience: 3\n",
            "resume_from_checkpoint:\n",
            "auto_resume_from_checkpoints: true\n",
            "local_rank:\n",
            "logging_steps: 1\n",
            "xformers_attention: true\n",
            "flash_attention:\n",
            "gptq_groupsize:\n",
            "gptq_model_v1:\n",
            "warmup_steps: 10\n",
            "eval_steps: 5\n",
            "save_steps: 10\n",
            "debug:\n",
            "deepspeed:\n",
            "weight_decay: 0.01\n",
            "fsdp:\n",
            "fsdp_config:\n",
            "special_tokens:\n",
            "  pad_token: \"<|endoftext|>\"\n",
            "  bos_token: \">>ABSTRACT<<\"\n",
            "  eos_token: \"<|endoftext|>\"\n"
          ]
        }
      ],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubZ7fVBVzhLz",
        "outputId": "54c742ea-364a-4e27-c3f3-57ec767cc9f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_pgvj6xms/none_9n33fanf/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_pgvj6xms/none_9n33fanf/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 529.45it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 554.80it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:25<00:00,  9.55s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:26<00:00,  9.65s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_h_to_4h', 'query_key_value', 'dense_4h_to_h', 'dense']\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'query_key_value', 'dense_h_to_4h', 'dense']\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:56:14,528] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:56:14,529] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444334080 || all params: 21363310592 || trainable%: 2.0798933671188187\n",
            "[2023-06-13 16:56:20,513] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-13 16:56:20,513] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-13 16:56:20,513] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "Parameter Offload: Total persistent parameters: 1982464 in 242 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.15.4 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.3\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230613_165635-07ucelhf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mstoic-water-25\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/07ucelhf\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "Traceback (most recent call last):\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 329, in <module>\n",
            "    fire.Fire(train)\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 316, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1537, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1811, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2643, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7f51e249af20>\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1962, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7f08d40490b0>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33mstoic-water-25\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/07ucelhf\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20230613_165635-07ucelhf/logs\u001b[0m\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 5389 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 5390) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-13_16:56:45\n",
            "  host      : 6ed654b3d76c\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 5390)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuaiP0pUzhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzy_ZDY4zhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWDQTL_vGSL"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqeHYEGDY7Q"
      },
      "source": [
        "### Upload checkpoints to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkekyF8DKerI"
      },
      "outputs": [],
      "source": [
        "%cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8SpVYpNY1y"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_79rGoKkA_"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK8IWmrtKvlJ"
      },
      "outputs": [],
      "source": [
        "# ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyQDH-YDY7R"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5TnMCeLDY7M"
      },
      "source": [
        "## Below are ad hoc cells handling issues during training\n",
        "\n",
        "current out dir:\n",
        "\n",
        "```\n",
        "/content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCBx0UZDY7M"
      },
      "source": [
        "### Force release VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0VdzsqSDY7M"
      },
      "outputs": [],
      "source": [
        "# First interupt the kernel, wait a few seconds then run this to kill finetune to release VRAM\n",
        "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7StF_PDY7N"
      },
      "source": [
        "### Clean the finetuned model and all checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4c-7KHDY7N"
      },
      "outputs": [],
      "source": [
        "# Only run this to start over\n",
        "!rm -rf /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6s4xi9jDY7N"
      },
      "source": [
        "### Zip the prepared dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-aBbwEDY7N"
      },
      "outputs": [],
      "source": [
        "!apt install zip\n",
        "!zip -r last_run_prepared.zip -xi last_run_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFC20jMDY7N"
      },
      "source": [
        "### Monitoring GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgPaXew2DY7O"
      },
      "outputs": [],
      "source": [
        "# Run this in a seperate terminal\n",
        "!nvitop -m full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFlPnbXDY7O"
      },
      "source": [
        "### Fix DISK FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5CWZu7DY7O",
        "outputId": "a59d29d3-afca-42d5-d185-c54bf5bfe864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/\n"
          ]
        }
      ],
      "source": [
        "%cd /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqYxcpZDY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h|grep G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIJxrv1DY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h /root/.local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25FTebpHDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/Trash/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K4d-2ocDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/wandb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOoZLugDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17SexQnqDY7P"
      },
      "source": [
        "### Check who is using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE8A5crNDY7P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install lsof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNCASirDY7Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!lsof /dev/nvidia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZv_qjgDY7Y"
      },
      "source": [
        "### A new bash without tmux etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkJAXwTDY7Z"
      },
      "outputs": [],
      "source": [
        "!bash --norc --noprofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3s54THDY7Z"
      },
      "source": [
        "### Clean up all checkpoints but last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdO1EybEDY7Z"
      },
      "outputs": [],
      "source": [
        "cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}