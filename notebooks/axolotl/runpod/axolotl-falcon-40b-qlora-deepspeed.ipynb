{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/notebooks/axolotl/runpod/axolotl-falcon-40b-qlora-deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpbQuaxDY7H"
      },
      "source": [
        "<!-- https://jupyterlab.readthedocs.io/en/stable/user/commands.html#commands-list -->\n",
        "<button data-commandLinker-command=\"apputils:change-theme\" data-commandlinker-args='{\"theme\": \"JupyterLab Dark\"}' href=\"#\">Dark theme</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/\"}' href=\"#\">llm-playground</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/\"}' href=\"#\">axolotl</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/notebooks/axolotl/runpod\"}' href=\"#\">Runpod notebooks</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples\"}' href=\"#\">axolotl configs</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/llm-playground/storage\"}' href=\"#\">Storage</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/axolotl-trained\"}' href=\"#\">axolotl-trained</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples/falcon/config-40b-qlora.yml\"}' href=\"#\">Edit qlora config</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/ds_config.json\"}' href=\"#\">Edit ds config</button>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yoSiRvDY7G"
      },
      "source": [
        "# Finetuning falcon-40b\n",
        "\n",
        "- Axolotl+QLoRA\n",
        "- minotaur datasets\n",
        "- deepspeed ZeRO 3 8xGPU\n",
        "\n",
        "Entry script:\n",
        "\n",
        "https://github.com/utensil/llm-playground/blob/main/scripts/entry/ax_lite.sh\n",
        "\n",
        "Mount volume disk to `/content`\n",
        "\n",
        "Save to:\n",
        "\n",
        "- repo: `utensil/llm-playground`\n",
        "- path: `notebooks/axolotl/runpod/axolotl-falcon-40b-qlora-deepspeed.ipynb`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZd-cf70HM2n"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrtaRkauKLL"
      },
      "source": [
        "### Set HF Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIyW4yWuHvd"
      },
      "outputs": [],
      "source": [
        "# %env HF_DATASETS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOfHUYQduRX"
      },
      "outputs": [],
      "source": [
        "#%env TRANSFORMERS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR9oblHTduRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaKgWKnduRY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJMsnPnOduRY"
      },
      "outputs": [],
      "source": [
        "!ln -s /content/cache /root/.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FkZ6RzFyiAue"
      },
      "source": [
        "### Speed up model download"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huZ5eZUUgJ76"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y aria2\n",
        "!git lfs install\n",
        "!pip install requests huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dqzd-Ul4iMPf"
      },
      "outputs": [],
      "source": [
        "#%cd /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmJ7tmaIifTR"
      },
      "outputs": [],
      "source": [
        "#!git clone https://github.com/utensil/llm-playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29dpWyRujiSZ"
      },
      "outputs": [],
      "source": [
        "%cd /content/llm-playground"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c8nAsJ4ggJ77",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!python /content/llm-playground/helper/download-model.py tiiuae/falcon-40b\n",
        "# /content/llm-playground/models/tiiuae_falcon-40b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydCdRNI9gJ78"
      },
      "outputs": [],
      "source": [
        "# Downloading shards: 100%|████████████████████████| 9/9 [19:19<00:00, 128.85s/it]\n",
        "# Loading checkpoint shards: 100%|██████████████████| 9/9 [01:30<00:00, 10.08s/it]\n",
        "# (OK):download completed.\n",
        "# 100%|████████████████████████████████████████████████████████████████████| 18/18 [01:44<00:00,  5.81s/it]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqQyPp5HbAm"
      },
      "source": [
        "### HF Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6wC1lP23B-4"
      },
      "outputs": [],
      "source": [
        "# For axolotl push_dataset_to_hub\n",
        "import os\n",
        "from huggingface_hub import notebook_login, login\n",
        "# Colab:\n",
        "# notebook_login()\n",
        "# RunPod:\n",
        "login(os.environ.get(\"HUGGINGFACE_TOKEN\"), add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AigYR_DY7X"
      },
      "source": [
        "### Update axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQYJdcd7DY7X"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn5aNACDY7X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl axolotl-update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdY5tXtYDY7X"
      },
      "outputs": [],
      "source": [
        "!cp -r axolotl-update/* axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsKmIUmDY7Y"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljX3Yir2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adru8lVlzhLi"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaorAF2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9eIEonzhLi"
      },
      "outputs": [],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1Ahx1QG5xm"
      },
      "source": [
        "### Init Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBBF-lS2HVPG"
      },
      "outputs": [],
      "source": [
        "!python /content/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_FP2VeXH0Un"
      },
      "outputs": [],
      "source": [
        "!ls /content/axolotl-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G86XUuQ1gJ8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOXz9ZCJgJ8J"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYU6HmYduRa"
      },
      "source": [
        "### Reinstall PyTorch with CUDA 11.8 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6v1nl6duRa"
      },
      "outputs": [],
      "source": [
        "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0A0aPWJzhLk"
      },
      "outputs": [],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPSrhKPHIrS"
      },
      "source": [
        "### Reinstall deepspeed (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PIACM6duRa"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-u6vhWHvt6"
      },
      "outputs": [],
      "source": [
        "!yes|pip uninstall deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIpqugJOHuhh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!TORCH_CUDA_ARCH_LIST=\"3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\" DS_BUILD_OPS=1 DS_BUILD_SPARSE_ATTN=0 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" # --global-option=\"bdist_wheel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8JzOr94dDm1"
      },
      "outputs": [],
      "source": [
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcKtKd_MdDm1"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tw4OcVHQdK"
      },
      "source": [
        "### Init Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJvyUmZdktu0"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC48y25Lkqa5"
      },
      "outputs": [],
      "source": [
        "# Try no config\n",
        "# !accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5E8urQEXiL"
      },
      "outputs": [],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": \"auto\",\n",
        "          \"eps\": \"auto\",\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "      \"type\": \"WarmupDecayLR\",\n",
        "      \"params\": {\n",
        "        \"total_num_steps\": \"auto\",\n",
        "        \"warmup_min_lr\": \"auto\",\n",
        "        \"warmup_max_lr\": \"auto\",\n",
        "        \"warmup_num_steps\": \"auto\"\n",
        "       }\n",
        "    },\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZJw_gCDY7J",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "# optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "# lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWOnrpzEr-1"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9sbxRAElkU"
      },
      "outputs": [],
      "source": [
        "%%writefile scripts/ft.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import logging\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  logging.info('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  logging.info('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  logging.info('setup_trainer_ex before')\n",
        "  logging.info(f'cfg.some_config = {cfg.some_config}')\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  logging.info('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6CD_zZHUpG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqrhYq-DY7J"
      },
      "source": [
        "## #1 DS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsRPtribgJ8O"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_oAE22wgJ8Q"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m66yoSyzK7uC"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joQh6mGBm3_J"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwPlElbm73f"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jICMPJuomFsx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfs-AbKDY7K"
      },
      "source": [
        "## #2 ACC mbs 32 2xA100 minotaur"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gNWb1VKgJ8S"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "base_model_config: /content/llm-playground/models/tiiuae_falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hf/ARC-Challenge.jsonl\n",
        "      - hf/ARC-Easy.jsonl\n",
        "      - hf/riddle_sense.jsonl\n",
        "    type: explainchoice:chat\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hf/gsm8k.jsonl\n",
        "      - hf/winogrande.jsonl\n",
        "    type: alpaca_chat.load_qa\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/n_task.jsonl\n",
        "      - custom/misconceptions.jsonl\n",
        "      - custom/context_insensitivity.jsonl\n",
        "    type: alpaca_chat\n",
        "  - path: camel-ai/math\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/biology\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/physics\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: camel-ai/chemistry\n",
        "    type: alpaca_chat.load_camel_ai\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/in_context_qa.jsonl\n",
        "    type: context_qa\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/in_context_qa.jsonl\n",
        "    type: context_qa.load_404\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/jokes_explained_500up.jsonl\n",
        "    type: sharegpt_jokes\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - custom/classify-self-chat.sharegpt.jsonl\n",
        "      - custom/coding-self-chat.sharegpt.jsonl\n",
        "      - custom/prose-gpt4.sharegpt.jsonl\n",
        "      - custom/prose-rewrite-gpt4.sharegpt.jsonl\n",
        "    type: sharegpt_simple.load_role\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - openai/tldr.jsonl\n",
        "    type: summarizetldr:chat\n",
        "  - path: winglian/evals\n",
        "    data_files:\n",
        "      - hellaswag/hellaswag.jsonl\n",
        "    type: explainchoice:chat\n",
        "  - path: metaeval/ScienceQA_text_only\n",
        "    type: concisechoice:chat\n",
        "  - path: teknium/GPT4-LLM-Cleaned\n",
        "    type: alpaca_chat\n",
        "  - path: teknium/GPTeacher-General-Instruct\n",
        "    data_files: gpt4-instruct-similarity-0.6-dataset.json\n",
        "    type: gpteacher:chat\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/aqua_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/creak_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/ecqa_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/esnli_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/qasc_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/qed_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/sensemaking_train.json\n",
        "      - Chain-of-Thought/formatted_cot_data/strategyqa_train.json\n",
        "      - GPTeacher/Roleplay/formatted_roleplay-similarity_0.6-instruct-dataset.json\n",
        "    type: alpaca_chat\n",
        "  - path: ehartford/WizardLM_alpaca_evol_instruct_70k_unfiltered\n",
        "    type: alpaca_chat\n",
        "  - path: ehartford/wizard_vicuna_70k_unfiltered\n",
        "    type: sharegpt:chat\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 32\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ],
      "metadata": {
        "id": "wcOH4Ftmjg5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1u8RtbDY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EeOND66dDnF"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnz7AF94LJTa"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR8QPcw3DY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbjGJ2GDY7L"
      },
      "source": [
        "## #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "00vedG_seEDL"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y0ltGgTqeEDM"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6hvCkF0DY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwPMxEdeLLDu"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XsSrgMvDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwP73VWDY7L"
      },
      "source": [
        "## #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-azUQO0dDnH"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_tDMBqDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFuqSRNLL5_"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VIorWx3DY7M",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TrgHEoizhLx"
      },
      "source": [
        "## #5 mbs 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NdJGjFTdDnI"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcOtbqlCdDnI"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJG6JK2zhLx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnqGCBezhLy"
      },
      "source": [
        "## #6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHaDkOkvdDnI"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aEj2fAgzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivJrMd8zhLz"
      },
      "source": [
        "## #7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99raQX83zhLz"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubZ7fVBVzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuaiP0pUzhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzy_ZDY4zhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWDQTL_vGSL"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqeHYEGDY7Q"
      },
      "source": [
        "### Upload checkpoints to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkekyF8DKerI"
      },
      "outputs": [],
      "source": [
        "%cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8SpVYpNY1y"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_79rGoKkA_"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK8IWmrtKvlJ"
      },
      "outputs": [],
      "source": [
        "# ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyQDH-YDY7R"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5TnMCeLDY7M"
      },
      "source": [
        "## Below are ad hoc cells handling issues during training\n",
        "\n",
        "current out dir:\n",
        "\n",
        "```\n",
        "/content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCBx0UZDY7M"
      },
      "source": [
        "### Force release VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0VdzsqSDY7M"
      },
      "outputs": [],
      "source": [
        "# First interupt the kernel, wait a few seconds then run this to kill finetune to release VRAM\n",
        "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7StF_PDY7N"
      },
      "source": [
        "### Clean the finetuned model and all checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4c-7KHDY7N"
      },
      "outputs": [],
      "source": [
        "# Only run this to start over\n",
        "!rm -rf /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6s4xi9jDY7N"
      },
      "source": [
        "### Zip the prepared dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-aBbwEDY7N"
      },
      "outputs": [],
      "source": [
        "!apt install zip\n",
        "!zip -r last_run_prepared.zip -xi last_run_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFC20jMDY7N"
      },
      "source": [
        "### Monitoring GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgPaXew2DY7O"
      },
      "outputs": [],
      "source": [
        "# Run this in a seperate terminal\n",
        "!nvitop -m full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFlPnbXDY7O"
      },
      "source": [
        "### Fix DISK FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5CWZu7DY7O"
      },
      "outputs": [],
      "source": [
        "%cd /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqYxcpZDY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h|grep G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIJxrv1DY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h /root/.local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25FTebpHDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/Trash/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K4d-2ocDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/wandb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOoZLugDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17SexQnqDY7P"
      },
      "source": [
        "### Check who is using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE8A5crNDY7P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install lsof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNCASirDY7Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!lsof /dev/nvidia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZv_qjgDY7Y"
      },
      "source": [
        "### A new bash without tmux etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkJAXwTDY7Z"
      },
      "outputs": [],
      "source": [
        "!bash --norc --noprofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3s54THDY7Z"
      },
      "source": [
        "### Clean up all checkpoints but last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4fSoQ8hteEDX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJtsGJ0oeEDX"
      },
      "outputs": [],
      "source": [
        "!cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk '{print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdO1EybEDY7Z"
      },
      "outputs": [],
      "source": [
        "!cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk 'NR > 0 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyk_zXK5eEDX"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}