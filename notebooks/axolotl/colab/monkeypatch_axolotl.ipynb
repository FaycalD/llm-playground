{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP37u2pYKBkYb5KsIgTS4zI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/notebooks/axolotl/colab/monkeypatch_axolotl.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5s8qcDVhY5q",
        "outputId": "829b0f2d-1dbc-4d77-a42e-76d02dc4c45b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'axolotl'...\n",
            "remote: Enumerating objects: 2614, done.\u001b[K\n",
            "remote: Counting objects: 100% (890/890), done.\u001b[K\n",
            "remote: Compressing objects: 100% (195/195), done.\u001b[K\n",
            "remote: Total 2614 (delta 791), reused 721 (delta 685), pack-reused 1724\u001b[K\n",
            "Receiving objects: 100% (2614/2614), 1.33 MiB | 9.54 MiB/s, done.\n",
            "Resolving deltas: 100% (1627/1627), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd axolotl"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpUiC4lnhyv6",
        "outputId": "f47eb7ab-4c30-485c-b131-077ee33a2681"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/axolotl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -e ."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJvpGvhhh0nD",
        "outputId": "518670ba-477d-4093-848c-0f1f89b1da8f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Obtaining file:///content/axolotl\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers@ git+https://github.com/huggingface/transformers.git (from axolotl==0.1)\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-install-esok7o5q/transformers_6129b103593a488c99b68220b5a40d08\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-install-esok7o5q/transformers_6129b103593a488c99b68220b5a40d08\n",
            "  Resolved https://github.com/huggingface/transformers.git to commit 8f093fb799246f7dd9104ff44728da0c53a9f67a\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting bitsandbytes>=0.39.0 (from axolotl==0.1)\n",
            "  Downloading bitsandbytes-0.39.0-py3-none-any.whl (92.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/92.2 MB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from axolotl==0.1)\n",
            "  Downloading accelerate-0.20.3-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.6/227.6 kB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting addict (from axolotl==0.1)\n",
            "  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting fire (from axolotl==0.1)\n",
            "  Downloading fire-0.5.0.tar.gz (88 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m88.3/88.3 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: PyYAML==6.0 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.1) (6.0)\n",
            "Collecting datasets (from axolotl==0.1)\n",
            "  Downloading datasets-2.12.0-py3-none-any.whl (474 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.6/474.6 kB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from axolotl==0.1)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m80.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wandb (from axolotl==0.1)\n",
            "  Downloading wandb-0.15.4-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m101.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops (from axolotl==0.1)\n",
            "  Downloading einops-0.6.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers (from axolotl==0.1)\n",
            "  Downloading xformers-0.0.20-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score==0.3.13 (from axolotl==0.1)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.0 (from axolotl==0.1)\n",
            "  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score==0.1.2 (from axolotl==0.1)\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from axolotl==0.1) (1.10.1)\n",
            "Requirement already satisfied: scikit-learn==1.2.2 in /usr/local/lib/python3.10/dist-packages (from axolotl==0.1) (1.2.2)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (2.0.1+cu118)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (4.65.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score==0.3.13->axolotl==0.1) (23.1)\n",
            "Collecting dill (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xxhash (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate==0.4.0->axolotl==0.1) (2023.4.0)\n",
            "Collecting huggingface-hub>=0.7.0 (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting responses<0.19 (from evaluate==0.4.0->axolotl==0.1)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score==0.1.2->axolotl==0.1) (1.16.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.1) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.2.2->axolotl==0.1) (3.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->axolotl==0.1) (5.9.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->axolotl==0.1) (9.0.0)\n",
            "Collecting aiohttp (from datasets->axolotl==0.1)\n",
            "  Downloading aiohttp-3.8.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (3.12.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1) (2022.10.31)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers@ git+https://github.com/huggingface/transformers.git->axolotl==0.1)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from fire->axolotl==0.1) (2.3.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (8.1.3)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb->axolotl==0.1)\n",
            "  Downloading GitPython-3.1.31-py3-none-any.whl (184 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb->axolotl==0.1)\n",
            "  Downloading sentry_sdk-1.25.1-py2.py3-none-any.whl (206 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m206.7/206.7 kB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb->axolotl==0.1)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb->axolotl==0.1)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb->axolotl==0.1)\n",
            "  Downloading setproctitle-1.3.2-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb->axolotl==0.1) (3.20.3)\n",
            "Collecting pyre-extensions==0.0.29 (from xformers->axolotl==0.1)\n",
            "  Downloading pyre_extensions-0.0.29-py3-none-any.whl (12 kB)\n",
            "Collecting typing-inspect (from pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pyre-extensions==0.0.29->xformers->axolotl==0.1) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (16.0.5)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->axolotl==0.1) (2.0.12)\n",
            "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets->axolotl==0.1)\n",
            "  Downloading multidict-6.0.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3 (from aiohttp->datasets->axolotl==0.1)\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting yarl<2.0,>=1.0 (from aiohttp->datasets->axolotl==0.1)\n",
            "  Downloading yarl-1.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting frozenlist>=1.1.1 (from aiohttp->datasets->axolotl==0.1)\n",
            "  Downloading frozenlist-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting aiosignal>=1.1.2 (from aiohttp->datasets->axolotl==0.1)\n",
            "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
            "  Downloading gitdb-4.0.10-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score==0.3.13->axolotl==0.1) (2022.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score==0.3.13->axolotl==0.1) (3.4)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (0.11.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (4.39.3)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (1.4.4)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (8.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score==0.3.13->axolotl==0.1) (3.0.9)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb->axolotl==0.1)\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score==0.3.13->axolotl==0.1) (1.3.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect->pyre-extensions==0.0.29->xformers->axolotl==0.1)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Building wheels for collected packages: rouge-score, transformers, fire, pathtools\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=4f4dfa3de237b24376b3e253c0284d106034f2086de524eff974fec08528e009\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.31.0.dev0-py3-none-any.whl size=7170473 sha256=164dfc7017620d58a037edc7544077dffe3b5500ed73f73567e8261748a2c55c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-9jc0rsg4/wheels/e7/9c/5b/e1a9c8007c343041e61cc484433d512ea9274272e3fcbe7c16\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116932 sha256=2f59363ebbfbcbc1f0f4d0d6ebb5b0d101808f66163050b92f37574e1660821a\n",
            "  Stored in directory: /root/.cache/pip/wheels/90/d4/f7/9404e5db0116bd4d43e5666eaa3e70ab53723e1e3ea40c9a95\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=dd6267f4f59e97d4e37762c1661146984b6d3dca2cd262a7ede8b09af42b0709\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built rouge-score transformers fire pathtools\n",
            "Installing collected packages: tokenizers, sentencepiece, safetensors, pathtools, bitsandbytes, addict, xxhash, smmap, setproctitle, sentry-sdk, mypy-extensions, multidict, frozenlist, fire, einops, docker-pycreds, dill, async-timeout, yarl, typing-inspect, rouge-score, responses, multiprocess, huggingface-hub, gitdb, aiosignal, transformers, pyre-extensions, GitPython, aiohttp, wandb, datasets, evaluate, xformers, bert-score, accelerate, axolotl\n",
            "  Running setup.py develop for axolotl\n",
            "Successfully installed GitPython-3.1.31 accelerate-0.20.3 addict-2.4.0 aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 axolotl-0.1 bert-score-0.3.13 bitsandbytes-0.39.0 datasets-2.12.0 dill-0.3.6 docker-pycreds-0.4.0 einops-0.6.1 evaluate-0.4.0 fire-0.5.0 frozenlist-1.3.3 gitdb-4.0.10 huggingface-hub-0.15.1 multidict-6.0.4 multiprocess-0.70.14 mypy-extensions-1.0.0 pathtools-0.1.2 pyre-extensions-0.0.29 responses-0.18.0 rouge-score-0.1.2 safetensors-0.3.1 sentencepiece-0.1.99 sentry-sdk-1.25.1 setproctitle-1.3.2 smmap-5.0.0 tokenizers-0.13.3 transformers-4.31.0.dev0 typing-inspect-0.9.0 wandb-0.15.4 xformers-0.0.20 xxhash-3.2.0 yarl-1.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U git+https://github.com/huggingface/peft.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFHNoXPYh6cL",
        "outputId": "c3c7d2c2-c8ea-44f0-d6e5-d94979ce3fe9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/peft.git\n",
            "  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-vajurb24\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-vajurb24\n",
            "  Resolved https://github.com/huggingface/peft.git to commit 189a6b8e357ecda05ccde13999e4c35759596a67\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (23.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (6.0)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (2.0.1+cu118)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (4.31.0.dev0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (0.20.3)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft==0.4.0.dev0) (0.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft==0.4.0.dev0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.13.0->peft==0.4.0.dev0) (16.0.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.15.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->peft==0.4.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers->peft==0.4.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft==0.4.0.dev0) (2.1.2)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->peft==0.4.0.dev0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft==0.4.0.dev0) (1.3.0)\n",
            "Building wheels for collected packages: peft\n",
            "  Building wheel for peft (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for peft: filename=peft-0.4.0.dev0-py3-none-any.whl size=59308 sha256=414b6cadbcf4223f6eb472b8145ff607431f62b70e86d9b45b7ecced17ddd36c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-5vh9a2_r/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n",
            "Successfully built peft\n",
            "Installing collected packages: peft\n",
            "Successfully installed peft-0.4.0.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate config --config_file configs/accelerate/default_config.yaml default"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "az7rO2_KA1aC",
        "outputId": "f5150d8a-f6c7-41b4-d55a-aa8d45da447f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-10 11:16:08.926713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "accelerate configuration saved at /root/.cache/huggingface/accelerate/default_config.yaml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile examples/falcon/config-1b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: tiiuae/falcon-rw-1b\n",
        "base_model_config: tiiuae/falcon-rw-1b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-rw-1b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "push_dataset_to_hub:\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project:\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: ./qlora-out\n",
        "\n",
        "some_config: some_string\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 2\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: false\n",
        "fp16: false\n",
        "tf32: false\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.000001\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXa3WM17EuMf",
        "outputId": "6a8e4ebd-883b-4c7c-deea-dd429001f2f5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting examples/falcon/config-1b-qlora.yml\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile scripts/ft.py\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  print('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  print('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  print('setup_trainer_ex before')\n",
        "  print(cfg.some_config)\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  print('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhT39A3TEFcZ",
        "outputId": "534629c9-cd84-4c66-9084-5b39be07e3d1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting scripts/ft.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!accelerate launch scripts/ft.py examples/falcon/config-1b-qlora.yml"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IwEcnbUv_I__",
        "outputId": "08013dd4-4037-4ed6-d633-3fd42baf0cfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-06-10 11:45:48.232175: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/lib64-nvidia did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/sys/fs/cgroup/memory.events /var/colab/cgroup/jupyter-children/memory.events')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//172.28.0.1'), PosixPath('http'), PosixPath('8013')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('--logtostderr --listen_host=172.28.0.12 --target_host=172.28.0.12 --tunnel_background_save_url=https'), PosixPath('//colab.research.google.com/tun/m/cc48301118ce562b961b3c22d803539adc1e0c19/gpu-t4-s-tccs3c5dv19r --tunnel_background_save_delay=10s --tunnel_periodic_background_save_frequency=30m0s --enable_output_coalescing=true --output_coalescing_required=true')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/env/python')}\n",
            "  warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//ipykernel.pylab.backend_inline')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 7.5\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /usr/local/lib/python3.10/dist-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "2023-06-10 11:45:56.336288: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "train_ex before\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-rw-1b\n",
            "Using pad_token, but it is not set yet.\n",
            "INFO:root:Loading prepared dataset from disk at last_run_prepared/0ecc5b78e3ce4254b22e749b093712b4...\n",
            "INFO:root:Prepared dataset loaded from disk...\n",
            "INFO:root:loading model and peft_config...\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense_h_to_4h', 'dense_4h_to_h', 'dense']\n",
            "trainable params: 50,331,648 || all params: 757,911,552 || trainable%: 6.6408339953630895\n",
            "setup_trainer_ex before\n",
            "some_string\n",
            "Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n",
            "setup_trainer_ex after\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to ./qlora-out\n",
            "INFO:root:Starting trainer...\n",
            "  0% 0/11097 [00:00<?, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.858, 'learning_rate': 2e-05, 'epoch': 0.0}\n",
            "{'loss': 1.6529, 'learning_rate': 4e-05, 'epoch': 0.0}\n",
            "{'loss': 1.6421, 'learning_rate': 6e-05, 'epoch': 0.0}\n",
            "{'loss': 1.7999, 'learning_rate': 8e-05, 'epoch': 0.0}\n",
            "{'loss': 1.7844, 'learning_rate': 0.0001, 'epoch': 0.0}\n",
            "  0% 5/11097 [00:07<4:34:41,  1.49s/it]\n",
            "  0% 0/75 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/75 [00:00<00:05, 13.53it/s]\u001b[A\n",
            "  5% 4/75 [00:00<00:10,  6.96it/s]\u001b[A\n",
            "  7% 5/75 [00:00<00:12,  5.64it/s]\u001b[A\n",
            "  8% 6/75 [00:00<00:12,  5.69it/s]\u001b[A\n",
            "  9% 7/75 [00:01<00:13,  4.88it/s]\u001b[A\n",
            " 11% 8/75 [00:01<00:14,  4.59it/s]\u001b[A\n",
            " 12% 9/75 [00:01<00:12,  5.14it/s]\u001b[A\n",
            " 13% 10/75 [00:01<00:13,  4.93it/s]\u001b[A\n",
            " 15% 11/75 [00:02<00:12,  5.10it/s]\u001b[A\n",
            " 16% 12/75 [00:02<00:12,  4.98it/s]\u001b[A\n",
            " 17% 13/75 [00:02<00:12,  4.87it/s]\u001b[A\n",
            " 19% 14/75 [00:02<00:12,  4.87it/s]\u001b[A\n",
            " 20% 15/75 [00:02<00:12,  4.85it/s]\u001b[A\n",
            " 21% 16/75 [00:03<00:12,  4.56it/s]\u001b[A\n",
            " 23% 17/75 [00:03<00:12,  4.69it/s]\u001b[A\n",
            " 24% 18/75 [00:03<00:11,  4.95it/s]\u001b[A\n",
            " 25% 19/75 [00:03<00:11,  4.70it/s]\u001b[A\n",
            " 27% 20/75 [00:03<00:12,  4.53it/s]\u001b[A\n",
            " 28% 21/75 [00:04<00:11,  4.55it/s]\u001b[A\n",
            " 29% 22/75 [00:04<00:11,  4.46it/s]\u001b[A\n",
            " 31% 23/75 [00:04<00:12,  4.29it/s]\u001b[A\n",
            " 32% 24/75 [00:04<00:11,  4.40it/s]\u001b[A\n",
            " 33% 25/75 [00:05<00:10,  4.68it/s]\u001b[A\n",
            " 35% 26/75 [00:05<00:09,  4.99it/s]\u001b[A\n",
            " 36% 27/75 [00:05<00:09,  4.84it/s]\u001b[A\n",
            " 37% 28/75 [00:05<00:10,  4.63it/s]\u001b[A\n",
            " 39% 29/75 [00:05<00:10,  4.49it/s]\u001b[A\n",
            " 40% 30/75 [00:06<00:10,  4.27it/s]\u001b[A\n",
            " 41% 31/75 [00:06<00:10,  4.24it/s]\u001b[A\n",
            " 43% 32/75 [00:06<00:09,  4.56it/s]\u001b[A\n",
            " 44% 33/75 [00:06<00:09,  4.52it/s]\u001b[A\n",
            " 45% 34/75 [00:07<00:09,  4.36it/s]\u001b[A\n",
            " 47% 35/75 [00:07<00:09,  4.27it/s]\u001b[A\n",
            " 48% 36/75 [00:07<00:09,  4.29it/s]\u001b[A\n",
            " 49% 37/75 [00:07<00:08,  4.28it/s]\u001b[A\n",
            " 51% 38/75 [00:08<00:08,  4.30it/s]\u001b[A\n",
            " 52% 39/75 [00:08<00:08,  4.30it/s]\u001b[A\n",
            " 53% 40/75 [00:08<00:08,  4.36it/s]\u001b[A\n",
            " 55% 41/75 [00:08<00:07,  4.36it/s]\u001b[A\n",
            " 56% 42/75 [00:08<00:07,  4.33it/s]\u001b[A\n",
            " 57% 43/75 [00:09<00:07,  4.24it/s]\u001b[A\n",
            " 59% 44/75 [00:09<00:06,  4.69it/s]\u001b[A\n",
            " 60% 45/75 [00:09<00:06,  4.62it/s]\u001b[A\n",
            " 61% 46/75 [00:09<00:06,  4.23it/s]\u001b[A\n",
            " 63% 47/75 [00:10<00:07,  3.96it/s]\u001b[A\n",
            " 64% 48/75 [00:10<00:06,  3.98it/s]\u001b[A\n",
            " 65% 49/75 [00:10<00:06,  4.09it/s]\u001b[A\n",
            " 67% 50/75 [00:10<00:06,  3.96it/s]\u001b[A\n",
            " 68% 51/75 [00:11<00:05,  4.04it/s]\u001b[A\n",
            " 69% 52/75 [00:11<00:05,  4.38it/s]\u001b[A\n",
            " 71% 53/75 [00:11<00:05,  3.82it/s]\u001b[A\n",
            " 72% 54/75 [00:11<00:05,  3.73it/s]\u001b[A\n",
            " 73% 55/75 [00:12<00:04,  4.14it/s]\u001b[A\n",
            " 75% 56/75 [00:12<00:04,  4.23it/s]\u001b[A\n",
            " 76% 57/75 [00:12<00:04,  4.07it/s]\u001b[A\n",
            " 77% 58/75 [00:12<00:04,  4.20it/s]\u001b[A\n",
            " 79% 59/75 [00:13<00:03,  4.60it/s]\u001b[A\n",
            " 80% 60/75 [00:13<00:03,  4.33it/s]\u001b[A\n",
            " 81% 61/75 [00:13<00:03,  4.35it/s]\u001b[A\n",
            " 83% 62/75 [00:13<00:03,  4.09it/s]\u001b[A\n",
            " 84% 63/75 [00:14<00:02,  4.12it/s]\u001b[A\n",
            " 85% 64/75 [00:14<00:02,  4.34it/s]\u001b[A\n",
            " 87% 65/75 [00:14<00:02,  4.46it/s]\u001b[A\n",
            " 88% 66/75 [00:14<00:01,  4.82it/s]\u001b[A\n",
            " 89% 67/75 [00:14<00:01,  5.02it/s]\u001b[A\n",
            " 91% 68/75 [00:15<00:01,  4.76it/s]\u001b[A\n",
            " 92% 69/75 [00:15<00:01,  4.77it/s]\u001b[A\n",
            " 93% 70/75 [00:15<00:00,  5.02it/s]\u001b[A\n",
            " 95% 71/75 [00:15<00:00,  4.86it/s]\u001b[A\n",
            " 96% 72/75 [00:15<00:00,  4.79it/s]\u001b[A\n",
            " 97% 73/75 [00:16<00:00,  4.82it/s]\u001b[A\n",
            " 99% 74/75 [00:16<00:00,  4.51it/s]\u001b[A\n",
            "                                       \n",
            "\u001b[A{'eval_loss': 1.9294008016586304, 'eval_runtime': 16.7773, 'eval_samples_per_second': 4.47, 'eval_steps_per_second': 4.47, 'epoch': 0.0}\n",
            "  0% 5/11097 [00:24<4:34:41,  1.49s/it]\n",
            "100% 75/75 [00:16<00:00,  4.46it/s]\u001b[A\n",
            "{'loss': 2.3621, 'learning_rate': 0.00012, 'epoch': 0.0}\n",
            "{'loss': 2.0084, 'learning_rate': 0.00014, 'epoch': 0.0}\n",
            "{'loss': 1.4122, 'learning_rate': 0.00016, 'epoch': 0.0}\n",
            "{'loss': 1.7174, 'learning_rate': 0.00018, 'epoch': 0.0}\n",
            "{'loss': 1.4513, 'learning_rate': 0.0002, 'epoch': 0.0}\n",
            "  0% 10/11097 [00:31<8:12:26,  2.66s/it]\n",
            "  0% 0/75 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/75 [00:00<00:05, 12.62it/s]\u001b[A\n",
            "  5% 4/75 [00:00<00:11,  6.21it/s]\u001b[A\n",
            "  7% 5/75 [00:00<00:13,  5.22it/s]\u001b[A\n",
            "  8% 6/75 [00:01<00:13,  5.27it/s]\u001b[A\n",
            "  9% 7/75 [00:01<00:14,  4.56it/s]\u001b[A\n",
            " 11% 8/75 [00:01<00:15,  4.27it/s]\u001b[A\n",
            " 12% 9/75 [00:01<00:14,  4.63it/s]\u001b[A\n",
            " 13% 10/75 [00:02<00:14,  4.47it/s]\u001b[A\n",
            " 15% 11/75 [00:02<00:13,  4.87it/s]\u001b[A\n",
            " 16% 12/75 [00:02<00:13,  4.71it/s]\u001b[A\n",
            " 17% 13/75 [00:02<00:13,  4.57it/s]\u001b[A\n",
            " 19% 14/75 [00:02<00:13,  4.50it/s]\u001b[A\n",
            " 20% 15/75 [00:03<00:12,  4.70it/s]\u001b[A\n",
            " 21% 16/75 [00:03<00:13,  4.29it/s]\u001b[A\n",
            " 23% 17/75 [00:03<00:13,  4.34it/s]\u001b[A\n",
            " 24% 18/75 [00:03<00:12,  4.72it/s]\u001b[A\n",
            " 25% 19/75 [00:03<00:12,  4.50it/s]\u001b[A\n",
            " 27% 20/75 [00:04<00:12,  4.41it/s]\u001b[A\n",
            " 28% 21/75 [00:04<00:12,  4.27it/s]\u001b[A\n",
            " 29% 22/75 [00:04<00:12,  4.32it/s]\u001b[A\n",
            " 31% 23/75 [00:04<00:12,  4.02it/s]\u001b[A\n",
            " 32% 24/75 [00:05<00:12,  4.14it/s]\u001b[A\n",
            " 33% 25/75 [00:05<00:11,  4.54it/s]\u001b[A\n",
            " 35% 26/75 [00:05<00:10,  4.81it/s]\u001b[A\n",
            " 36% 27/75 [00:05<00:10,  4.74it/s]\u001b[A\n",
            " 37% 28/75 [00:06<00:10,  4.63it/s]\u001b[A\n",
            " 39% 29/75 [00:06<00:10,  4.48it/s]\u001b[A\n",
            " 40% 30/75 [00:06<00:10,  4.22it/s]\u001b[A\n",
            " 41% 31/75 [00:06<00:10,  4.20it/s]\u001b[A\n",
            " 43% 32/75 [00:06<00:09,  4.44it/s]\u001b[A\n",
            " 44% 33/75 [00:07<00:09,  4.42it/s]\u001b[A\n",
            " 45% 34/75 [00:07<00:09,  4.25it/s]\u001b[A\n",
            " 47% 35/75 [00:07<00:09,  4.15it/s]\u001b[A\n",
            " 48% 36/75 [00:07<00:09,  4.15it/s]\u001b[A\n",
            " 49% 37/75 [00:08<00:09,  4.18it/s]\u001b[A\n",
            " 51% 38/75 [00:08<00:08,  4.20it/s]\u001b[A\n",
            " 52% 39/75 [00:08<00:08,  4.21it/s]\u001b[A\n",
            " 53% 40/75 [00:08<00:08,  4.23it/s]\u001b[A\n",
            " 55% 41/75 [00:09<00:07,  4.25it/s]\u001b[A\n",
            " 56% 42/75 [00:09<00:07,  4.23it/s]\u001b[A\n",
            " 57% 43/75 [00:09<00:07,  4.33it/s]\u001b[A\n",
            " 59% 44/75 [00:09<00:06,  4.58it/s]\u001b[A\n",
            " 60% 45/75 [00:10<00:06,  4.37it/s]\u001b[A\n",
            " 61% 46/75 [00:10<00:07,  4.06it/s]\u001b[A\n",
            " 63% 47/75 [00:10<00:07,  3.84it/s]\u001b[A\n",
            " 64% 48/75 [00:10<00:06,  3.86it/s]\u001b[A\n",
            " 65% 49/75 [00:11<00:06,  3.91it/s]\u001b[A\n",
            " 67% 50/75 [00:11<00:06,  3.86it/s]\u001b[A\n",
            " 68% 51/75 [00:11<00:05,  4.03it/s]\u001b[A\n",
            " 69% 52/75 [00:11<00:05,  4.18it/s]\u001b[A\n",
            " 71% 53/75 [00:12<00:05,  3.69it/s]\u001b[A\n",
            " 72% 54/75 [00:12<00:05,  3.74it/s]\u001b[A\n",
            " 73% 55/75 [00:12<00:04,  4.07it/s]\u001b[A\n",
            " 75% 56/75 [00:12<00:04,  4.02it/s]\u001b[A\n",
            " 76% 57/75 [00:13<00:04,  3.95it/s]\u001b[A\n",
            " 77% 58/75 [00:13<00:04,  4.13it/s]\u001b[A\n",
            " 79% 59/75 [00:13<00:03,  4.34it/s]\u001b[A\n",
            " 80% 60/75 [00:13<00:03,  4.17it/s]\u001b[A\n",
            " 81% 61/75 [00:14<00:03,  4.07it/s]\u001b[A\n",
            " 83% 62/75 [00:14<00:03,  3.91it/s]\u001b[A\n",
            " 84% 63/75 [00:14<00:03,  3.97it/s]\u001b[A\n",
            " 85% 64/75 [00:14<00:02,  4.05it/s]\u001b[A\n",
            " 87% 65/75 [00:15<00:02,  4.15it/s]\u001b[A\n",
            " 88% 66/75 [00:15<00:01,  4.60it/s]\u001b[A\n",
            " 89% 67/75 [00:15<00:01,  4.71it/s]\u001b[A\n",
            " 91% 68/75 [00:15<00:01,  4.50it/s]\u001b[A\n",
            " 92% 69/75 [00:15<00:01,  4.33it/s]\u001b[A\n",
            " 93% 70/75 [00:16<00:01,  4.70it/s]\u001b[A\n",
            " 95% 71/75 [00:16<00:00,  4.56it/s]\u001b[A\n",
            " 96% 72/75 [00:16<00:00,  4.48it/s]\u001b[A\n",
            " 97% 73/75 [00:16<00:00,  4.68it/s]\u001b[A\n",
            " 99% 74/75 [00:17<00:00,  4.32it/s]\u001b[A\n",
            "                                        \n",
            "\u001b[A{'eval_loss': 1.6967270374298096, 'eval_runtime': 17.4614, 'eval_samples_per_second': 4.295, 'eval_steps_per_second': 4.295, 'epoch': 0.0}\n",
            "  0% 10/11097 [00:48<8:12:26,  2.66s/it]\n",
            "100% 75/75 [00:17<00:00,  4.27it/s]\u001b[A\n",
            "{'loss': 1.5844, 'learning_rate': 0.00019999999598540582, 'epoch': 0.0}\n",
            "{'loss': 1.3093, 'learning_rate': 0.00019999998394162357, 'epoch': 0.0}\n",
            "{'loss': 2.4056, 'learning_rate': 0.00019999996386865424, 'epoch': 0.0}\n",
            "{'loss': 1.8433, 'learning_rate': 0.0001999999357664994, 'epoch': 0.0}\n",
            "{'loss': 2.3926, 'learning_rate': 0.00019999989963516136, 'epoch': 0.0}\n",
            "  0% 15/11097 [00:59<9:48:30,  3.19s/it]\n",
            "  0% 0/75 [00:00<?, ?it/s]\u001b[A\n",
            "  3% 2/75 [00:00<00:06, 11.57it/s]\u001b[A\n",
            "  5% 4/75 [00:00<00:11,  5.98it/s]\u001b[A\n",
            "  7% 5/75 [00:00<00:13,  5.15it/s]\u001b[A\n",
            "  8% 6/75 [00:01<00:13,  4.99it/s]\u001b[A\n",
            "  9% 7/75 [00:01<00:15,  4.42it/s]\u001b[A\n",
            " 11% 8/75 [00:01<00:16,  4.08it/s]\u001b[A\n",
            " 12% 9/75 [00:01<00:14,  4.43it/s]\u001b[A\n",
            " 13% 10/75 [00:02<00:14,  4.44it/s]\u001b[A\n",
            " 15% 11/75 [00:02<00:13,  4.66it/s]\u001b[A\n",
            " 16% 12/75 [00:02<00:13,  4.52it/s]\u001b[A\n",
            " 17% 13/75 [00:02<00:14,  4.35it/s]\u001b[A\n",
            " 19% 14/75 [00:02<00:14,  4.34it/s]\u001b[A\n",
            " 20% 15/75 [00:03<00:13,  4.46it/s]\u001b[A\n",
            " 21% 16/75 [00:03<00:14,  4.12it/s]\u001b[A\n",
            " 23% 17/75 [00:03<00:14,  4.14it/s]\u001b[A\n",
            " 24% 18/75 [00:03<00:12,  4.42it/s]\u001b[A\n",
            " 25% 19/75 [00:04<00:12,  4.31it/s]\u001b[A\n",
            " 27% 20/75 [00:04<00:13,  4.21it/s]\u001b[A\n",
            " 28% 21/75 [00:04<00:13,  4.09it/s]\u001b[A\n",
            " 29% 22/75 [00:04<00:13,  4.04it/s]\u001b[A\n",
            " 31% 23/75 [00:05<00:13,  3.87it/s]\u001b[A\n",
            " 32% 24/75 [00:05<00:13,  3.92it/s]\u001b[A\n",
            " 33% 25/75 [00:05<00:11,  4.24it/s]\u001b[A\n",
            " 35% 26/75 [00:05<00:10,  4.54it/s]\u001b[A\n",
            " 36% 27/75 [00:06<00:11,  4.33it/s]\u001b[A\n",
            " 37% 28/75 [00:06<00:11,  4.23it/s]\u001b[A\n",
            " 39% 29/75 [00:06<00:10,  4.18it/s]\u001b[A\n",
            " 40% 30/75 [00:06<00:11,  3.91it/s]\u001b[A\n",
            " 41% 31/75 [00:07<00:11,  3.74it/s]\u001b[A\n",
            " 43% 32/75 [00:07<00:10,  4.17it/s]\u001b[A\n",
            " 44% 33/75 [00:07<00:10,  4.17it/s]\u001b[A\n",
            " 45% 34/75 [00:07<00:10,  4.04it/s]\u001b[A\n",
            " 47% 35/75 [00:08<00:10,  3.82it/s]\u001b[A\n",
            " 48% 36/75 [00:08<00:10,  3.88it/s]\u001b[A\n",
            " 49% 37/75 [00:08<00:09,  3.88it/s]\u001b[A\n",
            " 51% 38/75 [00:08<00:09,  4.00it/s]\u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf ./qlora-out"
      ],
      "metadata": {
        "id": "HY2fip71GAsy"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSdzA9YlGWXK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}