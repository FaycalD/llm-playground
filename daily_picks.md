# Daily Picks

This is for tracking daily papers, daily news, my daily discoveries/thoughts/work in the area.

Inspired by [GenAI_LLM_timeline](https://github.com/hollobit/GenAI_LLM_timeline) and [Daily Papers](https://papers.labml.ai/papers/daily/) but personalized and focused.

- Milestone-ish models/datasets/apps are categorized as news, even if they come with papers.
- Papers are for better understanding the mechanisms and not just a new model trained differently, good blogs are also counted as papers.
- Discoveries are what changed my perspective or practice.
- News are dated by the time they happened. Discoveries and papers are dated by the time I noticeed their importance[^1].
- Style: only key words in the table, extra info should be available via the link or the food note.


|    Date    | Papers | News                                                 | Discoveries | Thoughts/work |
| :--------: | :----- | :--------------------------------------------------- | :---------- | :------------ |
| 4.21 | | | Training [logbook](https://github.com/facebookresearch/metaseq/tree/main/projects/OPT/chronicles) & [metric](https://wandb.ai/eleutherai/neox) |
| 4.21 | | | [axolotl](https://github.com/winglian/axolotl) & [genv](https://github.com/run-ai/genv) |
| 4.20 |  [Verifiability](https://arxiv.org/abs/2304.09848) |
| 4.19 | | |  [GPTCache](https://github.com/zilliztech/GPTCache) |
| 4.19 | | |  [GPTCache](https://github.com/zilliztech/GPTCache) |
| 4.19 | |  [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) |
| 4.19 | | |  [meerkat](https://github.com/HazyResearch/meerkat) |
| 4.19 | | | [CAMEL](https://github.com/lightaime/camel) & [chatarena](https://github.com/chatarena/chatarena) |
| 4.18 | | [LLaVA](https://github.com/haotian-liu/LLaVA) |
| 4.17 | | |   [Alpaca-CoT](https://github.com/PhoebusSi/Alpaca-CoT) |
| 4.17 | |  [RedPajama-Data](https://github.com/togethercomputer/RedPajama-Data) |
| 4.17 | |  | [alpaca_lora_4bit](https://github.com/johnsmith0031/alpaca_lora_4bit) |
| 4.16 |  [LLMs + Symbolic Solvers](https://arxiv.org/abs/2304.09102) |
| 4.16 |   [`suggest_premises`](https://github.com/BartoszPiotrowski/lean-premise-selection) |
| 4.15 | |[MiniGPT-4](https://minigpt-4.github.io/) |
| 4.15 | |[web-llm](https://github.com/mlc-ai/web-llm) |
| 4.14 | | | [Buzzard's talk](https://leanprover.zulipchat.com/#narrow/stream/219941-Machine-Learning-for-Theorem-Proving/topic/Bangalore.20talk.20on.20LLM.2BITP/near/348388948) |
| 4.14 | | | [ProofNet](https://github.com/zhangir-azerbayev/ProofNet) |
| 4.14 | [Multimodal C4](https://arxiv.org/abs/2304.06939) |
| 4.13 | [GPT-4 Annotating](https://arxiv.org/abs/2304.06588) |
| 4.12 | [Galactic ChitChat](https://arxiv.org/abs/2304.05406) |
| 4.12 | |  [Dolly v2](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) |
| 4.11 | [Toxicity](https://arxiv.org/abs/2304.05335) |
| 4.11 | [Privacy Attacks](https://arxiv.org/abs/2304.05197) |
| 4.11 | [Self-Debug](https://arxiv.org/abs/2304.05128) |
| 4.11 | [Auto-Sci](https://arxiv.org/abs/2304.05332) |
| 4.12 | | | [RunPod.io](https://runpod.io?ref=km0th85l) |
| 4.10 |  [pal](https://github.com/reasoning-machines/pal) |
| 4.9 | | |  [Patrick's talk](https://www.youtube.com/watch?v=tp_h3vzkObo) |
| 4.9 |  [ACT]([Trusted-AI/adversarial-robustness-toolbox](https://github.com/Trusted-AI/adversarial-robustness-toolbox)) |
| 4.9 | | |  [dagster](https://github.com/dagster-io/dagster) & [mage-ai](https://github.com/mage-ai/mage-ai) |
| 4.8 | | |  [data-centric-AI](https://github.com/daochenzha/data-centric-AI) |
| 4.8 | [Training Recipe](https://wandb.ai/craiyon/report/reports/Recipe-Training-Large-Models--VmlldzozNjc4MzQz) | 
| 4.7 | |  [lightning](https://github.com/Lightning-AI/lightning) & [lit-llama](https://github.com/Lightning-AI/lit-llama) |
| 4.7 |  | [Vicuna](https://github.com/lm-sys/FastChat) |
| 4.5 | | [SAM](https://ai.facebook.com/research/publications/segment-anything/) |
| 4.4 | | |  [text-generation-webui](https://github.com/oobabooga/text-generation-webui) |
| 4.4 | [LLM-Adapters](https://arxiv.org/abs/2304.01933) |
| 4.3 | | [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) |
| 4.2 |  [Code Self-Improvement](https://arxiv.org/abs/2304.01228) |
| 4.2 | | | [ChuanhuChatGPT](https://github.com/GaiZhenbiao/ChuanhuChatGPT) |
| 4.1 |  |  [LMFlow](https://github.com/OptimalScale/LMFlow) |
| 3.31 | [Choose Your Weapon](https://arxiv.org/abs/2304.06035) |
| 3.30 | [Humans in Humans Out](https://arxiv.org/abs/2303.17276) |
| 3.30 | | [BloombergGPT](https://arxiv.org/abs/2303.17564) |
| 3.30 | | [Auto-GPT](https://github.com/Torantulino/Auto-GPT) |
| 3.29 | | | [guardrails](https://github.com/ShreyaR/guardrails) & [lmql](https://github.com/eth-sri/lmql) & [kor](https://github.com/eyurtsev/kor) |
| 3.29 | | [GPT4All](https://github.com/nomic-ai/gpt4all) |
| 3.29 | | [LLaMA-Adapter](https://arxiv.org/abs/2303.16199) |
| 3.29 | | | [llama_index](https://github.com/jerryjliu/llama_index) | 
| 3.28 | | [Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) |
| 3.26 | [Low-Rank Simplicity Bias](https://openreview.net/forum?id=dn4B7Mes2z) |
| 3.24 | | [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) |
| 3.23 | | | [dalai](https://github.com/cocktailpeanut/dalai)[^4] |
| 3.23 | | [ChatGPT Plugins](https://openai.com/blog/chatgpt-plugins) |
| 3.23 | | | [Cursor.so](http://cursor.so/)[^3] |
| 3.22	| | [Sparks of AGI](https://arxiv.org/abs/2303.12712) |
| 3.20 | | [ChatGPT outage](https://openai.com/blog/march-20-chatgpt-outage) | 
| 3.16 | | [Alpaca LoRA](https://twitter.com/_akhaliq/status/1636416647518097408) |
| 3.15	| | [GPT-4 TR](https://arxiv.org/abs/2303.08774) |
|    3.14    |        | [GPT-4](https://openai.com/research/gpt-4)           |
| 3.13 | | [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) |
|    3.2     |        | | [miniF2F](https://github.com/openai/miniF2F)       |
|    3.1     |        | [ChatGPT API](https://openai.com/blog/chatgpt)       |
|    3.1     |        | | [galai](https://github.com/paperswithcode/galai) |
| 2.26 | | | [ColossalAI](https://github.com/hpcaitech/ColossalAI)[^2]
| 2.24 | | [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai)
|    2.10    |        | [ChatGPT Plus](https://openai.com/blog/chatgpt-plus) |
|    2.7    |        | [New Bing](https://blogs.microsoft.com/blog/2023/02/07/reinventing-search-with-a-new-ai-powered-microsoft-bing-and-edge-your-copilot-for-the-web/) |
|    2023    |
| 2022.11.30 |        | [ChatGPT](https://openai.com/blog/chatgpt)           |
| 2021.08.10 |        | [Codex](https://openai.com/blog/openai-codex)        |
| 2020.05.28 |        | [GPT-3](https://arxiv.org/abs/2005.14165)            |


## TODO

Decide whether include them and determine dates:

|    Date    | Papers | News                                                 | Discoveries | Thoughts/work |
| :--------: | :----- | :--------------------------------------------------- | :---------- | :------------ |
| 4.9 | | |  [spaCy](https://github.com/explosion/spaCy) |
| 3.31 | | |  [simple-llm-finetuner](https://github.com/lxe/simple-llm-finetuner) |
|    3.1     |        | | [lvwerra/trl](https://github.com/lvwerra/trl) |
| 1.6 |  |  [NeevaAI](https://neeva.com/blog/introducing-neevaai) |


## Related curated lists

### Papers & Notes

- [thunlp/PromptPapers](https://github.com/thunlp/PromptPapers) - Must-read papers on prompt-based tuning for pre-trained language models.
- [dair-ai/ML-Course-Notes](https://github.com/dair-ai/ML-Course-Notes) - üéì Sharing machine learning course / lecture notes.

### Models

- [Longyichen/Alpaca-family-library](https://github.com/Longyichen/Alpaca-family-library) - Summarize all low-cost replication methods for Chatgpt. 
- [imaurer/awesome-decentralized-llm](https://github.com/imaurer/awesome-decentralized-llm) - Collection of LLM resources that can be used to build products you can "own" or to perform reproducible research.
- [nichtdax/awesome-totally-open-chatgpt](https://github.com/nichtdax/awesome-totally-open-chatgpt) - A list of totally open alternatives to ChatGPT
- [FreedomIntelligence/LLMZoo](https://github.com/FreedomIntelligence/LLMZoo) - ‚ö°LLM Zoo is a project that provides data, models, and evaluation benchmark for large language models.‚ö°

### Training

- [zhilizju/Awesome-instruction-tuning](https://github.com/zhilizju/Awesome-instruction-tuning) - A curated list of awesome instruction tuning datasets, models, papers and repositories.
- [visenger/awesome-mlops](https://github.com/visenger/awesome-mlops) - A curated list of references for MLOps

### Reasoning

- [lupantech/dl4math](https://github.com/lupantech/dl4math) - Resources of deep learning for mathematical reasoning (DL4MATH).

### Prompting

- [dair-ai/Prompt-Engineering-Guide](https://github.com/dair-ai/Prompt-Engineering-Guide) - üêô Guides, papers, lecture, notebooks and resources for prompt engineering

### Apps

- [reorx/awesome-chatgpt-api](https://github.com/reorx/awesome-chatgpt-api) - Curated list of apps and tools that not only use the new ChatGPT API, but also allow users to configure their own API keys, enabling free and on-demand usage of their own quota.

[^1]: Models and datasets are already tracked seperately as simple machine-digestable files as `models.txt` and `datasets.txt`, and some on [my likes](https://huggingface.co/utensil/activity/likes). Repos are tracked by [my stars](https://github.com/utensil/awesome-stars/blob/master/topics.md), mostly in topic `chatgpt`, `chatgpt-api`, `ai`, `artificial-intelligence`, `data-science` and `data-analysis`, also in my star list [lean-llm](https://github.com/stars/utensil/lists/lean-llm) focusing on the building blocks of applying LLMs to the ITP/ATP area.

[^2]: The first open source RLHF pipeline

[^3]: Helped me experience prompt-based coding infinitely

[^4]: Helped me testing LLaMA and Alpaca locally
