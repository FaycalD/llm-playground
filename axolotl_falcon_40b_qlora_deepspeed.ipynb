{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/utensil/llm-playground/blob/main/axolotl_falcon_40b_qlora_deepspeed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97yoSiRvDY7G"
      },
      "source": [
        "# Finetuning falcon-40b\n",
        "\n",
        "- Axolotl+QLoRA\n",
        "- minotaur datasets\n",
        "- deepspeed ZeRO 3 8xGPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCpbQuaxDY7H"
      },
      "source": [
        "<!-- https://jupyterlab.readthedocs.io/en/stable/user/commands.html#commands-list -->\n",
        "<button data-commandLinker-command=\"apputils:change-theme\" data-commandlinker-args='{\"theme\": \"JupyterLab Dark\"}' href=\"#\">Dark theme</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/\"}' href=\"#\">llm-playground</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/\"}' href=\"#\">axolotl</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/notebooks/axolotl/runpod\"}' href=\"#\">Runpod notebooks</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples\"}' href=\"#\">axolotl configs</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/workspace/llm-playground/storage\"}' href=\"#\">Storage</button>\n",
        "<button data-commandLinker-command=\"filebrowser:go-to-path\" data-commandlinker-args='{\"path\": \"/content/axolotl-trained\"}' href=\"#\">axolotl-trained</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/examples/falcon/config-40b-qlora.yml\"}' href=\"#\">Edit qlora config</button>\n",
        "<button data-commandLinker-command=\"docmanager:open\" data-commandlinker-args='{\"path\": \"/workspace/axolotl/ds_config.json\"}' href=\"#\">Edit ds config</button>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uZd-cf70HM2n"
      },
      "source": [
        "## Prepare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJ7G9yDafGKD"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y aria2\n",
        "!git lfs install\n",
        "!pip install requests huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8COOvvYbfGKE"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/llm-playground/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M1twBbt_fGKE"
      },
      "outputs": [],
      "source": [
        "!rm -rf models\n",
        "!mkdir -p /content/models/\n",
        "!ln -s /content/models/ ./models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSA6ZtBIfGKE"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/download-model.py tiiuae/falcon-40b\n",
        "# /workspace/llm-playground/models/tiiuae_falcon-40b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-DNFYU-fGKE"
      },
      "outputs": [],
      "source": [
        "# Downloading shards: 100%|████████████████████████| 9/9 [19:19<00:00, 128.85s/it]\n",
        "# Loading checkpoint shards: 100%|██████████████████| 9/9 [01:30<00:00, 10.08s/it]\n",
        "# (OK):download completed.\n",
        "# 100%|████████████████████████████████████████████████████████████████████| 18/18 [01:44<00:00,  5.81s/it]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkrtaRkauKLL"
      },
      "source": [
        "### Set HF Cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpIyW4yWuHvd"
      },
      "outputs": [],
      "source": [
        "# %env HF_DATASETS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hHOfHUYQduRX"
      },
      "outputs": [],
      "source": [
        "#%env TRANSFORMERS_CACHE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oR9oblHTduRY"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KkaKgWKnduRY"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/cache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aJMsnPnOduRY"
      },
      "outputs": [],
      "source": [
        "!ln -s /content/cache /root/.cache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFqQyPp5HbAm"
      },
      "source": [
        "### HF Login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s6wC1lP23B-4",
        "outputId": "9e80e244-b214-4dec-cfba-8d98d0aeba6d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token is valid (permission: write).\n",
            "Your token has been saved in your configured git credential helpers (store).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# For axolotl push_dataset_to_hub\n",
        "import os\n",
        "from huggingface_hub import notebook_login, login\n",
        "# Colab:\n",
        "# notebook_login()\n",
        "# RunPod:\n",
        "login(os.environ.get(\"HUGGINGFACE_TOKEN\"), add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2AigYR_DY7X"
      },
      "source": [
        "### Update axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQYJdcd7DY7X"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PLn5aNACDY7X",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/OpenAccess-AI-Collective/axolotl axolotl-update"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdY5tXtYDY7X"
      },
      "outputs": [],
      "source": [
        "!cp -r axolotl-update/* axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHsKmIUmDY7Y"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljX3Yir2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!git status"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Adru8lVlzhLi",
        "outputId": "084d6af3-b595-4279-d599-0d49a4460b3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu117\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+52907a66, 52907a66, master\n",
            "torch cuda version ............... 11.7\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wbaorAF2DY7Y",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!pip install -e ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p_9eIEonzhLi",
        "outputId": "fc687682-4a63-4400-fb40-e769dd00bd29"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SD1Ahx1QG5xm"
      },
      "source": [
        "### Init Storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBBF-lS2HVPG",
        "outputId": "400163fc-7e26-4927-d66a-900e7cc9d060"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning https://huggingface.co/utensil/axolotl-trained into local empty directory.\n"
          ]
        }
      ],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -m"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_FP2VeXH0Un"
      },
      "outputs": [],
      "source": [
        "!ls /content/axolotl-trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XPAjakxfGKO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBubllkKfGKO"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "raYU6HmYduRa"
      },
      "source": [
        "### Reinstall PyTorch with CUDA 11.8 (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ym6v1nl6duRa",
        "outputId": "f0ec996a-0212-4080-a1aa-f647b0f57a22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (2.0.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.0.1%2Bcu118-cp39-cp39-linux_x86_64.whl (2267.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 GB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (4.6.3)\n",
            "Requirement already satisfied: sympy in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from torch) (2.0.0)\n",
            "Requirement already satisfied: cmake in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (3.26.4)\n",
            "Requirement already satisfied: lit in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from triton==2.0.0->torch) (16.0.5.post0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /root/miniconda3/envs/py3.9/lib/python3.9/site-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.0.1\n",
            "    Uninstalling torch-2.0.1:\n",
            "      Successfully uninstalled torch-2.0.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.0.1+cu118 requires torch==2.0.0, but you have torch 2.0.1+cu118 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed torch-2.0.1+cu118\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip3 install -U torch --index-url https://download.pytorch.org/whl/cu118"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W0A0aPWJzhLk",
        "outputId": "a3025d0a-f12e-4c87-836e-e71ba7abb80f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch                    2.0.1+cu118\n",
            "torchaudio               2.0.1+cu118\n",
            "torchvision              0.15.2\n"
          ]
        }
      ],
      "source": [
        "!pip list|grep torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4sPSrhKPHIrS"
      },
      "source": [
        "### Reinstall deepspeed (optional)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_PIACM6duRa"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i1-u6vhWHvt6"
      },
      "outputs": [],
      "source": [
        "!yes|pip uninstall deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xIpqugJOHuhh",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!TORCH_CUDA_ARCH_LIST=\"3.5;5.0;6.0;6.1;7.0;7.5;8.0;8.6+PTX\" DS_BUILD_OPS=1 DS_BUILD_SPARSE_ATTN=0 pip install deepspeed --global-option=\"build_ext\" --global-option=\"-j8\" # --global-option=\"bdist_wheel\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8JzOr94dDm1"
      },
      "outputs": [],
      "source": [
        "!pip install deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcKtKd_MdDm1"
      },
      "outputs": [],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_tw4OcVHQdK"
      },
      "source": [
        "### Init Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvyUmZdktu0",
        "outputId": "25275dd8-4787-4b77-f7bf-4f2bbe66f0b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/workspace/axolotl\n"
          ]
        }
      ],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mC48y25Lkqa5"
      },
      "outputs": [],
      "source": [
        "# Try no config\n",
        "# !accelerate config default"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cL5E8urQEXiL",
        "outputId": "1a7ac7ab-c81d-4e10-ac9e-df10ed3181c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting ds_config.json\n"
          ]
        }
      ],
      "source": [
        "%%writefile ds_config.json\n",
        "{\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 3,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"offload_param\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": true\n",
        "        },\n",
        "        \"overlap_comm\": true,\n",
        "        \"contiguous_gradients\": true,\n",
        "        \"sub_group_size\": 0,\n",
        "        \"reduce_bucket_size\": \"auto\",\n",
        "        \"stage3_prefetch_bucket_size\": \"auto\",\n",
        "        \"stage3_param_persistence_threshold\": \"auto\",\n",
        "        \"stage3_max_live_parameters\": 0,\n",
        "        \"stage3_max_reuse_distance\": 0,\n",
        "        \"stage3_gather_16bit_weights_on_model_save\": true\n",
        "    },\n",
        "   \"bf16\": {\n",
        "        \"enabled\": \"auto\"\n",
        "    },\n",
        "    \"fp16\": {\n",
        "        \"enabled\": \"auto\",\n",
        "        \"auto_cast\": false,\n",
        "        \"loss_scale\": 0,\n",
        "        \"initial_scale_power\": 32,\n",
        "        \"loss_scale_window\": 1000,\n",
        "        \"hysteresis\": 2,\n",
        "        \"min_loss_scale\": 1\n",
        "    },\n",
        "    \"optimizer\": {\n",
        "        \"type\": \"AdamW\",\n",
        "        \"params\": {\n",
        "          \"lr\": \"auto\",\n",
        "          \"betas\": \"auto\",\n",
        "          \"eps\": \"auto\",\n",
        "          \"weight_decay\": \"auto\"\n",
        "        }\n",
        "    },\n",
        "    \"scheduler\": {\n",
        "      \"type\": \"WarmupDecayLR\",\n",
        "      \"params\": {\n",
        "        \"total_num_steps\": \"auto\",\n",
        "        \"warmup_min_lr\": \"auto\",\n",
        "        \"warmup_max_lr\": \"auto\",\n",
        "        \"warmup_num_steps\": \"auto\"\n",
        "       }\n",
        "    },\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"wall_clock_breakdown\": false\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efZJw_gCDY7J",
        "outputId": "30a5cd1b-eda1-4384-f9d5-090dd376961d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: tiiuae/falcon-40b\n",
        "base_model_config: tiiuae/falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "# optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "# lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTWOnrpzEr-1"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iz9sbxRAElkU"
      },
      "outputs": [],
      "source": [
        "%%writefile scripts/ft.py\n",
        "import os\n",
        "from pathlib import Path\n",
        "import fire\n",
        "import logging\n",
        "import finetune\n",
        "from axolotl.utils.trainer import setup_trainer as setup_trainer_orig\n",
        "\n",
        "logging.basicConfig(level=os.getenv(\"LOG_LEVEL\", \"INFO\"))\n",
        "\n",
        "def train_ex(\n",
        "    config: Path = Path(\"configs/\"),\n",
        "    prepare_ds_only: bool = False,\n",
        "    **kwargs,\n",
        "):\n",
        "  logging.info('train_ex before')\n",
        "  finetune.train(config, prepare_ds_only, **kwargs)\n",
        "  logging.info('train_ex after')\n",
        "\n",
        "def setup_trainer_ex(cfg, train_dataset, eval_dataset, model, tokenizer):\n",
        "  logging.info('setup_trainer_ex before')\n",
        "  logging.info(f'cfg.some_config = {cfg.some_config}')\n",
        "  trainer = setup_trainer_orig(cfg, train_dataset, eval_dataset, model, tokenizer)\n",
        "  logging.info('setup_trainer_ex after')\n",
        "  return trainer\n",
        "\n",
        "finetune.setup_trainer = setup_trainer_ex\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    fire.Fire(train_ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vj6CD_zZHUpG"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2CqrhYq-DY7J"
      },
      "source": [
        "## #1 Acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhAG_JTFfGKa",
        "outputId": "f6c30c13-d00f-4a82-8f61-83902de69566"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: HF_HUB_DISABLE_PROGRESS_BARS=1\n"
          ]
        }
      ],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v4M3iIufGKa"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m66yoSyzK7uC"
      },
      "outputs": [],
      "source": [
        "%cd /workspace/axolotl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "joQh6mGBm3_J"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ULwPlElbm73f"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jICMPJuomFsx",
        "outputId": "241a2589-fb00-414b-b27c-d5fc27e131ae",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib'), PosixPath('/usr/local/nvidia/lib64')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_tuxr7tmt/none_tidxp0vy/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_tuxr7tmt/none_tidxp0vy/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so'), PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... tiiuae/falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 532.88it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 542.04it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Downloading shards: 100%|████████████████████████| 9/9 [19:19<00:00, 128.86s/it]\n",
            "Downloading shards: 100%|████████████████████████| 9/9 [19:19<00:00, 128.85s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:30<00:00, 10.08s/it]\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:31<00:00, 10.11s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense', 'dense_4h_to_h', 'dense_h_to_4h']\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['query_key_value', 'dense', 'dense_h_to_4h', 'dense_4h_to_h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_021328-4a19kniz\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmorning-wind-31\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/4a19kniz\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 1.0674, 'learning_rate': 2e-05, 'epoch': 0.0}                          \n",
            "  0%|                                       | 1/11097 [00:02<8:21:23,  2.71s/it]INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "INFO:torch.nn.parallel.distributed:Reducer buckets have been rebuilt in this iteration.\n",
            "{'loss': 0.9643, 'learning_rate': 4e-05, 'epoch': 0.0}                          \n",
            "{'loss': 0.9338, 'learning_rate': 6e-05, 'epoch': 0.0}                          \n",
            "{'loss': 0.6366, 'learning_rate': 8e-05, 'epoch': 0.0}                          \n",
            "{'loss': 0.8581, 'learning_rate': 0.0001, 'epoch': 0.0}                         \n",
            "  0%|                                       | 5/11097 [00:09<5:32:07,  1.80s/it]\n",
            "  0%|                                                    | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|██▎                                         | 2/38 [00:00<00:09,  3.86it/s]\u001b[A\n",
            "  8%|███▍                                        | 3/38 [00:01<00:12,  2.76it/s]\u001b[A\n",
            " 11%|████▋                                       | 4/38 [00:01<00:15,  2.18it/s]\u001b[A\n",
            " 13%|█████▊                                      | 5/38 [00:02<00:15,  2.08it/s]\u001b[A\n",
            " 16%|██████▉                                     | 6/38 [00:02<00:15,  2.03it/s]\u001b[A\n",
            " 18%|████████                                    | 7/38 [00:03<00:15,  1.99it/s]\u001b[A\n",
            " 21%|█████████▎                                  | 8/38 [00:03<00:15,  1.95it/s]\u001b[A\n",
            " 24%|██████████▍                                 | 9/38 [00:04<00:14,  1.97it/s]\u001b[A\n",
            " 26%|███████████▎                               | 10/38 [00:04<00:14,  1.90it/s]\u001b[A\n",
            " 29%|████████████▍                              | 11/38 [00:05<00:14,  1.91it/s]\u001b[A\n",
            " 32%|█████████████▌                             | 12/38 [00:05<00:14,  1.86it/s]\u001b[A\n",
            " 34%|██████████████▋                            | 13/38 [00:06<00:13,  1.88it/s]\u001b[A\n",
            " 37%|███████████████▊                           | 14/38 [00:06<00:12,  1.88it/s]\u001b[A\n",
            " 39%|████████████████▉                          | 15/38 [00:07<00:12,  1.88it/s]\u001b[A\n",
            " 42%|██████████████████                         | 16/38 [00:08<00:11,  1.89it/s]\u001b[A\n",
            " 45%|███████████████████▏                       | 17/38 [00:08<00:11,  1.89it/s]\u001b[A\n",
            " 47%|████████████████████▎                      | 18/38 [00:09<00:10,  1.89it/s]\u001b[A\n",
            " 50%|█████████████████████▌                     | 19/38 [00:09<00:10,  1.90it/s]\u001b[A\n",
            " 53%|██████████████████████▋                    | 20/38 [00:10<00:09,  1.89it/s]\u001b[A\n",
            " 55%|███████████████████████▊                   | 21/38 [00:10<00:08,  1.89it/s]\u001b[A\n",
            " 58%|████████████████████████▉                  | 22/38 [00:11<00:08,  1.91it/s]\u001b[A\n",
            " 61%|██████████████████████████                 | 23/38 [00:11<00:07,  1.88it/s]\u001b[A\n",
            " 63%|███████████████████████████▏               | 24/38 [00:12<00:07,  1.84it/s]\u001b[A\n",
            " 66%|████████████████████████████▎              | 25/38 [00:12<00:07,  1.85it/s]\u001b[A\n",
            " 68%|█████████████████████████████▍             | 26/38 [00:13<00:06,  1.90it/s]\u001b[A\n",
            " 71%|██████████████████████████████▌            | 27/38 [00:13<00:06,  1.76it/s]\u001b[A\n",
            " 74%|███████████████████████████████▋           | 28/38 [00:14<00:05,  1.80it/s]\u001b[A\n",
            " 76%|████████████████████████████████▊          | 29/38 [00:15<00:04,  1.83it/s]\u001b[A\n",
            " 79%|█████████████████████████████████▉         | 30/38 [00:15<00:04,  1.84it/s]\u001b[A\n",
            " 82%|███████████████████████████████████        | 31/38 [00:16<00:03,  1.81it/s]\u001b[A\n",
            " 84%|████████████████████████████████████▏      | 32/38 [00:16<00:03,  1.84it/s]\u001b[A\n",
            " 87%|█████████████████████████████████████▎     | 33/38 [00:17<00:02,  1.89it/s]\u001b[A\n",
            " 89%|██████████████████████████████████████▍    | 34/38 [00:17<00:02,  1.89it/s]\u001b[A\n",
            " 92%|███████████████████████████████████████▌   | 35/38 [00:18<00:01,  1.93it/s]\u001b[A\n",
            " 95%|████████████████████████████████████████▋  | 36/38 [00:18<00:01,  1.89it/s]\u001b[A\n",
            " 97%|█████████████████████████████████████████▊ | 37/38 [00:19<00:00,  1.89it/s]\u001b[A\n",
            "                                                                                \u001b[A\n",
            "\u001b[A{'eval_loss': 1.0059092044830322, 'eval_runtime': 20.2979, 'eval_samples_per_second': 3.695, 'eval_steps_per_second': 1.872, 'epoch': 0.0}\n",
            "  0%|                                       | 5/11097 [00:29<5:32:07,  1.80s/it]\n",
            "100%|███████████████████████████████████████████| 38/38 [00:19<00:00,  1.91it/s]\u001b[A\n",
            "{'loss': 0.9844, 'learning_rate': 0.00012, 'epoch': 0.0}                        \u001b[A\n",
            "{'loss': 0.9058, 'learning_rate': 0.00014, 'epoch': 0.0}                        \n",
            "{'loss': 0.7808, 'learning_rate': 0.00016, 'epoch': 0.0}                        \n",
            "{'loss': 0.7109, 'learning_rate': 0.00018, 'epoch': 0.0}                        \n",
            "{'loss': 1.3204, 'learning_rate': 0.0002, 'epoch': 0.0}                         \n",
            "  0%|                                     | 10/11097 [00:38<10:04:58,  3.27s/it]\n",
            "  0%|                                                    | 0/38 [00:00<?, ?it/s]\u001b[A\n",
            "  5%|██▎                                         | 2/38 [00:00<00:09,  3.87it/s]\u001b[A\n",
            "  8%|███▍                                        | 3/38 [00:01<00:12,  2.76it/s]\u001b[A\n",
            " 11%|████▋                                       | 4/38 [00:01<00:15,  2.18it/s]\u001b[A\n",
            " 13%|█████▊                                      | 5/38 [00:02<00:15,  2.08it/s]\u001b[A\n",
            " 16%|██████▉                                     | 6/38 [00:02<00:15,  2.03it/s]\u001b[A\n",
            " 18%|████████                                    | 7/38 [00:03<00:15,  1.99it/s]\u001b[A\n",
            " 21%|█████████▎                                  | 8/38 [00:03<00:15,  1.95it/s]\u001b[A\n",
            " 24%|██████████▍                                 | 9/38 [00:04<00:14,  1.97it/s]\u001b[A\n",
            " 26%|███████████▎                               | 10/38 [00:04<00:14,  1.90it/s]\u001b[A\n",
            " 29%|████████████▍                              | 11/38 [00:05<00:14,  1.91it/s]\u001b[A\n",
            " 32%|█████████████▌                             | 12/38 [00:05<00:14,  1.85it/s]\u001b[A\n",
            " 34%|██████████████▋                            | 13/38 [00:06<00:13,  1.88it/s]\u001b[A\n",
            " 37%|███████████████▊                           | 14/38 [00:06<00:12,  1.88it/s]\u001b[A\n",
            " 39%|████████████████▉                          | 15/38 [00:07<00:12,  1.87it/s]\u001b[A\n",
            " 42%|██████████████████                         | 16/38 [00:08<00:11,  1.90it/s]\u001b[A\n",
            " 45%|███████████████████▏                       | 17/38 [00:08<00:11,  1.89it/s]\u001b[A\n",
            " 47%|████████████████████▎                      | 18/38 [00:09<00:10,  1.89it/s]\u001b[A\n",
            " 50%|█████████████████████▌                     | 19/38 [00:09<00:10,  1.90it/s]\u001b[A\n",
            " 53%|██████████████████████▋                    | 20/38 [00:10<00:09,  1.89it/s]\u001b[A\n",
            " 55%|███████████████████████▊                   | 21/38 [00:10<00:08,  1.89it/s]\u001b[A\n",
            " 58%|████████████████████████▉                  | 22/38 [00:11<00:08,  1.91it/s]\u001b[A\n",
            " 61%|██████████████████████████                 | 23/38 [00:11<00:07,  1.88it/s]\u001b[A\n",
            " 63%|███████████████████████████▏               | 24/38 [00:12<00:07,  1.84it/s]\u001b[A\n",
            " 66%|████████████████████████████▎              | 25/38 [00:12<00:07,  1.85it/s]\u001b[A\n",
            " 68%|█████████████████████████████▍             | 26/38 [00:13<00:06,  1.90it/s]\u001b[A\n",
            " 71%|██████████████████████████████▌            | 27/38 [00:13<00:06,  1.76it/s]\u001b[A\n",
            " 74%|███████████████████████████████▋           | 28/38 [00:14<00:05,  1.80it/s]\u001b[A\n",
            " 76%|████████████████████████████████▊          | 29/38 [00:15<00:04,  1.83it/s]\u001b[A\n",
            " 79%|█████████████████████████████████▉         | 30/38 [00:15<00:04,  1.84it/s]\u001b[A\n",
            " 82%|███████████████████████████████████        | 31/38 [00:16<00:03,  1.81it/s]\u001b[A\n",
            " 84%|████████████████████████████████████▏      | 32/38 [00:16<00:03,  1.84it/s]\u001b[A\n",
            " 87%|█████████████████████████████████████▎     | 33/38 [00:17<00:02,  1.89it/s]\u001b[A\n",
            " 89%|██████████████████████████████████████▍    | 34/38 [00:17<00:02,  1.89it/s]\u001b[A\n",
            " 92%|███████████████████████████████████████▌   | 35/38 [00:18<00:01,  1.93it/s]\u001b[A\n",
            " 95%|████████████████████████████████████████▋  | 36/38 [00:18<00:01,  1.89it/s]\u001b[A\n",
            " 97%|█████████████████████████████████████████▊ | 37/38 [00:19<00:00,  1.89it/s]\u001b[A^C\n",
            "WARNING:torch.distributed.elastic.agent.server.api:Received 2 death signal, shutting down workers\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2296 closing signal SIGINT\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 2297 closing signal SIGINT\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3362, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2661, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/peft/peft_model.py\", line 739, in forward\n",
            "    return self.base_model(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 759, in forward\n",
            "    transformer_outputs = self.transformer(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 654, in forward\n",
            "    outputs = block(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1501, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 255, in forward\n",
            "    (query_layer, key_layer, value_layer) = self._split_heads(fused_qkv)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae/falcon-40b/2ac60b04625e6694fb6143c00b9f93a01c7a000f/modelling_RW.py\", line 201, in _split_heads\n",
            "    k = qkv[:, :, :, [-2]]\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1883, in _inner_training_loop\n",
            "    self._maybe_log_save_evaluate(tr_loss, model, trial, epoch, ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2184, in _maybe_log_save_evaluate\n",
            "    metrics = self.evaluate(ignore_keys=ignore_keys_for_eval)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2930, in evaluate\n",
            "    output = eval_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3119, in evaluation_loop\n",
            "    loss, logits, labels = self.prediction_step(model, inputs, prediction_loss_only, ignore_keys=ignore_keys)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 3362, in prediction_step\n",
            "    loss, outputs = self.compute_loss(model, inputs, return_outputs=True)\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yIfs-AbKDY7K"
      },
      "source": [
        "## #2 Deepspeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w0Ex80D5fGKb",
        "outputId": "9ba747f5-fc69-4e8d-ee52-1fbef813e231"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: ACCELERATE_USE_DEEPSPEED=true\n"
          ]
        }
      ],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v1MDQqWjfGKb",
        "outputId": "0f8fcd2a-69ff-46fe-d8ce-c1cdbea12e0c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overwriting examples/falcon/config-40b-qlora.yml\n"
          ]
        }
      ],
      "source": [
        "%%writefile examples/falcon/config-40b-qlora.yml\n",
        "# 1b: tiiuae/falcon-rw-1b\n",
        "# 7b: tiiuae/falcon-7b\n",
        "# 40b: tiiuae/falcon-40b\n",
        "base_model: /workspace/llm-playground/models/tiiuae_falcon-40b\n",
        "base_model_config: /workspace/llm-playground/models/tiiuae_falcon-40b\n",
        "# required by falcon custom model code: https://huggingface.co/tiiuae/falcon-7b/tree/main\n",
        "trust_remote_code: true\n",
        "model_type: AutoModelForCausalLM\n",
        "tokenizer_type: AutoTokenizer\n",
        "load_in_8bit: false\n",
        "# enable 4bit for QLoRA\n",
        "load_in_4bit: true\n",
        "gptq: false\n",
        "strict: false\n",
        "\n",
        "push_dataset_to_hub: utensil\n",
        "hf_use_auth_token: true\n",
        "\n",
        "datasets:\n",
        "  - path: QingyiSi/Alpaca-CoT\n",
        "    data_files:\n",
        "      - Chain-of-Thought/formatted_cot_data/gsm8k_train.json\n",
        "    type: \"alpaca:chat\"\n",
        "\n",
        "dataset_prepared_path: last_run_prepared\n",
        "val_set_size: 0.01\n",
        "# enable QLoRA\n",
        "adapter: qlora\n",
        "lora_model_dir:\n",
        "sequence_len: 2048\n",
        "max_packed_sequence_len:\n",
        "\n",
        "# hyperparameters from QLoRA paper Appendix B.2\n",
        "# \"We find hyperparameters to be largely robust across datasets\"\n",
        "lora_r: 64\n",
        "lora_alpha: 16\n",
        "# 0.1 for models up to 13B\n",
        "# 0.05 for 33B and 65B models\n",
        "lora_dropout: 0.05\n",
        "# add LoRA modules on all linear layers of the base model\n",
        "lora_target_modules:\n",
        "lora_target_linear: true\n",
        "lora_fan_in_fan_out:\n",
        "\n",
        "wandb_project: falcon-qlora\n",
        "wandb_watch:\n",
        "wandb_run_id:\n",
        "wandb_log_model:\n",
        "output_dir: /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "\n",
        "# QLoRA paper Table 9\n",
        "# - 16 for 7b & 13b\n",
        "# - 32 for 33b, 64 for 64b\n",
        "# Max size tested on A6000\n",
        "# - 7b: 40\n",
        "# - 40b: 4\n",
        "# decrease if OOM, increase for max VRAM utilization\n",
        "micro_batch_size: 1\n",
        "gradient_accumulation_steps: 1\n",
        "num_epochs: 3\n",
        "# Optimizer for QLoRA\n",
        "# optimizer: paged_adamw_32bit\n",
        "torchdistx_path:\n",
        "# lr_scheduler: cosine\n",
        "# QLoRA paper Table 9\n",
        "# - 2e-4 for 7b & 13b\n",
        "# - 1e-4 for 33b & 64b\n",
        "learning_rate: 0.0002\n",
        "train_on_inputs: false\n",
        "group_by_length: false\n",
        "bf16: true\n",
        "fp16: false\n",
        "tf32: true\n",
        "gradient_checkpointing: true\n",
        "# stop training after this many evaluation losses have increased in a row\n",
        "# https://huggingface.co/transformers/v4.2.2/_modules/transformers/trainer_callback.html#EarlyStoppingCallback\n",
        "early_stopping_patience: 3\n",
        "resume_from_checkpoint:\n",
        "auto_resume_from_checkpoints: true\n",
        "local_rank:\n",
        "logging_steps: 1\n",
        "xformers_attention: true\n",
        "flash_attention:\n",
        "gptq_groupsize:\n",
        "gptq_model_v1:\n",
        "warmup_steps: 10\n",
        "eval_steps: 5\n",
        "save_steps: 10\n",
        "debug:\n",
        "deepspeed:\n",
        "weight_decay: 0.01\n",
        "fsdp:\n",
        "fsdp_config:\n",
        "special_tokens:\n",
        "  pad_token: \"<|endoftext|>\"\n",
        "  bos_token: \">>ABSTRACT<<\"\n",
        "  eos_token: \"<|endoftext|>\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1u8RtbDY7K",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EeOND66dDnF",
        "outputId": "5f62dddd-cde3-4df9-b319-7a9afccf867b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "--------------------------------------------------\n",
            "DeepSpeed C++/CUDA extension op report\n",
            "--------------------------------------------------\n",
            "NOTE: Ops not installed will be just-in-time (JIT) compiled at\n",
            "      runtime if needed. Op compatibility means that your system\n",
            "      meet the required dependencies to JIT install the op.\n",
            "--------------------------------------------------\n",
            "JIT compiled ops requires ninja\n",
            "ninja .................. \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "op name ................ installed .. compatible\n",
            "--------------------------------------------------\n",
            "async_io ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adagrad ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "cpu_adam ............... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_adam ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "fused_lamb ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "quantizer .............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "random_ltd ............. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.0\n",
            "\u001b[93m [WARNING] \u001b[0m using untested triton version (2.0.0), only 1.0.0 is known to be compatible\n",
            "sparse_attn ............ \u001b[93m[NO]\u001b[0m ....... \u001b[93m[NO]\u001b[0m\n",
            "spatial_inference ...... \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer ............ \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "stochastic_transformer . \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "transformer_inference .. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "utils .................. \u001b[92m[YES]\u001b[0m ...... \u001b[92m[OKAY]\u001b[0m\n",
            "--------------------------------------------------\n",
            "DeepSpeed general environment info:\n",
            "torch install path ............... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch']\n",
            "torch version .................... 2.0.1+cu118\n",
            "deepspeed install path ........... ['/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed']\n",
            "deepspeed info ................... 0.9.3+52907a66, 52907a66, master\n",
            "torch cuda version ............... 11.8\n",
            "torch hip version ................ None\n",
            "nvcc version ..................... 11.8\n",
            "deepspeed wheel compiled w. ...... torch 2.0, cuda 11.8\n"
          ]
        }
      ],
      "source": [
        "!ds_report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnz7AF94LJTa"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xR8QPcw3DY7K",
        "outputId": "e1534679-b8e6-4524-a05d-490d66e62a47",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setting ds_accelerator to cuda (auto detect)\n",
            "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
            "\t`--num_processes` was set to a value of `2`\n",
            "\t\tMore than one GPU was found, enabling multi-GPU training.\n",
            "\t\tIf this was unintended please pass in `--num_processes=1`.\n",
            "\t`--num_machines` was set to a value of `1`\n",
            "\t`--mixed_precision` was set to a value of `'no'`\n",
            "\t`--dynamo_backend` was set to a value of `'no'`\n",
            "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "Setting ds_accelerator to cuda (auto detect)\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g_gl17qn/none_wv8lcrd7/attempt_0/0/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "\n",
            "===================================BUG REPORT===================================\n",
            "Welcome to bitsandbytes. For bug reports, please run\n",
            "\n",
            "python -m bitsandbytes\n",
            "\n",
            " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
            "================================================================================\n",
            "bin /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/nvidia/lib64'), PosixPath('/usr/local/nvidia/lib')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: /usr/local/nvidia/lib:/usr/local/nvidia/lib64 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('//matplotlib_inline.backend_inline'), PosixPath('module')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDvnE4umHheXhWsDJbbukYvvyc47/mC4z8syS93btA72T90WDrQagOy5O+DrhdXOvr5i/JwsTlAImy57eLRrtRFOrQq73jyi7Dzo0tvrAiNLVgX2q2dFLoplRyXDXiVYLPmPieMWQOeUCLeSb8FC5zzllcocZwjMXpxScDerZqnlAR0ccpSkGyKIod4ZMkn/29A/C5kHEb/wT8cOAq+MWJ/2okZZgbiR0AMV4DynAkrtcx9JnJnTs9chiMyH+dyCS42Ai24sHWJBkQo6TfxXkyKo9GOpu3Y2WLgrHyaot9Lk5mA1mujyIWdlReD2nvjeCQKjl3KW3xZ73m4nD97MydWSWoJfEWlr+VZvk8EWsZk3CYLZCIBLdod6xXJJ0DD0pvTIq11c8VB7XkgVjapuU/sC8M6HFzHW/NBeE+xX/txPkZkIGqrnxeQ0AtBXdN9ukyNGhGzTkPYJNliiYpY0dCvVuz/BJ2FawFTQGnD1EHOenUCRajREFGCbKoYZqi40j8= utensil@Utensils-MacBook-Pro.local')}\n",
            "  warn(msg)\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/tmp/torchelastic_g_gl17qn/none_wv8lcrd7/attempt_0/1/error.json')}\n",
            "  warn(msg)\n",
            "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
            "/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/cuda_setup/main.py:149: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We'll flip a coin and try one of these, in order to fail forward.\n",
            "Either way, this might cause trouble in the future:\n",
            "If you get `CUDA error: invalid device function` errors, the above might be the cause and the solution is to make sure only one ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] in the paths that we search based on your env.\n",
            "  warn(msg)\n",
            "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so.11.0\n",
            "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
            "CUDA SETUP: Detected CUDA version 118\n",
            "CUDA SETUP: Loading binary /root/miniconda3/envs/py3.9/lib/python3.9/site-packages/bitsandbytes-0.39.0-py3.9.egg/bitsandbytes/libbitsandbytes_cuda118.so...\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /workspace/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:root:`trust_remote_code` is set to true. Please make sure that you reviewed the remote code/model.\n",
            "INFO:root:loading tokenizer... /workspace/llm-playground/models/tiiuae_falcon-40b\n",
            "Using bos_token, but it is not set yet.\n",
            "Using pad_token, but it is not set yet.\n",
            "Using unk_token, but it is not set yet.\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 469.58it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "WARNING:datasets.builder:Found cached dataset parquet (/root/.cache/huggingface/datasets/utensil___parquet/utensil--31a4e867d804a957707db033c9abcd13-7b1208a23cdad6e9/0.0.0/14a00e99c0d15a23649d0db8944380ac81082d4b021f398733dd84f3a6c569a7)\n",
            "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 593.76it/s]\n",
            "INFO:root:loading model and peft_config...\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:16<00:00,  8.54s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense_4h_to_h', 'dense_h_to_4h', 'query_key_value', 'dense']\n",
            "Loading checkpoint shards: 100%|██████████████████| 9/9 [01:51<00:00, 12.39s/it]\n",
            "INFO:root:converting PEFT model w/ prepare_model_for_kbit_training\n",
            "INFO:root:found linear modules: ['dense', 'dense_4h_to_h', 'query_key_value', 'dense_h_to_4h']\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "[2023-06-15 02:32:44,703] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-15 02:32:44,703] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "[2023-06-15 02:32:44,703] [INFO] [comm.py:625:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "INFO:torch.distributed.distributed_c10d:Waiting in store based barrier to initialize process group for rank: 0, key: store_based_barrier_key:1 (world_size=2, worker_count=1, timeout=0:30:00)\n",
            "trainable params: 444,334,080 || all params: 21,363,310,592 || trainable%: 2.0798933671188187\n",
            "[2023-06-15 02:33:19,971] [WARNING] [comm.py:152:init_deepspeed_backend] NCCL backend in DeepSpeed not yet implemented\n",
            "[2023-06-15 02:33:19,972] [INFO] [comm.py:594:init_distributed] cdb=None\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:1 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:1 with 2 nodes.\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Compiling torch model\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:root:Pre-saving adapter config to /content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
            "INFO:root:Starting trainer...\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 0\n",
            "INFO:torch.distributed.distributed_c10d:Added key: store_based_barrier_key:2 to store for rank: 1\n",
            "INFO:torch.distributed.distributed_c10d:Rank 1: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "INFO:torch.distributed.distributed_c10d:Rank 0: Completed store-based barrier for key:store_based_barrier_key:2 with 2 nodes.\n",
            "Parameter Offload: Total persistent parameters: 1982464 in 242 params\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mutensil\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.4\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/workspace/axolotl/wandb/run-20230615_023334-bl6nnrb9\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mnorthern-totem-32\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/utensil/falcon-qlora/runs/bl6nnrb9\u001b[0m\n",
            "  0%|                                                 | 0/11097 [00:00<?, ?it/s]You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1963, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7f3b0e402200>\n",
            "Traceback (most recent call last):\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 352, in <module>\n",
            "    fire.Fire(train)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 141, in Fire\n",
            "    component_trace = _Fire(component, args, parsed_flag_args, context, name)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 475, in _Fire\n",
            "    component, remaining_args = _CallAndUpdateTrace(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/fire/core.py\", line 691, in _CallAndUpdateTrace\n",
            "    component = fn(*varargs, **kwargs)\n",
            "  File \"/workspace/axolotl/scripts/finetune.py\", line 337, in train\n",
            "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1539, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 1801, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/transformers/trainer.py\", line 2647, in training_step\n",
            "    self.accelerator.backward(loss)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/accelerator.py\", line 1835, in backward\n",
            "    self.deepspeed_engine_wrapped.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/utils/deepspeed.py\", line 167, in backward\n",
            "    self.engine.backward(loss, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/engine.py\", line 1859, in backward\n",
            "    self.optimizer.backward(loss, retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/stage3.py\", line 1963, in backward\n",
            "    self.loss_scaler.backward(loss.float(), retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/fp16/loss_scaler.py\", line 63, in backward\n",
            "    scaled_loss.backward(retain_graph=retain_graph)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/_tensor.py\", line 487, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/autograd/function.py\", line 274, in apply\n",
            "    return user_fn(self, *args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/checkpoint.py\", line 141, in backward\n",
            "    outputs = ctx.run_function(*detached_inputs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 642, in custom_forward\n",
            "    return module(*inputs, use_cache=use_cache, output_attentions=output_attentions)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 396, in forward\n",
            "    attn_outputs = self.self_attention(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1538, in _call_impl\n",
            "    result = forward_call(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/hooks.py\", line 165, in new_forward\n",
            "    output = old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/tiiuae_falcon-40b/modelling_RW.py\", line 252, in forward\n",
            "    fused_qkv = self.query_key_value(hidden_states)  # [batch_size, seq_length, 3 x hidden_size]\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    result = hook(self, args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 371, in _pre_forward_module_hook\n",
            "    self.pre_sub_module_forward_function(module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/parameter_offload.py\", line 483, in pre_sub_module_forward_function\n",
            "    param_coordinator.fetch_sub_module(sub_module)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/utils/_contextlib.py\", line 115, in decorate_context\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 251, in fetch_sub_module\n",
            "    self.__all_gather_params(params_to_fetch)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partitioned_param_coordinator.py\", line 381, in __all_gather_params\n",
            "    handle = partitioned_params[0].all_gather_coalesced(partitioned_params)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/utils/nvtx.py\", line 15, in wrapped_fn\n",
            "    ret_val = func(*args, **kwargs)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/zero/partition_parameters.py\", line 928, in all_gather_coalesced\n",
            "    dtype=get_only_unique_item(p.dtype for p in params),\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/deepspeed/runtime/utils.py\", line 824, in get_only_unique_item\n",
            "    raise RuntimeError(f\"expected there to be only one unique element in {items}\")\n",
            "RuntimeError: expected there to be only one unique element in <generator object Init._convert_to_deepspeed_param.<locals>.all_gather_coalesced.<locals>.<genexpr> at 0x7fbd7808b3c0>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 1).\u001b[0m Press Control-C to abort syncing.\n",
            "WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 3875 closing signal SIGTERM\n",
            "ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 3876) of binary: /root/miniconda3/envs/py3.9/bin/python3\n",
            "Traceback (most recent call last):\n",
            "  File \"/root/miniconda3/envs/py3.9/bin/accelerate\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/accelerate_cli.py\", line 45, in main\n",
            "    args.func(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 960, in launch_command\n",
            "    multi_gpu_launcher(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/accelerate/commands/launch.py\", line 649, in multi_gpu_launcher\n",
            "    distrib_run.run(args)\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/run.py\", line 785, in run\n",
            "    elastic_launch(\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 134, in __call__\n",
            "    return launch_agent(self._config, self._entrypoint, list(args))\n",
            "  File \"/root/miniconda3/envs/py3.9/lib/python3.9/site-packages/torch/distributed/launcher/api.py\", line 250, in launch_agent\n",
            "    raise ChildFailedError(\n",
            "torch.distributed.elastic.multiprocessing.errors.ChildFailedError: \n",
            "============================================================\n",
            "scripts/finetune.py FAILED\n",
            "------------------------------------------------------------\n",
            "Failures:\n",
            "  <NO_OTHER_FAILURES>\n",
            "------------------------------------------------------------\n",
            "Root Cause (first observed failure):\n",
            "[0]:\n",
            "  time      : 2023-06-15_02:33:42\n",
            "  host      : a03032734f1e\n",
            "  rank      : 1 (local_rank: 1)\n",
            "  exitcode  : 1 (pid: 3876)\n",
            "  error_file: <N/A>\n",
            "  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcbjGJ2GDY7L"
      },
      "source": [
        "## #3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6hvCkF0DY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwPMxEdeLLDu"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9XsSrgMvDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVwP73VWDY7L"
      },
      "source": [
        "## #4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r-azUQO0dDnH"
      },
      "outputs": [],
      "source": [
        "%env ACCELERATE_USE_DEEPSPEED=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3_tDMBqDY7L",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzFuqSRNLL5_"
      },
      "outputs": [],
      "source": [
        "#%%writefile examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VIorWx3DY7M",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TrgHEoizhLx"
      },
      "source": [
        "## #5 mbs 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0NdJGjFTdDnI"
      },
      "outputs": [],
      "source": [
        "%env HF_HUB_DISABLE_PROGRESS_BARS=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcOtbqlCdDnI"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WpJG6JK2zhLx",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hrnqGCBezhLy"
      },
      "source": [
        "## #6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHaDkOkvdDnI"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aEj2fAgzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UivJrMd8zhLz"
      },
      "source": [
        "## #7"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99raQX83zhLz"
      },
      "outputs": [],
      "source": [
        "!cat examples/falcon/config-40b-qlora.yml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ubZ7fVBVzhLz"
      },
      "outputs": [],
      "source": [
        "!accelerate launch scripts/finetune.py examples/falcon/config-40b-qlora.yml --deepspeed ds_config.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuaiP0pUzhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wzy_ZDY4zhL0"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFWDQTL_vGSL"
      },
      "source": [
        "# Upload"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spqeHYEGDY7Q"
      },
      "source": [
        "### Upload checkpoints to HF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkekyF8DKerI"
      },
      "outputs": [],
      "source": [
        "%cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eL8SpVYpNY1y"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint-"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj_79rGoKkA_"
      },
      "outputs": [],
      "source": [
        "!ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XK8IWmrtKvlJ"
      },
      "outputs": [],
      "source": [
        "# ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgyQDH-YDY7R"
      },
      "outputs": [],
      "source": [
        "!python /workspace/llm-playground/helper/storage.py utensil/axolotl-trained /content/ -u"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H5TnMCeLDY7M"
      },
      "source": [
        "## Below are ad hoc cells handling issues during training\n",
        "\n",
        "current out dir:\n",
        "\n",
        "```\n",
        "/content/axolotl-trained/falcon-qlora-40b-gsm8k/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdCBx0UZDY7M"
      },
      "source": [
        "### Force release VRAM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_0VdzsqSDY7M"
      },
      "outputs": [],
      "source": [
        "# First interupt the kernel, wait a few seconds then run this to kill finetune to release VRAM\n",
        "!ps aux|grep python|grep finetune|awk '{print $2}'|xargs kill"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT7StF_PDY7N"
      },
      "source": [
        "### Clean the finetuned model and all checkpoints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FT4c-7KHDY7N"
      },
      "outputs": [],
      "source": [
        "# Only run this to start over\n",
        "!rm -rf /content/axolotl-trained/falcon-qlora-40b-gsm8k/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6s4xi9jDY7N"
      },
      "source": [
        "### Zip the prepared dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZ-aBbwEDY7N"
      },
      "outputs": [],
      "source": [
        "!apt install zip\n",
        "!zip -r last_run_prepared.zip -xi last_run_prepared"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BFC20jMDY7N"
      },
      "source": [
        "### Monitoring GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MgPaXew2DY7O"
      },
      "outputs": [],
      "source": [
        "# Run this in a seperate terminal\n",
        "!nvitop -m full"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYFlPnbXDY7O"
      },
      "source": [
        "### Fix DISK FULL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yv5CWZu7DY7O"
      },
      "outputs": [],
      "source": [
        "%cd /"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wvqYxcpZDY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h|grep G"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tIIJxrv1DY7P"
      },
      "outputs": [],
      "source": [
        "!du -d 2 -h /root/.local"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "25FTebpHDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/Trash/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8K4d-2ocDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.local/share/wandb/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtOoZLugDY7P"
      },
      "outputs": [],
      "source": [
        "!rm -rf /root/.cache/wandb/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "17SexQnqDY7P"
      },
      "source": [
        "### Check who is using GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kE8A5crNDY7P",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!apt install lsof"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwNCASirDY7Q",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!lsof /dev/nvidia*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxZv_qjgDY7Y"
      },
      "source": [
        "### A new bash without tmux etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NDkJAXwTDY7Z"
      },
      "outputs": [],
      "source": [
        "!bash --norc --noprofile"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pR3s54THDY7Z"
      },
      "source": [
        "### Clean up all checkpoints but last one"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdO1EybEDY7Z"
      },
      "outputs": [],
      "source": [
        "cd /content/axolotl-trained/falcon-qlora-40b-gsm8k/ && ls -lhta |grep checkpoint- | awk 'NR > 1 {print $9}' | xargs rm -rf"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}